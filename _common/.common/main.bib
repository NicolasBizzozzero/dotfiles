
@inproceedings{ahmadContextAttentiveDocument2019,
  ids = {ahmadContextAttentiveDocument2019a,ahmadContextAttentiveDocument2019b,ahmadContextAttentiveDocument2019c},
  title = {Context {{Attentive Document Ranking}} and {{Query Suggestion}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}} - {{SIGIR}}'19},
  author = {Ahmad, Wasi Uddin and Chang, Kai-Wei and Wang, Hongning},
  date = {2019},
  pages = {385--394},
  publisher = {{ACM Press}},
  location = {{Paris, France}},
  doi = {10.1145/3331184.3331246},
  url = {http://dl.acm.org/citation.cfm?doid=3331184.3331246},
  urldate = {2020-02-07},
  abstract = {We present a context-aware neural ranking model to exploit users’ on-task search activities and enhance retrieval performance. In particular, a two-level hierarchical recurrent neural network is introduced to learn search context representation of individual queries, search tasks, and corresponding dependency structure by jointly optimizing two companion retrieval tasks: document ranking and query suggestion. To identify variable dependency structure between search context and users’ ongoing search activities, attention at both levels of recurrent states are introduced. Extensive experiment comparisons against a rich set of baseline methods and an in-depth ablation analysis confirm the value of our proposed approach for modeling search context buried in search tasks.},
  eventtitle = {The 42nd {{International ACM SIGIR Conference}}},
  isbn = {978-1-4503-6172-9},
  langid = {english},
  keywords = {document ranking,neural ir models,query suggestion,search tasks},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/LNRBLT5E/Ahmad et al. - 2019 - Context Attentive Document Ranking and Query Sugge.pdf;/home/johnlocke/.zotero/storage/93RPV84M/3331184.html}
}

@inproceedings{ahmadMultiTaskLearningDocument2018,
  title = {Multi-{{Task Learning}} for {{Document Ranking}} and {{Query Suggestion}}},
  author = {Ahmad, Wasi Uddin and Chang, Kai-Wei and Wang, Hongning},
  date = {2018},
  url = {https://openreview.net/forum?id=SJ1nzBeA-},
  urldate = {2021-02-22},
  abstract = {We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search. It consists of two major components, a document ranker, and a query recommender....},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  annotation = {ZSCC: 0000022},
  file = {/home/johnlocke/.zotero/storage/7BS3VQTB/Ahmad et al_2018_Multi-Task Learning for Document Ranking and Query Suggestion.pdf;/home/johnlocke/.zotero/storage/EJ3BSL8E/forum.html}
}

@article{anastasopoulosTiedMultitaskLearning2018,
  ids = {anastasopoulosTiedMultitaskLearning2018a},
  title = {Tied {{Multitask Learning}} for {{Neural Speech Translation}}},
  author = {Anastasopoulos, Antonios and Chiang, David},
  date = {2018-02-19},
  url = {https://arxiv.org/abs/1802.06655v2},
  urldate = {2020-03-06},
  abstract = {We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.},
  langid = {english},
  annotation = {ZSCC: 0000044},
  file = {/home/johnlocke/.zotero/storage/E59K9G2R/Anastasopoulos_Chiang_2018_Tied multitask learning for neural speech translation.pdf;/home/johnlocke/.zotero/storage/JN6E5WNY/1802.06655.pdf;/home/johnlocke/.zotero/storage/NB65ER3D/Anastasopoulos_Chiang_2018_Tied Multitask Learning for Neural Speech Translation.pdf;/home/johnlocke/.zotero/storage/RV3ZPL4E/1802.html}
}

@unpublished{arjovskyUnitaryEvolutionRecurrent2016,
  title = {Unitary {{Evolution Recurrent Neural Networks}}},
  author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  date = {2016-05-25},
  eprint = {1511.06464},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1511.06464},
  urldate = {2019-12-10},
  abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very longterm dependencies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/WCFWFYQ7/Arjovsky et al_2016_Unitary Evolution Recurrent Neural Networks.pdf}
}

@online{AttentionAttention2018,
  title = {Attention? {{Attention}}!},
  shorttitle = {Attention?},
  date = {2018-06-24T11:07:00+00:00},
  url = {https://lilianweng.github.io/2018/06/24/attention-attention.html},
  urldate = {2020-02-23},
  abstract = {Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.},
  langid = {english},
  organization = {{Lil'Log}},
  file = {/home/johnlocke/.zotero/storage/RD87UCAV/attention-attention.html}
}

@unpublished{bahdanauNeuralMachineTranslation2016,
  ids = {bahdanauNeuralMachineTranslation2016a},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2019-12-10},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/B8Z5F6XP/Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/home/johnlocke/.zotero/storage/252SB3SW/1409.html}
}

@unpublished{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2021-03-05},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: 0002787},
  file = {/home/johnlocke/.zotero/storage/NZR6LFR7/Ba et al_2016_Layer Normalization.pdf;/home/johnlocke/.zotero/storage/DNADRJ9Z/1607.html}
}

@inproceedings{bar-yossefContextsensitiveQueryAutocompletion2011,
  ids = {bar-yossefContextsensitiveQueryAutocompletion2011a,bar-yossefContextsensitiveQueryAutocompletion2011b},
  title = {Context-Sensitive Query Auto-Completion},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web},
  author = {Bar-Yossef, Ziv and Kraus, Naama},
  date = {2011-03-28},
  series = {{{WWW}} '11},
  pages = {107--116},
  publisher = {{Association for Computing Machinery}},
  location = {{Hyderabad, India}},
  doi = {10.1145/1963405.1963424},
  url = {https://doi.org/10.1145/1963405.1963424},
  urldate = {2020-01-27},
  abstract = {Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. In order to evaluate our approach, we performed extensive experimentation over the public AOL query log. We demonstrate that when the recent user's queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion's MRR is 48\% higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion's MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31.5\% in MRR relative to MostPopularCompletion on average.},
  isbn = {978-1-4503-0632-4},
  keywords = {context-awareness,query auto-completion,query expansion},
  annotation = {ZSCC: 0000191},
  file = {/home/johnlocke/.zotero/storage/UUYBYIIA/Bar-Yossef et Kraus - 2011 - Context-sensitive query auto-completion.pdf;/home/johnlocke/.zotero/storage/QSWID7HT/1963405.html}
}

@inproceedings{bengioCurriculumLearning2009,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
  date = {2009-06-14},
  series = {{{ICML}} '09},
  pages = {41--48},
  publisher = {{Association for Computing Machinery}},
  location = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553380},
  url = {https://doi.org/10.1145/1553374.1553380},
  urldate = {2020-03-02},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  isbn = {978-1-60558-516-1},
  annotation = {ZSCC: 0001787},
  file = {/home/johnlocke/.zotero/storage/S6ERCH9R/Bengio et al_2009_Curriculum learning.pdf}
}

@inproceedings{bhatiaQuerySuggestionsAbsence2011,
  ids = {bhatiaQuerySuggestionsAbsence2011a,bhatiaQuerySuggestionsAbsence2011b},
  title = {Query Suggestions in the Absence of Query Logs},
  booktitle = {Proceedings of the 34th International {{ACM SIGIR}} Conference on {{Research}} and Development in {{Information Retrieval}}},
  author = {Bhatia, Sumit and Majumdar, Debapriyo and Mitra, Prasenjit},
  date = {2011},
  pages = {795--804},
  annotation = {ZSCC: 0000158},
  file = {/home/johnlocke/.zotero/storage/M45JZ3G4/Bhatia et al_2011_Query suggestions in the absence of query logs.pdf;/home/johnlocke/.zotero/storage/5QBIQK7H/2009916.html;/home/johnlocke/.zotero/storage/BV25L35G/2009916.html;/home/johnlocke/.zotero/storage/N279QHKI/2009916.html}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006},
  publisher = {{springer}},
  annotation = {ZSCC: 0040766},
  file = {/home/johnlocke/.zotero/storage/A2RZ824Y/998831.html}
}

@article{bodenreiderUnifiedMedicalLanguage2004,
  title = {The {{Unified Medical Language System}} ({{UMLS}}): Integrating Biomedical Terminology},
  shorttitle = {The {{Unified Medical Language System}} ({{UMLS}})},
  author = {Bodenreider, Olivier},
  date = {2004-01-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res},
  volume = {32},
  eprint = {14681409},
  eprinttype = {pmid},
  pages = {D267-D270},
  issn = {0305-1048},
  doi = {10.1093/nar/gkh061},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC308795/},
  urldate = {2019-12-17},
  abstract = {The Unified Medical Language System (http://umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.},
  issue = {Database issue},
  pmcid = {PMC308795},
  annotation = {ZSCC: 0002662},
  file = {/home/johnlocke/.zotero/storage/JFNNJERF/Bodenreider_2004_The Unified Medical Language System (UMLS).pdf}
}

@unpublished{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2017-06-19},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1607.04606},
  urldate = {2020-02-27},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0002777},
  file = {/home/johnlocke/.zotero/storage/4IETXK55/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf}
}

@inproceedings{boldiQueryflowGraphModel2008,
  title = {The Query-Flow Graph: Model and Applications},
  shorttitle = {The Query-Flow Graph},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Information}} and Knowledge Management},
  author = {Boldi, Paolo and Bonchi, Francesco and Castillo, Carlos and Donato, Debora and Gionis, Aristides and Vigna, Sebastiano},
  date = {2008},
  pages = {609--618},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000367},
  file = {/home/johnlocke/.zotero/storage/38338WZK/Boldi et al_2008_The query-flow graph.pdf;/home/johnlocke/.zotero/storage/2TSZ7RGM/1458082.html}
}

@unpublished{bommasaniOpportunitiesRisksFoundation2021,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  options = {useprefix=true},
  date = {2021-08-18},
  eprint = {2108.07258},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2021-08-24},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/BPL5UZEE/Bommasani et al_2021_On the Opportunities and Risks of Foundation Models.pdf;/home/johnlocke/.zotero/storage/TT2B6856/2108.html}
}

@inproceedings{bonchiEfficientQueryRecommendations2012,
  title = {Efficient Query Recommendations in the Long Tail via Center-Piece Subgraphs},
  booktitle = {Proceedings of the 35th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Bonchi, Francesco and Perego, Raffaele and Silvestri, Fabrizio and Vahabi, Hossein and Venturini, Rossano},
  date = {2012},
  pages = {345--354},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000043},
  file = {/home/johnlocke/.zotero/storage/RNYKFJ89/Bonchi et al_2012_Efficient query recommendations in the long tail via center-piece subgraphs.pdf;/home/johnlocke/.zotero/storage/AJYXNBW7/2348283.html}
}

@inproceedings{bordinoPenguinsSweatersSerendipitous2013,
  ids = {bordinoPenguinsSweatersSerendipitous2013a},
  title = {Penguins in Sweaters, or Serendipitous Entity Search on User-Generated Content},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Information}} \& {{Knowledge Management}}},
  author = {Bordino, Ilaria and Mejova, Yelena and Lalmas, Mounia},
  date = {2013},
  pages = {109--118},
  annotation = {ZSCC: 0000051},
  file = {/home/johnlocke/.zotero/storage/U2TA4URT/Bordino et al_2013_Penguins in sweaters, or serendipitous entity search on user-generated content.pdf;/home/johnlocke/.zotero/storage/ID8H3L93/2505515.html}
}

@article{broccoloGeneratingSuggestionsQueries2012,
  title = {Generating Suggestions for Queries in the Long Tail with an Inverted Index},
  author = {Broccolo, Daniele and Marcon, Lorenzo and Nardini, Franco Maria and Perego, Raffaele and Silvestri, Fabrizio},
  date = {2012-03},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {48},
  number = {2},
  pages = {326--339},
  issn = {03064573},
  doi = {10.1016/j.ipm.2011.07.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457311000756},
  urldate = {2020-01-20},
  abstract = {This paper proposes an efficient and effective solution to the problem of choosing the queries to suggest to web search engine users in order to help them in rapidly satisfying their information needs. By exploiting a weak function for assessing the similarity between the current query and the knowledge base built from historical users’ sessions, we re-conduct the suggestion generation phase to the processing of a full-text query over an inverted index. The resulting query recommendation technique is very efficient and scalable, and is less affected by the data-sparsity problem than most state-of-the-art proposals. Thus, it is particularly effective in generating suggestions for rare queries occurring in the long tail of the query popularity distribution. The quality of suggestions generated is assessed by evaluating the effectiveness in forecasting the users’ behavior recorded in historical query logs, and on the basis of the results of a reproducible user study conducted on publicly-available, human-assessed data. The experimental evaluation conducted shows that our proposal remarkably outperforms two other state-of-the-art solutions, and that it can generate useful suggestions even for rare and never seen queries.},
  langid = {english},
  annotation = {ZSCC: 0000036},
  file = {/home/johnlocke/.zotero/storage/MWQSQP2Z/Broccolo et al_2012_Generating suggestions for queries in the long tail with an inverted index.pdf;/home/johnlocke/.zotero/storage/LM5MHKPJ/S0306457311000756.html}
}

@article{caiSurveyQueryAuto2016,
  title = {A {{Survey}} of {{Query Auto Completion}} in {{Information Retrieval}}},
  author = {Cai, Fei and de Rijke, Maarten},
  options = {useprefix=true},
  date = {2016},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  shortjournal = {FNT in Information Retrieval},
  volume = {10},
  number = {4},
  pages = {273--363},
  issn = {1554-0669, 1554-0677},
  doi = {10.1561/1500000055},
  url = {http://www.nowpublishers.com/article/Details/INR-055},
  urldate = {2020-01-20},
  langid = {english},
  annotation = {ZSCC: 0000062},
  file = {/home/johnlocke/.zotero/storage/AMR7PU27/Cai_de Rijke_2016_A Survey of Query Auto Completion in Information Retrieval.pdf;/home/johnlocke/.zotero/storage/27A4M2WW/INR-055.html}
}

@inproceedings{callanLemurProjectIts2012,
  title = {The Lemur Project and Its {{ClueWeb12}} Dataset},
  booktitle = {Invited Talk at the {{SIGIR}} 2012 {{Workshop}} on {{Open-Source Information Retrieval}}},
  author = {Callan, Jamie},
  date = {2012},
  annotation = {ZSCC: 0000009},
  file = {/home/johnlocke/.zotero/storage/WIUIJNJQ/Callan_2012_The lemur project and its ClueWeb12 dataset.pdf}
}

@inproceedings{caoContextawareQuerySuggestion2008,
  title = {Context-Aware Query Suggestion by Mining Click-through and Session Data},
  booktitle = {Proceeding of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} 08},
  author = {Cao, Huanhuan and Jiang, Daxin and Pei, Jian and He, Qi and Liao, Zhen and Chen, Enhong and Li, Hang},
  date = {2008},
  pages = {875},
  publisher = {{ACM Press}},
  location = {{Las Vegas, Nevada, USA}},
  doi = {10.1145/1401890.1401995},
  url = {http://dl.acm.org/citation.cfm?doid=1401890.1401995},
  urldate = {2020-01-28},
  abstract = {Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware – they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offline modellearning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user’s search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence suffix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1.8 billion search queries, 2.6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions.},
  eventtitle = {The 14th {{ACM SIGKDD}} International Conference},
  isbn = {978-1-60558-193-4},
  langid = {english},
  annotation = {ZSCC: 0000568},
  file = {/home/johnlocke/.zotero/storage/BE7XANQE/Cao et al. - 2008 - Context-aware query suggestion by mining click-thr.pdf;/home/johnlocke/.zotero/storage/545NLXFU/1401890.html}
}

@inproceedings{chaudhuriGeneralizationFrameworkQuery1990,
  title = {Generalization and a {{Framework}} for {{Query Modification}}},
  booktitle = {Proceedings of the {{Sixth International Conference}} on {{Data Engineering}}},
  author = {Chaudhuri, Surajit},
  date = {1990},
  pages = {138--145},
  publisher = {{IEEE Computer Society}},
  annotation = {ZSCC: 0000086},
  file = {/home/johnlocke/.zotero/storage/H4U5FBTP/645475.html}
}

@unpublished{chenAttentionbasedHierarchicalNeural2018,
  ids = {chenAttentionbasedHierarchicalNeural2018a},
  title = {Attention-Based {{Hierarchical Neural Query Suggestion}}},
  author = {Chen, Wanyu and Cai, Fei and Chen, Honghui and de Rijke, Maarten},
  options = {useprefix=true},
  date = {2018},
  eprint = {1805.02816},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1805.02816},
  urldate = {2019-12-10},
  abstract = {Query suggestions help users of a search engine to refine their queries. Previous work on query suggestion has mainly focused on incorporating directly observable features such as query cooccurrence and semantic similarity. The structure of such features is often set manually, as a result of which hidden dependencies between queries and users may be ignored. We propose an Attentionbased Hierarchical Neural Query Suggestion (AHNQS) model that combines a hierarchical structure with a session-level neural network and a user-level neural network to model the short- and long-term search history of a user. An attention mechanism is used to capture user preferences. We quantify the improvements of AHNQS over state-of-the-art recurrent neural network-based query suggestion baselines on the AOL query log dataset, with improvements of up to 21.86\% and 22.99\% in terms of MRR@10 and Recall@10, respectively, over the state-of-the-art; improvements are especially large for short sessions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/johnlocke/.zotero/storage/QSG83DGQ/Chen et al_2018_Attention-based Hierarchical Neural Query Suggestion.pdf;/home/johnlocke/.zotero/storage/Y9FESWJJ/Chen et al_2018_Attention-based Hierarchical Neural Query Suggestion.pdf;/home/johnlocke/.zotero/storage/U9CEY5BR/1805.html}
}

@inproceedings{chenContextAwareClickModel2020,
  ids = {chenContextAwareClickModel2020a},
  title = {A {{Context-Aware Click Model}} for {{Web Search}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Chen, Jia and Mao, Jiaxin and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
  date = {2020-01-20},
  pages = {88--96},
  publisher = {{ACM}},
  location = {{Houston TX USA}},
  doi = {10.1145/3336191.3371819},
  url = {http://dl.acm.org/doi/10.1145/3336191.3371819},
  urldate = {2020-01-29},
  abstract = {To better exploit the search logs, various click models have been proposed to extract implicit relevance feedback from user clicks. Most traditional click models are based on probability graphical models (PGMs) with manually designed dependencies. Recently, some researchers also adopt neural-based methods to improve the accuracy of click prediction. However, most of the existing click models only model user behavior in query level. As the previous iterations within the session may have an impact on the current search round, we can leverage these behavior signals to better model user behaviors. In this paper, we propose a novel neuralbased Context-Aware Click Model (CACM) for Web search. CACM consists of a context-aware relevance estimator and an examination predictor. The relevance estimator utilizes session context information, i.e., the query sequence and clickthrough data, as well as the pre-trained embeddings learned from a session- ow graph to estimate the context-aware relevance of each search result. The examination predictor estimates the examination probability of each result. We further investigate several combination functions to integrate the context-aware relevance and examination probability into click prediction. Experiment results on a public Web search dataset show that CACM outperforms existing click models in both relevance estimation and click prediction tasks.},
  eventtitle = {{{WSDM}} '20: {{The Thirteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  isbn = {978-1-4503-6822-3},
  langid = {english},
  keywords = {click model,click prediction,document ranking,web search},
  file = {/home/johnlocke/.zotero/storage/DYYQB755/Chen et al_2020_A Context-Aware Click Model for Web Search.pdf;/home/johnlocke/.zotero/storage/QN5SR2AY/Chen et al. - 2020 - A Context-Aware Click Model for Web Search.pdf}
}

@unpublished{chengLongShortTermMemoryNetworks2016,
  title = {Long {{Short-Term Memory-Networks}} for {{Machine Reading}}},
  author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  date = {2016-09-20},
  eprint = {1601.06733},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1601.06733},
  urldate = {2020-02-23},
  abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000407},
  file = {/home/johnlocke/.zotero/storage/AMFL4MDV/Cheng et al_2016_Long Short-Term Memory-Networks for Machine Reading.pdf;/home/johnlocke/.zotero/storage/FAIANWMX/1601.html}
}

@article{chenHierarchicalNeuralQuery2020,
  title = {Hierarchical Neural Query Suggestion with an Attention Mechanism},
  author = {Chen, Wanyu and Cai, Fei and Chen, Honghui and de Rijke, Maarten},
  date = {2020-11-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {57},
  number = {6},
  pages = {102040},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457318308732},
  urldate = {2021-03-11},
  abstract = {Query suggestions help users of a search engine to refine their queries. Previous work on query suggestion has mainly focused on incorporating directly observable features such as query co-occurrence and semantic similarity. The structure of such features is often set manually, as a result of which hidden dependencies between queries and users may be ignored. We propose an Attention-based Hierarchical Neural Query Suggestion (AHNQS) model that uses an attention mechanism to automatically capture user preferences. AHNQS combines a session-level neural network and a user-level neural network into a hierarchical structure to model the short- and long-term search history of a user. We quantify the improvements of AHNQS over state-of-the-art recurrent neural network-based query suggestion baselines on the AOL query log dataset, with improvements of up to 9.66\% and 12.51\% in terms of Recall@10 and MRR@10, respectively; improvements are especially obvious for short sessions and inactive users with few search sessions.},
  langid = {english},
  keywords = {Neural methods for information retrieval,Query suggestion},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/LQX24QGI/S0306457318308732.html}
}

@article{chenHybridFrameworkSession2021,
  ids = {chenHybridFrameworkSession2021a},
  title = {A {{Hybrid Framework}} for {{Session Context Modeling}}},
  author = {Chen, Jia and Mao, Jiaxin and Liu, Yiqun and Ye, Ziyi and Ma, Weizhi and Wang, Chao and Zhang, Min and Ma, Shaoping},
  date = {2021-05-06},
  journaltitle = {ACM Transactions on Information Systems},
  shortjournal = {ACM Trans. Inf. Syst.},
  volume = {39},
  number = {3},
  pages = {30:1--30:35},
  issn = {1046-8188},
  doi = {10.1145/3448127},
  url = {https://doi.org/10.1145/3448127},
  urldate = {2022-01-11},
  abstract = {Understanding user intent is essential for various retrieval tasks. By leveraging contextual information within sessions, e.g., query history and user click behaviors, search systems can capture user intent more accurately and thus perform better. However, most existing systems only consider intra-session contexts and may suffer from the problem of lacking contextual information, because short search sessions account for a large proportion in practical scenarios. We believe that in these scenarios, considering more contexts, e.g., cross-session dependencies, may help alleviate the problem and contribute to better performance. Therefore, we propose a novel Hybrid framework for Session Context Modeling (HSCM), which realizes session-level multi-task learning based on the self-attention mechanism. To alleviate the problem of lacking contextual information within current sessions, HSCM exploits the cross-session contexts by sampling user interactions under similar search intents in the historical sessions and further aggregating them into the local contexts. Besides, application of the self-attention mechanism rather than RNN-based frameworks in modeling session-level sequences also helps (1) better capture interactions within sessions, (2) represent the session contexts in parallelization. Experimental results on two practical search datasets show that HSCM not only outperforms strong baseline solutions such as HiNT, CARS, and BERTserini in document ranking, but also performs significantly better than most existing query suggestion methods. According to the results in an additional experiment, we have also found that HSCM is superior to most ranking models in click prediction.},
  keywords = {Document ranking,query suggestion,self-attention mechanism},
  annotation = {ZSCC: 0000001}
}

@article{chenPersonalizedQuerySuggestion2018,
  title = {Personalized Query Suggestion Based on User Behavior},
  author = {Chen, Wanyu and Hao, Zepeng and Shao, Taihua and Chen, Honghui},
  date = {2018},
  journaltitle = {International Journal of Modern Physics C},
  volume = {29},
  number = {04},
  pages = {1850036},
  publisher = {{World Scientific}},
  annotation = {ZSCC: 0000005},
  file = {/home/johnlocke/.zotero/storage/9BMVCAN2/S0129183118500365.html}
}

@unpublished{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  options = {useprefix=true},
  date = {2014-09-02},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2019-12-10},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {ZSCC: 0007747},
  file = {/home/johnlocke/.zotero/storage/RKBDZXLR/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf}
}

@unpublished{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  date = {2014-12-11},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.3555},
  urldate = {2019-12-10},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0003882},
  file = {/home/johnlocke/.zotero/storage/3RX874QP/Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf}
}

@book{ComprendreSortirProcrastination,
  title = {Comprendre et {{Sortir}} de La {{Procrastination}}.Pdf},
  file = {/home/johnlocke/.zotero/storage/EAN25YA3/Comprendre et Sortir de la Procrastination.pdf}
}

@unpublished{correiaAdaptivelySparseTransformers2019,
  title = {Adaptively {{Sparse Transformers}}},
  author = {Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
  date = {2019-09-06},
  eprint = {1909.00015},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.00015},
  urldate = {2022-01-10},
  abstract = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with \$\textbackslash alpha\$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the \$\textbackslash alpha\$ parameter -- which controls the shape and sparsity of \$\textbackslash alpha\$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  annotation = {ZSCC: 0000103},
  file = {/home/johnlocke/.zotero/storage/LS8WZ5S3/Correia et al_2019_Adaptively Sparse Transformers.pdf;/home/johnlocke/.zotero/storage/GVGG86MH/1909.html}
}

@online{CurriculumReinforcementLearning2020,
  title = {Curriculum for {{Reinforcement Learning}}},
  date = {2020-01-29T18:00:00+00:00},
  url = {https://lilianweng.github.io/2020/01/29/curriculum-for-reinforcement-learning.html},
  urldate = {2020-02-24},
  abstract = {A curriculum is an efficient tool for humans to progressively learn from simple concepts to hard problems. It breaks down complex knowledge by providing a sequence of learning steps of increasing difficulty. In this post, we will examine how the idea of curriculum can help reinforcement learning models learn to...},
  langid = {english},
  organization = {{Lil'Log}},
  file = {/home/johnlocke/.zotero/storage/6V3YYL57/curriculum-for-reinforcement-learning.html}
}

@inproceedings{daiDeeperTextUnderstanding2019,
  ids = {daiDeeperTextUnderstanding2019a},
  title = {Deeper Text Understanding for {{IR}} with Contextual Neural Language Modeling},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Dai, Zhuyun and Callan, Jamie},
  date = {2019},
  pages = {985--988},
  annotation = {ZSCC: 0000033},
  file = {/home/johnlocke/.zotero/storage/5QUN23NS/Dai_Callan_2019_Deeper text understanding for IR with contextual neural language modeling.pdf;/home/johnlocke/.zotero/storage/IWPQUTJG/3331184.html}
}

@unpublished{dasSequencetoSetSemanticTagging2019,
  title = {Sequence-to-{{Set Semantic Tagging}}: {{End-to-End Multi-label Prediction}} Using {{Neural Attention}} for {{Complex Query Reformulation}} and {{Automated Text Categorization}}},
  shorttitle = {Sequence-to-{{Set Semantic Tagging}}},
  author = {Das, Manirupa and Li, Juanxi and Fosler-Lussier, Eric and Lin, Simon and Moosavinasab, Soheil and Rust, Steve and Huang, Yungui and Ramnath, Rajiv},
  date = {2019},
  eprint = {1911.04427},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/CY2M4XLT/Das et al_2019_Sequence-to-Set Semantic Tagging.pdf;/home/johnlocke/.zotero/storage/JJQGCF2S/1911.html}
}

@inproceedings{dasSequencetoSetSemanticTagging2020,
  title = {Sequence-to-{{Set Semantic Tagging}} for {{Complex Query Reformulation}} and {{Automated Text Categorization}} in {{Biomedical IR}} Using {{Self-Attention}}},
  booktitle = {Proceedings of the 19th {{SIGBioMed Workshop}} on {{Biomedical Language Processing}}},
  author = {Das, Manirupa and Li, Juanxi and Fosler-Lussier, Eric and Lin, Simon and Rust, Steve and Huang, Yungui and Ramnath, Rajiv},
  date = {2020},
  pages = {14--27},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/R5Q7FGTE/Das et al_2020_Sequence-to-Set Semantic Tagging for Complex Query Reformulation and Automated.pdf;/home/johnlocke/.zotero/storage/FGIFQHAZ/2020.bionlp-1.2.html}
}

@article{dauphinLanguageModelingGated2017,
  title = {Language {{Modeling}} with {{Gated Convolutional Networks}}},
  author = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  date = {2017},
  pages = {9},
  abstract = {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016b) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText103 benchmark, even though it features longterm dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.},
  langid = {english},
  annotation = {ZSCC: 0000688},
  file = {/home/johnlocke/.zotero/storage/HFREDFPN/Dauphin et al. - Language Modeling with Gated Convolutional Network.pdf}
}

@inproceedings{dayanFeudalReinforcementLearning1993,
  title = {Feudal Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dayan, Peter and Hinton, Geoffrey E.},
  date = {1993},
  pages = {271--278},
  annotation = {ZSCC: 0000584},
  file = {/home/johnlocke/.zotero/storage/66HANUHM/Dayan_Hinton_1993_Feudal reinforcement learning.pdf}
}

@article{deanSoftwareEngineeringAdvice2007,
  title = {Software Engineering Advice from Building Large-Scale Distributed Systems},
  author = {Dean, Jeff},
  date = {2007},
  journaltitle = {CS295 Lecture at Stanford University},
  volume = {1},
  number = {2.1},
  pages = {1--2},
  file = {/home/johnlocke/.zotero/storage/EURQS732/Dean_2007_Software engineering advice from building large-scale distributed systems.pdf}
}

@unpublished{dehghaniAvoidingYourTeacher2017,
  title = {Avoiding {{Your Teacher}}'s {{Mistakes}}: {{Training Neural Networks}} with {{Controlled Weak Supervision}}},
  shorttitle = {Avoiding {{Your Teacher}}'s {{Mistakes}}},
  author = {Dehghani, Mostafa and Severyn, Aliaksei and Rothe, Sascha and Kamps, Jaap},
  date = {2017-12-07},
  eprint = {1711.00313},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.00313},
  urldate = {2020-02-24},
  abstract = {Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels. This feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality. What if we could somehow inform the model about the label quality? In this paper, we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion: a "target network" and a "confidence network". The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to weight the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. We evaluate our learning strategy on two different tasks: document ranking and sentiment classification. The results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/F6AIDQWZ/Dehghani et al_2017_Avoiding Your Teacher's Mistakes.pdf;/home/johnlocke/.zotero/storage/7DGB4X3N/1711.html}
}

@unpublished{dehghaniFidelityWeightedLearning2018,
  title = {Fidelity-{{Weighted Learning}}},
  author = {Dehghani, Mostafa and Mehrjou, Arash and Gouws, Stephan and Kamps, Jaap and Schölkopf, Bernhard},
  date = {2018-05-23},
  eprint = {1711.02799},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.02799},
  urldate = {2020-02-24},
  abstract = {Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data? We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. To this end, we propose "fidelity-weighted learning" (FWL), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels). Both student and teacher are learned from the data. We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/johnlocke/.zotero/storage/AXJSHDW2/Dehghani et al_2018_Fidelity-Weighted Learning.pdf;/home/johnlocke/.zotero/storage/46GG6SRZ/1711.html}
}

@inproceedings{dehghaniLearningAttendCopy2017,
  title = {Learning to Attend, Copy, and Generate for Session-Based Query Suggestion},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Dehghani, Mostafa and Rothe, Sascha and Alfonseca, Enrique and Fleury, Pascal},
  date = {2017},
  pages = {1747--1756},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000033},
  file = {/home/johnlocke/.zotero/storage/HW3GDUJG/Dehghani et al_2017_Learning to attend, copy, and generate for session-based query suggestion.pdf;/home/johnlocke/.zotero/storage/BA4NCVGG/citation.html}
}

@article{dengDeepLearningMethods2014,
  title = {Deep {{Learning}}: {{Methods}} and {{Applications}}},
  shorttitle = {Deep {{Learning}}},
  author = {Deng, Li and Yu, Dong},
  date = {2014-06-30},
  journaltitle = {Foundations and Trends® in Signal Processing},
  shortjournal = {SIG},
  volume = {7},
  number = {3–4},
  pages = {197--387},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000039},
  url = {https://www.nowpublishers.com/article/Details/SIG-039},
  urldate = {2020-02-24},
  abstract = {Deep Learning: Methods and Applications},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/QCI333CT/Deng_Yu_2014_Deep Learning.pdf;/home/johnlocke/.zotero/storage/MDEEPANS/SIG-039.html}
}

@unpublished{denoyerDeepSequentialNeural2014,
  title = {Deep {{Sequential Neural Network}}},
  author = {Denoyer, Ludovic and Gallinari, Patrick},
  date = {2014-10-02},
  eprint = {1410.0510},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.0510},
  urldate = {2020-02-27},
  abstract = {Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000044},
  file = {/home/johnlocke/.zotero/storage/RXNS4N4M/Denoyer_Gallinari_2014_Deep Sequential Neural Network.pdf;/home/johnlocke/.zotero/storage/I3EVDTMT/1410.html}
}

@unpublished{devlinBERTPretrainingDeep2019,
  ids = {devlinBERTPretrainingDeep2019a,devlinBertPretrainingDeep2018},
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2019-12-10},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/EMV3LCF3/Devlin et al_2019_BERT.pdf;/home/johnlocke/.zotero/storage/4D53KHNC/1810.html;/home/johnlocke/.zotero/storage/DAI26AQM/1810.html}
}

@unpublished{diazQueryExpansionLocallytrained2016,
  ids = {diazQueryExpansionLocallytrained2016a},
  title = {Query Expansion with Locally-Trained Word Embeddings},
  author = {Diaz, Fernando and Mitra, Bhaskar and Craswell, Nick},
  date = {2016},
  eprint = {1605.07891},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000192},
  file = {/home/johnlocke/.zotero/storage/P6NUBGML/Diaz et al_2016_Query expansion with locally-trained word embeddings.pdf;/home/johnlocke/.zotero/storage/TUMY23JC/1605.html}
}

@unpublished{dingGeneratingHighQualityQuery2018,
  title = {Generating {{High-Quality Query Suggestion Candidates}} for {{Task-Based Search}}},
  author = {Ding, Heng and Zhang, Shuo and Garigliotti, Darío and Balog, Krisztian},
  date = {2018},
  volume = {10772},
  eprint = {1802.07997},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {625--631},
  doi = {10.1007/978-3-319-76941-7_54},
  url = {http://arxiv.org/abs/1802.07997},
  urldate = {2020-01-20},
  abstract = {We address the task of generating query suggestions for task-based search. The current state of the art relies heavily on suggestions provided by a major search engine. In this paper, we solve the task without reliance on search engines. Specifically, we focus on the first step of a two-stage pipeline approach, which is dedicated to the generation of query suggestion candidates. We present three methods for generating candidate suggestions and apply them on multiple information sources. Using a purpose-built test collection, we find that these methods are able to generate high-quality suggestion candidates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,H.3.3},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/QEGKK8UV/Ding et al_2018_Generating High-Quality Query Suggestion Candidates for Task-Based Search.pdf;/home/johnlocke/.zotero/storage/V6FKX8T6/1802.html}
}

@inproceedings{doetschFastRobustTraining2014,
  title = {Fast and Robust Training of Recurrent Neural Networks for Offline Handwriting Recognition},
  booktitle = {2014 14th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  author = {Doetsch, Patrick and Kozielski, Michal and Ney, Hermann},
  date = {2014},
  pages = {279--284},
  publisher = {{IEEE}},
  annotation = {ZSCC: 0000130},
  file = {/home/johnlocke/.zotero/storage/KUIUVQEM/Doetsch et al_2014_Fast and robust training of recurrent neural networks for offline handwriting.pdf;/home/johnlocke/.zotero/storage/65ZW4SNG/6981033.html}
}

@inproceedings{duanOnlineSpellingCorrection2011,
  title = {Online Spelling Correction for Query Completion},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web - {{WWW}} '11},
  author = {Duan, Huizhong and Hsu, Bo-June (Paul)},
  date = {2011},
  pages = {117},
  publisher = {{ACM Press}},
  location = {{Hyderabad, India}},
  doi = {10.1145/1963405.1963425},
  url = {http://portal.acm.org/citation.cfm?doid=1963405.1963425},
  urldate = {2020-02-07},
  abstract = {In this paper, we study the problem of online spelling correction for query completions. Misspelling is a common phenomenon among search engines queries. In order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. As latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient.},
  eventtitle = {The 20th International Conference},
  isbn = {978-1-4503-0632-4},
  langid = {english},
  annotation = {ZSCC: 0000108},
  file = {/home/johnlocke/.zotero/storage/KM2FQIXI/Duan et Hsu - 2011 - Online spelling correction for query completion.pdf}
}

@article{efthimiadisQueryExpansion1996,
  title = {Query {{Expansion}}.},
  author = {Efthimiadis, Efthimis N.},
  date = {1996},
  journaltitle = {Annual review of information science and technology (ARIST)},
  volume = {31},
  pages = {121--87},
  annotation = {ZSCC: 0000536},
  file = {/home/johnlocke/.zotero/storage/CKJIYR97/eric.ed.gov.html}
}

@inproceedings{eickhoffLessonsJourneyQuery2014,
  ids = {eickhoffLessonsJourneyQuery2014a},
  title = {Lessons from the Journey: A Query Log Analysis of within-Session Learning},
  shorttitle = {Lessons from the Journey},
  booktitle = {Proceedings of the 7th {{ACM}} International Conference on {{Web}} Search and Data Mining},
  author = {Eickhoff, Carsten and Teevan, Jaime and White, Ryen and Dumais, Susan},
  date = {2014},
  pages = {223--232},
  annotation = {ZSCC: 0000096},
  file = {/home/johnlocke/.zotero/storage/FZ5ZHNMJ/Eickhoff et al. - 2014 - Lessons from the journey a query log analysis of .pdf}
}

@online{EvolutionStrategies2019,
  title = {Evolution {{Strategies}}},
  date = {2019-09-05T12:00:00+00:00},
  url = {https://lilianweng.github.io/2019/09/05/evolution-strategies.html},
  urldate = {2020-02-24},
  abstract = {Gradient descent is not the only option when learning optimal model parameters. Evolution Strategies (ES) works out well in the cases where we don’t know the precise analytic form of an objective function or cannot compute the gradients directly. This post dives into several classic ES methods, as well as...},
  langid = {english},
  organization = {{Lil'Log}},
  file = {/home/johnlocke/.zotero/storage/3H8SIJNQ/evolution-strategies.html}
}

@thesis{F319064750Pdf,
  title = {F319064750.Pdf},
  url = {https://esc.fnwi.uva.nl/thesis/centraal/files/f319064750.pdf},
  urldate = {2020-02-20},
  file = {/home/johnlocke/.zotero/storage/H52989B8/f319064750.pdf}
}

@inproceedings{fangFormalStudyInformation2004,
  title = {A Formal Study of Information Retrieval Heuristics},
  booktitle = {Proceedings of the 27th Annual International Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '04},
  author = {Fang, Hui and Tao, Tao and Zhai, ChengXiang},
  date = {2004},
  pages = {49},
  publisher = {{ACM Press}},
  location = {{Sheffield, United Kingdom}},
  doi = {10.1145/1008992.1009004},
  url = {http://portal.acm.org/citation.cfm?doid=1008992.1009004},
  urldate = {2019-12-11},
  abstract = {Empirical studies of information retrieval methods show that good retrieval performance is closely related to the use of various retrieval heuristics, such as TF-IDF weighting. One basic research question is thus what exactly are these “necessary” heuristics that seem to cause good retrieval performance. In this paper, we present a formal study of retrieval heuristics. We formally define a set of basic desirable constraints that any reasonable retrieval function should satisfy, and check these constraints on a variety of representative retrieval functions. We find that none of these retrieval functions satisfies all the constraints unconditionally. Empirical results show that when a constraint is not satisfied, it often indicates non-optimality of the method, and when a constraint is satisfied only for a certain range of parameter values, its performance tends to be poor when the parameter is out of the range. In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations and make it possible to evaluate any existing or new retrieval formula analytically.},
  eventtitle = {The 27th Annual International Conference},
  isbn = {978-1-58113-881-8},
  langid = {english},
  annotation = {ZSCC: 0000333},
  file = {/home/johnlocke/.zotero/storage/9NDJAQB9/Fang et al_2004_A formal study of information retrieval heuristics.pdf}
}

@unpublished{fengSurveyDataAugmentation2021,
  title = {A {{Survey}} of {{Data Augmentation Approaches}} for {{NLP}}},
  author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
  date = {2021-05-07},
  eprint = {2105.03075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.03075},
  urldate = {2021-05-19},
  abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/XK53I5DF/Feng et al_2021_A Survey of Data Augmentation Approaches for NLP.pdf;/home/johnlocke/.zotero/storage/URD2S2GD/2105.html}
}

@article{freitagBeamSearchStrategies2017,
  title = {Beam {{Search Strategies}} for {{Neural Machine Translation}}},
  author = {Freitag, Markus and Al-Onaizan, Yaser},
  date = {2017},
  journaltitle = {Proceedings of the First Workshop on Neural Machine Translation},
  eprint = {1702.01806},
  eprinttype = {arxiv},
  pages = {56--60},
  doi = {10.18653/v1/W17-3207},
  url = {http://arxiv.org/abs/1702.01806},
  urldate = {2019-12-10},
  abstract = {The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-toright beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-toright while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43\% for the two language pairs German→English and Chinese→English without losing any translation quality.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/johnlocke/.zotero/storage/IPERG2P8/Freitag_Al-Onaizan_2017_Beam Search Strategies for Neural Machine Translation.pdf}
}

@article{gallierAlgebraTopologyDifferential,
  title = {Algebra, {{Topology}}, {{Diﬀerential Calculus}}, and {{Optimization Theory For Computer Science}} and {{Machine Learning}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  pages = {1962},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/9CTDNKEI/Gallier et Quaintance - Algebra, Topology, Diﬀerential Calculus, and Optim.pdf}
}

@article{gallierAlgebraTopologyDifferential2017,
  title = {Algebra, {{Topology}}, {{Differential Calculus}}, and {{Optimization Theory For Computer Science}} and {{Engineering}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  date = {2017},
  file = {/home/johnlocke/.zotero/storage/4XM6QJGG/Gallier_Quaintance_2017_Algebra, Topology, Differential Calculus, and Optimization Theory For Computer.pdf}
}

@unpublished{gargMultiresolutionTransformerNetworks2019,
  ids = {gargMultiresolutionTransformerNetworks2019a,gargMultiresolutionTransformerNetworks2019b},
  title = {Multiresolution {{Transformer Networks}}: {{Recurrence}} Is {{Not Essential}} for {{Modeling Hierarchical Structure}}},
  shorttitle = {Multiresolution {{Transformer Networks}}},
  author = {Garg, Vikas K. and Dhillon, Inderjit S. and Yu, Hsiang-Fu},
  date = {2019},
  eprint = {1908.10408},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/5DXCDYFN/Garg et al_2019_Multiresolution Transformer Networks.pdf;/home/johnlocke/.zotero/storage/IUHCC7FW/1908.html;/home/johnlocke/.zotero/storage/XABM4AMM/1908.html}
}

@article{garigliottiGeneratingQuerySuggestions2017,
  ids = {garigliottiGeneratingQuerySuggestions2017a},
  title = {Generating {{Query Suggestions}} to {{Support Task-Based Search}}},
  author = {Garigliotti, Darío and Balog, Krisztian},
  date = {2017},
  journaltitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '17},
  eprint = {1708.08289},
  eprinttype = {arxiv},
  pages = {1153--1156},
  doi = {10.1145/3077136.3080745},
  url = {http://arxiv.org/abs/1708.08289},
  urldate = {2020-01-20},
  abstract = {We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,H.3.3},
  annotation = {ZSCC: 0000004},
  file = {/home/johnlocke/.zotero/storage/9EJU4VQY/Garigliotti_Balog_2017_Generating Query Suggestions to Support Task-Based Search.pdf;/home/johnlocke/.zotero/storage/V9UX5PIH/Garigliotti_Balog_2017_Generating Query Suggestions to Support Task-Based Search.pdf}
}

@article{gayo-avelloSurveySessionDetection2009,
  title = {A Survey on Session Detection Methods in Query Logs and a Proposal for Future Evaluation},
  author = {Gayo-Avello, Daniel},
  date = {2009-05-30},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {179},
  number = {12},
  pages = {1822--1843},
  issn = {00200255},
  doi = {10.1016/j.ins.2009.01.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S002002550900053X},
  urldate = {2020-01-15},
  langid = {english},
  annotation = {ZSCC: 0000104},
  file = {/home/johnlocke/.zotero/storage/X3862FET/Gayo-Avello_2009_A survey on session detection methods in query logs and a proposal for future.pdf;/home/johnlocke/.zotero/storage/4VDZXQZ8/S002002550900053X.html}
}

@inproceedings{geryEvaluationWebUsage2003,
  title = {Evaluation of Web Usage Mining Approaches for User's next Request Prediction},
  booktitle = {Proceedings of the 5th {{ACM}} International Workshop on {{Web}} Information and Data Management},
  author = {Géry, Mathias and Haddad, Hatem},
  date = {2003},
  pages = {74--81},
  annotation = {ZSCC: 0000154},
  file = {/home/johnlocke/.zotero/storage/KENZ8MSG/Gery et Haddad - Mathias Ge´ ry, Hatem Haddad.pdf;/home/johnlocke/.zotero/storage/C44PFRRK/956699.html}
}

@article{geryMathiasGeRy,
  title = {Mathias {{Ge}}´ Ry, {{Hatem Haddad}}},
  author = {Gery, Mathias and Haddad, Ext-Hatem},
  pages = {8},
  abstract = {Analysis of Web server logs is one of the important challenge to provide Web intelligent services. In this paper, we describe a framework for a recommender system that predicts the user’s next requests based on their behaviour discovered from Web Logs data. We compare results from three usage mining approaches: association rules, sequential rules and generalised sequential rules. We use two selection rules criteria: highest confidence and lastsubsequence. Experiments are performed on three collections of real usage data: one from an Intranet Web site and two from an Internet Web site.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{gokerContextInformationRetrieval2009,
  title = {Context and {{Information Retrieval}}},
  author = {Göker, Ayşe and Myrhaug, Hans and Bierig, Ralf},
  date = {2009-10-23},
  journaltitle = {Information Retrieval},
  series = {Wiley {{Online Books}}},
  pages = {131--157},
  issn = {9780470033647},
  doi = {10.1002/9780470033647.ch7},
  url = {https://onlinelibrary-wiley-com-s.docadis.ups-tlse.fr/doi/10.1002/9780470033647.ch7},
  urldate = {2020-01-27},
  abstract = {Summary This chapter contains sections titled: Introduction What is Context? Context in Information Retrieval Context Modelling and Representation Context and Content Related Topics Evaluating Context-aware IR Systems Summary Exercises References},
  keywords = {context in information retrieval,context modelling and representation,evaluating context-aware IR systems,perceptions of context in related fields,personalisation and context,principles of methodology,representation of context,scaling up participant number - formative to summative evaluation,searching with context information,user models and context relationship},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/johnlocke/.zotero/storage/Z5SHADKY/9780470033647.html}
}

@book{gokerInformationRetrievalSearching2009,
  title = {Information Retrieval: Searching in the 21st Century},
  shorttitle = {Information Retrieval},
  editor = {Goker, Ayse and Davies, J.},
  date = {2009},
  publisher = {{Wiley}},
  location = {{Chichester, U.K}},
  isbn = {978-0-470-02762-2},
  langid = {english},
  pagetotal = {295},
  keywords = {Information retrieval},
  annotation = {ZSCC: 0000107  OCLC: ocn424454743},
  file = {/home/johnlocke/.zotero/storage/WGBI3EDJ/Goker et Davies - 2009 - Information retrieval searching in the 21st centu.pdf}
}

@book{golubMatrixComputations2013,
  ids = {golubMatrixComputations2012},
  title = {Matrix Computations},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  date = {2013},
  series = {Johns {{Hopkins}} Studies in the Mathematical Sciences},
  edition = {Fourth edition},
  publisher = {{The Johns Hopkins University Press}},
  location = {{Baltimore}},
  isbn = {978-1-4214-0794-4},
  langid = {english},
  pagetotal = {756},
  keywords = {Data processing,Matrices},
  annotation = {ZSCC: 0000014  OCLC: ocn824733531},
  file = {/home/johnlocke/.zotero/storage/4YCZK49T/Golub et Van Loan - 2013 - Matrix computations.pdf;/home/johnlocke/.zotero/storage/AZIX5SK5/books.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT press}},
  annotation = {ZSCC: 0013150},
  file = {/home/johnlocke/.zotero/storage/RZCBLDME/Goodfellow et al_2016_Deep learning.pdf;/home/johnlocke/.zotero/storage/MZTNIKPL/books.html}
}

@unpublished{goodfellowMaxoutNetworks2013,
  title = {Maxout Networks},
  author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  date = {2013},
  eprint = {1302.4389},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0001908},
  file = {/home/johnlocke/.zotero/storage/GJ32CM93/Goodfellow et al_2013_Maxout networks.pdf}
}

@unpublished{gravesNeuralTuringMachines2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date = {2014-12-10},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.5401},
  urldate = {2020-02-23},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0001293},
  file = {/home/johnlocke/.zotero/storage/9862ES8X/Graves et al_2014_Neural Turing Machines.pdf;/home/johnlocke/.zotero/storage/DVJ68YGU/1410.html}
}

@article{greffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  date = {2017},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {28},
  number = {10},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  pages = {2222--2232},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2582924},
  url = {http://arxiv.org/abs/1503.04069},
  urldate = {2019-12-10},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (≈ 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T10,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/572TMDDB/Greff et al_2017_LSTM.pdf}
}

@inproceedings{groverNode2vecScalableFeature2016,
  title = {Node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle = {Node2vec},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Grover, Aditya and Leskovec, Jure},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {855--864},
  publisher = {{Association for Computing Machinery}},
  location = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939754},
  url = {https://doi.org/10.1145/2939672.2939754},
  urldate = {2020-02-21},
  abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
  isbn = {978-1-4503-4232-2},
  keywords = {feature learning,graph representations,information networks,node embeddings},
  annotation = {ZSCC: 0002639},
  file = {/home/johnlocke/.zotero/storage/R8VRBWQN/Grover_Leskovec_2016_node2vec.pdf}
}

@inproceedings{guanUtilizingQueryChange2013,
  ids = {guanUtilizingQueryChange2013a},
  title = {Utilizing Query Change for Session Search},
  booktitle = {Proceedings of the 36th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval - {{SIGIR}} '13},
  author = {Guan, Dongyi and Zhang, Sicong and Yang, Hui},
  date = {2013},
  pages = {453},
  publisher = {{ACM Press}},
  location = {{Dublin, Ireland}},
  doi = {10.1145/2484028.2484055},
  url = {http://dl.acm.org/citation.cfm?doid=2484028.2484055},
  urldate = {2020-01-31},
  abstract = {Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent’s actions are query changes that we observe and the search agent’s actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.},
  eventtitle = {The 36th International {{ACM SIGIR}} Conference},
  isbn = {978-1-4503-2034-4},
  langid = {english},
  annotation = {ZSCC: 0000082},
  file = {/home/johnlocke/.zotero/storage/2H79P9K2/Guan et al. - 2013 - Utilizing query change for session search.pdf;/home/johnlocke/.zotero/storage/GZMSUUXU/Guan et al. - 2013 - Utilizing query change for session search.pdf;/home/johnlocke/.zotero/storage/LERRES7V/2484028.html}
}

@unpublished{guIncorporatingCopyingMechanism2016,
  title = {Incorporating {{Copying Mechanism}} in {{Sequence-to-Sequence Learning}}},
  author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.},
  date = {2016-06-08},
  eprint = {1603.06393},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1603.06393},
  urldate = {2019-12-10},
  abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure. COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET. For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/johnlocke/.zotero/storage/5GH6TDP4/Gu et al_2016_Incorporating Copying Mechanism in Sequence-to-Sequence Learning.pdf}
}

@inproceedings{guoDeepRelevanceMatching2016,
  title = {A {{Deep Relevance Matching Model}} for {{Ad-hoc Retrieval}}},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Guo, Jiafeng and Fan, Yixing and Ai, Qingyao and Croft, W. Bruce},
  date = {2016-10-24},
  series = {{{CIKM}} '16},
  pages = {55--64},
  publisher = {{Association for Computing Machinery}},
  location = {{Indianapolis, Indiana, USA}},
  doi = {10.1145/2983323.2983769},
  url = {https://doi.org/10.1145/2983323.2983769},
  urldate = {2020-02-23},
  abstract = {In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.},
  isbn = {978-1-4503-4073-1},
  keywords = {ad-hoc retrieval,neural models,ranking models,relevance matching,semantic matching},
  annotation = {ZSCC: 0000313},
  file = {/home/johnlocke/.zotero/storage/Z82W6QMK/Guo et al_2016_A Deep Relevance Matching Model for Ad-hoc Retrieval.pdf}
}

@inproceedings{guoLongTextGeneration2018,
  title = {Long Text Generation via Adversarial Training with Leaked Information},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Guo, Jiaxian and Lu, Sidi and Cai, Han and Zhang, Weinan and Yu, Yong and Wang, Jun},
  date = {2018},
  volume = {32},
  number = {1},
  file = {/home/johnlocke/.zotero/storage/5ELU4QXZ/Guo et al_2018_Long text generation via adversarial training with leaked information.pdf;/home/johnlocke/.zotero/storage/NYLNXAYA/11957.html}
}

@inproceedings{guoUnifiedDiscriminativeModel2008,
  title = {A Unified and Discriminative Model for Query Refinement},
  booktitle = {Proceedings of the 31st Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Guo, Jiafeng and Xu, Gu and Li, Hang and Cheng, Xueqi},
  date = {2008},
  pages = {379--386},
  annotation = {ZSCC: 0000143},
  file = {/home/johnlocke/.zotero/storage/MZ99YU9V/Guo et al. - A Uniﬁed and Discriminative Model for Query Reﬁnem.pdf;/home/johnlocke/.zotero/storage/85XWWVTG/1390334.html}
}

@unpublished{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  date = {2020-05-05},
  eprint = {2004.10964},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.10964},
  urldate = {2020-07-09},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000007},
  file = {/home/johnlocke/.zotero/storage/E7G444SE/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf}
}

@inproceedings{hagenQuerySessionDetection2011,
  title = {Query Session Detection as a Cascade},
  booktitle = {Proceedings of the 20th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Hagen, Matthias and Stein, Benno and Rüb, Tino},
  date = {2011},
  pages = {147--152},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000021},
  file = {/home/johnlocke/.zotero/storage/YN5E5TZM/Hagen et al_2011_Query session detection as a cascade.pdf;/home/johnlocke/.zotero/storage/MPFTMW7A/2063576.html}
}

@inproceedings{hagenSearchSessionDetection2013,
  title = {From Search Session Detection to Search Mission Detection},
  booktitle = {Proceedings of the 10th {{Conference}} on {{Open Research Areas}} in {{Information Retrieval}}},
  author = {Hagen, Matthias and Gomoll, Jakob and Beyer, Anna and Stein, Benno},
  date = {2013},
  pages = {85--92},
  abstract = {Search mission detection aims at identifying those queries a user submits for the same information need. Such knowledge offers interesting insights into behavioral usage patterns and often can help to better support a user. However, most existing query log studies focus on search sessions only (consecutive queries for the same need) and ignore multitasking behavior (interleaved information needs) as well as hierarchies of short-term search goals in multiple sessions that form a long-term search task such as vacation planning. To better understand the dialog between user and search engine we distinguish between (1) physical search sessions, characterized by the time gap between queries, (2) logical search sessions, characterized by consecutive queries for the same information need within a physical session, and (3) search missions, characterized by logical sessions, multitasking behavior, and hierarchical goals.},
  annotation = {ZSCC: 0000025},
  file = {/home/johnlocke/.zotero/storage/CQ7WWJHI/Hagen et al_2013_From search session detection to search mission detection.pdf}
}

@inproceedings{hanInferringSearchQueries2019,
  ids = {hanInferringSearchQueries2019a},
  title = {Inferring {{Search Queries}} from {{Web Documents}} via a {{Graph-Augmented Sequence}} to {{Attention Network}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Han, Fred X. and Niu, Di and Lai, Kunfeng and Guo, Weidong and He, Yancheng and Xu, Yu},
  date = {2019},
  pages = {2792--2798},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/M7VX8TSV/Han et al_2019_Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to.pdf;/home/johnlocke/.zotero/storage/5BXX2U46/3308558.html}
}

@unpublished{hardFederatedLearningMobile2019,
  ids = {hardFederatedLearningMobile2018},
  title = {Federated {{Learning}} for {{Mobile Keyboard Prediction}}},
  author = {Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Françoise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chloé and Ramage, Daniel},
  date = {2019-02-28},
  eprint = {1811.03604},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.03604},
  urldate = {2020-02-07},
  abstract = {We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the FederatedAveraging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/BJNFKDNY/Hard et al. - 2019 - Federated Learning for Mobile Keyboard Prediction.pdf;/home/johnlocke/.zotero/storage/UG2NVIPZ/1811.html}
}

@inproceedings{hassanawadallahSupportingComplexSearch2014,
  ids = {hassanawadallahSupportingComplexSearch2014a},
  title = {Supporting Complex Search Tasks},
  booktitle = {Proceedings of the 23rd {{ACM International Conference}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Hassan Awadallah, Ahmed and White, Ryen W. and Pantel, Patrick and Dumais, Susan T. and Wang, Yi-Min},
  date = {2014},
  pages = {829--838},
  annotation = {ZSCC: 0000047},
  file = {/home/johnlocke/.zotero/storage/2YLLZXK2/Hassan Awadallah et al. - 2014 - Supporting Complex Search Tasks.pdf;/home/johnlocke/.zotero/storage/9GH9AHC8/2661829.html}
}

@unpublished{heBagTricksImage2018,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  date = {2018-12-05},
  eprint = {1812.01187},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.01187},
  urldate = {2021-10-06},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/DDFXSXY5/He et al_2018_Bag of Tricks for Image Classification with Convolutional Neural Networks.pdf;/home/johnlocke/.zotero/storage/6AUVDVDT/1812.html}
}

@inproceedings{heUserBehaviourTask2017,
  title = {User Behaviour and Task Characteristics: {{A}} Field Study of Daily Information Behaviour},
  shorttitle = {User Behaviour and Task Characteristics},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Conference Human Information Interaction}} and {{Retrieval}}},
  author = {He, Jiyin and Yilmaz, Emine},
  date = {2017},
  pages = {67--76},
  annotation = {ZSCC: 0000012},
  file = {/home/johnlocke/.zotero/storage/Z42G7G9N/He_Yilmaz_2017_User behaviour and task characteristics.pdf}
}

@inproceedings{heWebQueryRecommendation2009,
  title = {Web Query Recommendation via Sequential Query Prediction},
  booktitle = {2009 {{IEEE}} 25th International Conference on Data Engineering},
  author = {He, Qi and Jiang, Daxin and Liao, Zhen and Hoi, Steven CH and Chang, Kuiyu and Lim, Ee-Peng and Li, Hang},
  date = {2009},
  pages = {1443--1454},
  publisher = {{IEEE}},
  annotation = {ZSCC: 0000115},
  file = {/home/johnlocke/.zotero/storage/AREDYBR6/He et al_2009_Web query recommendation via sequential query prediction.pdf;/home/johnlocke/.zotero/storage/A49KZSP5/4812545.html}
}

@article{hidasiRecurrentNeuralNetworks2018,
  title = {Recurrent {{Neural Networks}} with {{Top-k Gains}} for {{Session-based Recommendations}}},
  author = {Hidasi, Balázs and Karatzoglou, Alexandros},
  date = {2018},
  journaltitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management - CIKM '18},
  eprint = {1706.03847},
  eprinttype = {arxiv},
  pages = {843--852},
  doi = {10.1145/3269206.3271761},
  url = {http://arxiv.org/abs/1706.03847},
  urldate = {2020-02-18},
  abstract = {RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an sessionbased manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35\% in terms of MRR and Recall@20 over previous sessionbased RNN solutions and up to 53\% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {ZSCC: 0000104},
  file = {/home/johnlocke/.zotero/storage/HWIEU4X5/Hidasi et Karatzoglou - 2018 - Recurrent Neural Networks with Top-k Gains for Ses.pdf}
}

@unpublished{hidasiSessionbasedRecommendationsRecurrent2016,
  title = {Session-Based {{Recommendations}} with {{Recurrent Neural Networks}}},
  author = {Hidasi, Balázs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
  date = {2016-03-29},
  eprint = {1511.06939},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1511.06939},
  urldate = {2020-02-18},
  abstract = {We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000008},
  file = {/home/johnlocke/.zotero/storage/MH48C4S4/Hidasi et al_2016_Session-based Recommendations with Recurrent Neural Networks.pdf;/home/johnlocke/.zotero/storage/GHCWY678/1511.html}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997},
  journaltitle = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  annotation = {ZSCC: 0025836},
  file = {/home/johnlocke/.zotero/storage/3CLSJ8XX/Hochreiter_Schmidhuber_1997_Long short-term memory.pdf;/home/johnlocke/.zotero/storage/RMQKNA3G/neco.1997.9.8.html}
}

@article{holscherWebSearchBehavior2000,
  title = {Web Search Behavior of {{Internet}} Experts and Newbies},
  author = {Hölscher, Christoph and Strube, Gerhard},
  date = {2000-06-01},
  journaltitle = {Computer Networks},
  shortjournal = {Computer Networks},
  volume = {33},
  number = {1},
  pages = {337--346},
  issn = {1389-1286},
  doi = {10.1016/S1389-1286(00)00031-1},
  url = {http://www.sciencedirect.com/science/article/pii/S1389128600000311},
  urldate = {2020-03-01},
  abstract = {Searching for relevant information on the World Wide Web is often a laborious and frustrating task for casual and experienced users. To help improve searching on the Web based on a better understanding of user characteristics, we investigate what types of knowledge are relevant for Web-based information seeking, and which knowledge structures and strategies are involved. Two experimental studies are presented, which address these questions from different angles and with different methodologies. In the first experiment, 12 established Internet experts are first interviewed about search strategies and then perform a series of realistic search tasks on the World Wide Web. From this study a model of information seeking on the World Wide Web is derived and then tested in a second study. In the second experiment two types of potentially relevant types of knowledge are compared directly. Effects of Web experience and domain-specific background knowledge are investigated with a series of search tasks in an economics-related domain (introduction of the Euro currency). We find differential and combined effects of both Web experience and domain knowledge: while successful search performance requires the combination of the two types of expertise, specific strategies directly related to Web experience or domain knowledge can be identified.},
  langid = {english},
  keywords = {Expertise,Information retrieval,Internet search engines,Logfile analysis},
  file = {/home/johnlocke/.zotero/storage/ZGENYIXU/Hölscher_Strube_2000_Web search behavior of Internet experts and newbies.pdf;/home/johnlocke/.zotero/storage/42CA3GAW/S1389128600000311.html}
}

@unpublished{howardUniversalLanguageModel2018,
  title = {Universal Language Model Fine-Tuning for Text Classification},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0001866},
  file = {/home/johnlocke/.zotero/storage/34YFWA93/Howard_Ruder_2018_Universal language model fine-tuning for text classification.pdf;/home/johnlocke/.zotero/storage/246KSTFC/1801.html}
}

@inproceedings{huangAnalyzingEvaluatingQuery2009,
  ids = {huangAnalyzingEvaluatingQuery2009a,huangAnalyzingEvaluatingQuery2009b},
  title = {Analyzing and Evaluating Query Reformulation Strategies in Web Search Logs},
  booktitle = {Proceedings of the 18th {{ACM}} Conference on {{Information}} and Knowledge Management},
  author = {Huang, Jeff and Efthimiadis, Efthimis N.},
  date = {2009},
  pages = {77--86},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000273},
  file = {/home/johnlocke/.zotero/storage/Y8XAASCU/Huang_Efthimiadis_2009_Analyzing and evaluating query reformulation strategies in web search logs.pdf;/home/johnlocke/.zotero/storage/2HHW3TH8/1645953.html;/home/johnlocke/.zotero/storage/DPCD272Y/1645953.html}
}

@unpublished{huangDenselyConnectedConvolutional2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  options = {useprefix=true},
  date = {2018-01-28},
  eprint = {1608.06993},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1608.06993},
  urldate = {2020-02-17},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/ZH6KVQTX/Huang et al_2018_Densely Connected Convolutional Networks.pdf;/home/johnlocke/.zotero/storage/GY9LQCP7/1608.html}
}

@inproceedings{huangImprovingEntityRecommendation2018,
  title = {Improving {{Entity Recommendation}} with {{Search Log}} and {{Multi-Task Learning}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Jizhou and Zhang, Wei and Sun, Yaming and Wang, Haifeng and Liu, Ting},
  date = {2018-07},
  pages = {4107--4114},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/571},
  url = {https://www.ijcai.org/proceedings/2018/571},
  urldate = {2020-02-21},
  abstract = {Entity recommendation, providing search users with an improved experience by assisting them in finding related entities for a given query, has become an indispensable feature of today’s Web search engine. Existing studies typically only consider the query issued at the current time step while ignoring the in-session preceding queries. Thus, they typically fail to handle the ambiguous queries such as “apple” because the model could not understand which apple (company or fruit) is talked about. In this work, we believe that the in-session contexts convey valuable evidences that could facilitate the semantic modeling of queries, and take that into consideration for entity recommendation. Furthermore, in order to better model the semantics of queries, we learn the model in a multi-task learning setting where the query representation is shared across entity recommendation and contextaware ranking. We evaluate our approach using large-scale, real-world search logs of a widely used commercial Web search engine. The experimental results show that incorporating context information significantly improves entity recommendation, and learning the model in a multi-task learning setting could bring further improvements.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-18}}\vphantom\{\}},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  annotation = {ZSCC: 0000006},
  file = {/home/johnlocke/.zotero/storage/EM8JRD6N/Huang et al. - 2018 - Improving Entity Recommendation with Search Log an.pdf}
}

@inproceedings{huangLearningDeepStructured2013,
  title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Information}} \& {{Knowledge Management}}},
  author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
  date = {2013-10-27},
  series = {{{CIKM}} '13},
  pages = {2333--2338},
  publisher = {{Association for Computing Machinery}},
  location = {{San Francisco, California, USA}},
  doi = {10.1145/2505515.2505665},
  url = {https://doi.org/10.1145/2505515.2505665},
  urldate = {2020-02-23},
  abstract = {Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
  isbn = {978-1-4503-2263-8},
  keywords = {clickthrough data,deep learning,semantic model,web search},
  annotation = {ZSCC: 0000909},
  file = {/home/johnlocke/.zotero/storage/A5JCTVHQ/Huang et al_2013_Learning deep structured semantic models for web search using clickthrough data.pdf}
}

@inproceedings{huConvolutionalNeuralNetwork2014,
  title = {Convolutional Neural Network Architectures for Matching Natural Language Sentences},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
  date = {2014-12-08},
  series = {{{NIPS}}'14},
  pages = {2042--2050},
  publisher = {{MIT Press}},
  location = {{Montreal, Canada}},
  abstract = {Semantic matching is of central importance to many natural language tasks [2,28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.},
  annotation = {ZSCC: 0000784}
}

@article{iqbalProductionRankingSystems2019,
  title = {Production {{Ranking Systems}}: {{A Review}}},
  author = {Iqbal, Murium and Subedi, Nishan and Aryafar, Kamelia},
  date = {2019},
  pages = {10},
  abstract = {The problem of ranking is a multi-billion dollar problem. In this paper we present an overview of several production quality ranking systems. We show that due to conflicting goals of employing the most effective machine learning models and responding to users in real time, ranking systems have evolved into a system of systems, where each subsystem can be viewed as a component layer. We view these layers as being data processing, representation learning, candidate selection and online inference. Each layer employs different algorithms and tools, with every end-to-end ranking system spanning multiple architectures. Our goal is to familiarize the general audience with a working knowledge of ranking at scale, the tools and algorithms employed and the challenges introduced by adopting a layered approach.},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/J5MFNB6B/Iqbal et al_2019_Production Ranking Systems.pdf}
}

@thesis{irainPlateformeAnalysePerformances2019,
  type = {These de doctorat},
  title = {Plateforme d'analyse de Performances Des Méthodes de Localisation Des Données Dans Le Cloud Basées Sur l'apprentissage Automatique Exploitant Des Délais de Messages},
  author = {Irain, Malik},
  date = {2019-12-17},
  institution = {{Toulouse 3}},
  url = {https://theses.fr/2019TOU30195},
  urldate = {2021-10-14},
  abstract = {L'utilisation du cloud est une nécessité aujourd'hui, les données produites et utilisées par tous les types d'utilisateurs (individus particuliers, entreprises, structures administratives) ayant atteint une masse trop importante pour être stockées autrement. L'utilisation du cloud nécessite la signature, explicite ou non, d'un contrat avec un fournisseur de service de stockage. Ce contrat mentionne les niveaux de qualité de service requis selon différents critères. Parmi ces critères se trouve la localisation des données. Cependant, ce critère n'est pas facilement vérifiable par un utilisateur. C'est pour cela que la recherche dans le domaine de la vérification de localisation de données a suscité plusieurs travaux depuis quelques années, mais les solutions proposées restent encore perfectibles. Le travail proposé dans le cadre de cette thèse consiste à étudier les solutions de vérification de localisation par les clients, c'est-à-dire les solutions estimant la localisation des données et fonctionnant à l'aide de points de repère. L'approche à investiguer peut être résumée comme suit : en exploitant les délais de communication et en utilisant des modèles de temps de traversée du réseau, estimer, avec une certaine erreur de distance, la localisation des données. Pour cela, le travail réalisé est le suivant : • Une revue de l'état de l'art des différentes méthodes permettant aux utilisateurs de connaitre la localisation de leurs données. • La conception d'une notation unifiée pour les méthodes étudiées dans la revue de l'état de l'art, avec une proposition de deux scores pour évaluer et comparer les méthodes. • La mise en place d'une plateforme de collecte de mesures réseau. Grâce à cette plateforme, deux jeux de données ont été récoltés, un au niveau national et l'autre un niveau mondial. Ces deux jeux de données permettent d'évaluer les différentes méthodes présentées dans la revue de l'état de l'art. • La mise en place d'une architecture d'évaluation à partir des deux jeux de données et des scores définis, afin d'établir la qualité des méthodes (taux de succès) et la qualité des résultats (précision du résultat) grâce aux scores proposés.},
  editora = {Mammeri, Zoubir and Jorda, Jacques},
  editoratype = {collaborator},
  keywords = {Analyse de performance,Analyse des données,Apprentissage automatique,Cloud,Data location,Informatique dans les nuages,Localisation des données,Machine learning,Performance analysis,Qualité de service,Quality of service},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/4E528JSU/Iraïn - Plateforme d'analyse de performances des méthodes .pdf}
}

@unpublished{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2019-02-25},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.05407},
  urldate = {2019-12-10},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/ILCZFDYV/Izmailov et al_2019_Averaging Weights Leads to Wider Optima and Better Generalization.pdf}
}

@article{jaechMatchTensorDeepRelevance2017,
  ids = {jaechMatchtensorDeepRelevance2017,jaechMatchtensorDeepRelevance2017a},
  title = {Match-{{Tensor}}: A {{Deep Relevance Model}} for {{Search}}},
  shorttitle = {Match-{{Tensor}}},
  author = {Jaech, Aaron and Kamisetty, Hetunandan and Ringger, Eric and Clarke, Charlie},
  date = {2017-01-26},
  url = {https://arxiv.org/abs/1701.07795v1},
  urldate = {2020-02-24},
  abstract = {The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.},
  langid = {english},
  annotation = {ZSCC: 0000013},
  file = {/home/johnlocke/.zotero/storage/JS39PGM2/Jaech et al_2017_Match-Tensor.pdf;/home/johnlocke/.zotero/storage/GI8JF6NB/1701.html;/home/johnlocke/.zotero/storage/LX33BDJV/1701.html}
}

@inproceedings{jainSynthesizingHighUtility2011,
  title = {Synthesizing High Utility Suggestions for Rare Web Search Queries},
  booktitle = {Proceedings of the 34th International {{ACM SIGIR}} Conference on {{Research}} and Development in {{Information Retrieval}}},
  author = {Jain, Alpa and Ozertem, Umut and Velipasaoglu, Emre},
  date = {2011},
  pages = {805--814},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000031},
  file = {/home/johnlocke/.zotero/storage/VN9RPMSB/2009916.html}
}

@article{jansenDefiningSessionWeb2007,
  ids = {jansenDefiningSessionWeb2007a,jansenDefiningSessionWeb2007c},
  title = {Defining a Session on {{Web}} Search Engines},
  author = {Jansen, Bernard J. and Spink, Amanda and Blakely, Chris and Koshman, Sherry},
  date = {2007},
  journaltitle = {Journal of the American Society for Information Science and Technology},
  volume = {58},
  number = {6},
  pages = {862--871},
  publisher = {{Wiley Online Library}},
  annotation = {ZSCC: 0000203},
  file = {/home/johnlocke/.zotero/storage/HGNKAMFN/Jansen et al_2007_Defining a session on Web search engines.pdf;/home/johnlocke/.zotero/storage/47LZF26W/asi.html;/home/johnlocke/.zotero/storage/M3CFPBXK/asi.html;/home/johnlocke/.zotero/storage/N6MPPP57/asi.html}
}

@article{jansenHowAreWe2006,
  title = {How Are We Searching the {{World Wide Web}}? {{A}} Comparison of Nine Search Engine Transaction Logs},
  shorttitle = {How Are We Searching the {{World Wide Web}}?},
  author = {Jansen, Bernard J. and Spink, Amanda},
  date = {2006-01-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  series = {Formal {{Methods}} for {{Information Retrieval}}},
  volume = {42},
  number = {1},
  pages = {248--263},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2004.10.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0306457304001396},
  urldate = {2020-03-01},
  abstract = {The Web and especially major Web search engines are essential tools in the quest to locate online information for many people. This paper reports results from research that examines characteristics and changes in Web searching from nine studies of five Web search engines based in the US and Europe. We compare interactions occurring between users and Web search engines from the perspectives of session length, query length, query complexity, and content viewed among the Web search engines. The results of our research shows (1) users are viewing fewer result pages, (2) searchers on US-based Web search engines use more query operators than searchers on European-based search engines, (3) there are statistically significant differences in the use of Boolean operators and result pages viewed, and (4) one cannot necessary apply results from studies of one particular Web search engine to another Web search engine. The wide spread use of Web search engines, employment of simple queries, and decreased viewing of result pages may have resulted from algorithmic enhancements by Web search engine companies. We discuss the implications of the findings for the development of Web search engines and design of online content.},
  langid = {english},
  keywords = {Transaction log analysis,Web search engines,Web searching},
  file = {/home/johnlocke/.zotero/storage/CP5WQRC7/Jansen_Spink_2006_How are we searching the World Wide Web.pdf;/home/johnlocke/.zotero/storage/M2RXE43F/S0306457304001396.html}
}

@article{jansenMethodologicalApproachDiscovering2000,
  ids = {jansenMethodologicalApproachDiscovering2000a,jansenMethodologicalApproachDiscovering2005},
  title = {Methodological Approach in Discovering User Search Patterns through {{Web}} Log Analysis},
  author = {Jansen, Bernard J. and Spink, Amanda},
  date = {2000},
  journaltitle = {Bulletin of the American Society for Information Science and Technology},
  volume = {27},
  number = {1},
  pages = {15--17},
  publisher = {{Wiley Online Library}},
  file = {/home/johnlocke/.zotero/storage/XNFRNNEM/Jansen_Spink_2000_Methodological approach in discovering user search patterns through Web log.pdf;/home/johnlocke/.zotero/storage/F63SVPDC/bult.html;/home/johnlocke/.zotero/storage/IH27K8FR/bult.html}
}

@inproceedings{jansenQueryModificationsPatterns2007,
  title = {Query Modifications Patterns during Web Searching},
  booktitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  author = {Jansen, Bernard J. and Spink, Amanda and Narayan, Bhuva},
  date = {2007},
  pages = {439--444},
  publisher = {{IEEE}},
  annotation = {ZSCC: 0000028},
  file = {/home/johnlocke/.zotero/storage/8P2QM4V6/Jansen et al_2007_Query modifications patterns during web searching.pdf;/home/johnlocke/.zotero/storage/MHSNWVWI/4151723.html}
}

@inproceedings{jiangLearningUserReformulation2014,
  ids = {jiangLearningUserReformulation2014a},
  title = {Learning User Reformulation Behavior for Query Auto-Completion},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval},
  author = {Jiang, Jyun-Yu and Ke, Yen-Yu and Chien, Pao-Yu and Cheng, Pu-Jen},
  date = {2014-07-03},
  series = {{{SIGIR}} '14},
  pages = {445--454},
  publisher = {{Association for Computing Machinery}},
  location = {{Gold Coast, Queensland, Australia}},
  doi = {10.1145/2600428.2609614},
  url = {https://doi.org/10.1145/2600428.2609614},
  urldate = {2020-02-22},
  abstract = {It is crucial for query auto-completion to accurately predict what a user is typing. Given a query prefix and its context (e.g., previous queries), conventional context-aware approaches often produce relevant queries to the context. The purpose of this paper is to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance. We first conduct an in-depth analysis of how the users reformulate their queries. Based on the analysis, we propose a supervised approach to query auto-completion, where three kinds of reformulation-related features are considered, including term-level, query-level and session-level features. These features carefully capture how the users change preceding queries along the query sessions. Extensive experiments have been conducted on the large-scale query log of a commercial search engine. The experimental results demonstrate a significant improvement over 4 competitive baselines.},
  isbn = {978-1-4503-2257-7},
  keywords = {algorithms,experimentation,performance},
  annotation = {ZSCC: 0000082},
  file = {/home/johnlocke/.zotero/storage/JFQERWSH/Jiang et al_2014_Learning user reformulation behavior for query auto-completion.pdf}
}

@inproceedings{jiangRINReformulationInference2018,
  ids = {jiangRINReformulationInference2018a},
  title = {{{RIN}}: {{Reformulation Inference Network}} for {{Context-Aware Query Suggestion}}},
  shorttitle = {{{RIN}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}} - {{CIKM}} '18},
  author = {Jiang, Jyun-Yu and Wang, Wei},
  date = {2018},
  pages = {197--206},
  publisher = {{ACM Press}},
  location = {{Torino, Italy}},
  doi = {10.1145/3269206.3271808},
  url = {http://dl.acm.org/citation.cfm?doid=3269206.3271808},
  urldate = {2020-02-07},
  abstract = {Search engine users always endeavor to reformulate queries during search sessions for articulating their information needs because it is not always easy to articulate the search intents. To further ameliorate the reformulation process, search engines may provide some query suggestions based on previous queries. In this paper, we propose Reformulation Inference Network (RIN) to learn how users reformulate queries, thereby benefiting context-aware query suggestion. Instead of categorizing reformulations into predefined patterns, we represent queries and reformulations in a homomorphic hidden space through heterogeneous network embedding. To capture the structure of the session context, a recurrent neural network (RNN) with the attention mechanism is employed to encode the search session by reading the homomorphic query and reformulation embeddings. It enables the model to explicitly captures the former reformulation for each query in the search session and directly learn user reformulation behaviors, from which query suggestion may benefit as shown in previous studies. To generate query suggestions, a binary classifier and an RNN-based decoder are introduced as the query discriminator and the query generator. Inspired by the intuition that model accurately predicting the next reformulation can also correctly infer the next intended query, a reformulation inferencer is then designed for inferring the next reformulation in the latent space of homomorphic embeddings. Therefore, both question suggestion and reformulation prediction can be simultaneously optimized by multi-task learning. Extensive experiments are conducted on publicly available AOL search engine logs. The experimental results demonstrate that RIN outperforms competitive baselines across various situations for both discriminative and generative tasks of context-aware query suggestion.},
  eventtitle = {The 27th {{ACM International Conference}}},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  annotation = {ZSCC: 0000007},
  file = {/home/johnlocke/.zotero/storage/3FVRKD8E/Jiang et Wang - 2018 - RIN Reformulation Inference Network for Context-A.pdf;/home/johnlocke/.zotero/storage/6WNKLJZH/Jiang_Wang_2018_RIN.pdf;/home/johnlocke/.zotero/storage/4M37BRCP/3269206.html}
}

@inproceedings{jonesSessionTimeoutAutomatic2008,
  ids = {jonesSessionTimeoutAutomatic2008a},
  title = {Beyond the Session Timeout: Automatic Hierarchical Segmentation of Search Topics in Query Logs},
  shorttitle = {Beyond the Session Timeout},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Information}} and Knowledge Management},
  author = {Jones, Rosie and Klinkner, Kristina Lisa},
  date = {2008},
  pages = {699--708},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000447},
  file = {/home/johnlocke/.zotero/storage/Y3LBD852/Jones_Klinkner_2008_Beyond the session timeout.pdf;/home/johnlocke/.zotero/storage/BE8TF6FE/1458082.html}
}

@article{jordanHierarchicalMixturesExperts1994,
  title = {Hierarchical {{Mixtures}} of {{Experts}} and the {{EM Algorithm}}},
  author = {Jordan, Michael I. and Jacobs, Robert A.},
  date = {1994-03-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {6},
  number = {2},
  pages = {181--214},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.2.181},
  url = {https://doi.org/10.1162/neco.1994.6.2.181},
  urldate = {2020-02-27},
  abstract = {We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain.},
  annotation = {ZSCC: 0003395},
  file = {/home/johnlocke/.zotero/storage/5VN8KBM3/Jordan_Jacobs_1994_Hierarchical Mixtures of Experts and the EM Algorithm.pdf;/home/johnlocke/.zotero/storage/FIVWBD57/neco.1994.6.2.html}
}

@unpublished{joulinBagTricksEfficient2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  date = {2016-08-09},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1607.01759},
  urldate = {2020-02-27},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0001547},
  file = {/home/johnlocke/.zotero/storage/UB798J5K/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf}
}

@unpublished{joulinFastTextZipCompressing2016,
  title = {{{FastText}}.Zip: {{Compressing}} Text Classification Models},
  shorttitle = {{{FastText}}.Zip},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and Jégou, Hérve and Mikolov, Tomas},
  date = {2016-12-12},
  eprint = {1612.03651},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1612.03651},
  urldate = {2020-02-27},
  abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000227},
  file = {/home/johnlocke/.zotero/storage/C9IX597A/Joulin et al. - 2016 - FastText.zip Compressing text classification mode.pdf}
}

@article{jozefowiczEmpiricalExplorationRecurrent2015,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  date = {2015},
  pages = {9},
  abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear.},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/ANPERKJC/Jozefowicz et al_2015_An Empirical Exploration of Recurrent Network Architectures.pdf}
}

@article{jRealLifeInformation1998,
  title = {Real Life Information Retrieval: A Study of User Queries on the {{Web}}},
  shorttitle = {Real Life Information Retrieval},
  author = {J, JansenBernard and SpinkAmanda and BatemanJudy and SaracevicTefko},
  date = {1998-04-01},
  journaltitle = {ACM SIGIR Forum},
  publisher = {{ACM}},
  url = {https://dl.acm.org/doi/abs/10.1145/281250.281253},
  urldate = {2020-02-28},
  abstract = {We analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of Excite, a major Internet search service. We provide data on: (i) queries --- the number of search terms, and the use...},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  		PUB27 		New York, NY, USA},
  file = {/home/johnlocke/.zotero/storage/AGNS93PA/J et al_1998_Real life information retrieval.pdf;/home/johnlocke/.zotero/storage/A7SDQUMW/281250.html}
}

@article{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement Learning: {{A}} Survey},
  shorttitle = {Reinforcement Learning},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  date = {1996},
  journaltitle = {Journal of artificial intelligence research},
  volume = {4},
  pages = {237--285},
  annotation = {ZSCC: 0007264},
  file = {/home/johnlocke/.zotero/storage/ZWIKGDT7/Kaelbling et al_1996_Reinforcement learning.pdf}
}

@inproceedings{kaiDealingOutofvocabularyWords1998,
  title = {Dealing with Out-of-Vocabulary Words and Speech Disfluencies in an n-Gram Based Speech Understanding System},
  booktitle = {Fifth {{International Conference}} on {{Spoken Language Processing}}},
  author = {Kai, Atsuhiko and Hirose, Yoshifumi and Nakagawa, Seiichi},
  date = {1998},
  annotation = {ZSCC: 0000032},
  file = {/home/johnlocke/.zotero/storage/I4ZD9JAH/Kai et al_1998_Dealing with out-of-vocabulary words and speech disfluencies in an n-gram based.pdf;/home/johnlocke/.zotero/storage/VZS2PE3P/i98_0785.html}
}

@unpublished{kaleSemanticQuerySegmentation2017,
  title = {Towards {{Semantic Query Segmentation}}},
  author = {Kale, Ajinkya and Taula, Thrivikrama and Hewavitharana, Sanjika and Srivastava, Amit},
  date = {2017-07-25},
  eprint = {1707.07835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.07835},
  urldate = {2020-02-23},
  abstract = {Query Segmentation is one of the critical components for understanding users' search intent in Information Retrieval tasks. It involves grouping tokens in the search query into meaningful phrases which help downstream tasks like search relevance and query understanding. In this paper, we propose a novel approach to segment user queries using distributed query embeddings. Our key contribution is a supervised approach to the segmentation task using low-dimensional feature vectors for queries, getting rid of traditional hand tuned and heuristic NLP features which are quite expensive. We benchmark on a 50,000 human-annotated web search engine query corpus achieving comparable accuracy to state-of-the-art techniques. The advantage of our technique is its fast and does not use external knowledge-base like Wikipedia for score boosting. This helps us generalize our approach to other domains like eCommerce without any fine-tuning. We demonstrate the effectiveness of this method on another 50,000 human-annotated eCommerce query corpus from eBay search logs. Our approach is easy to implement and generalizes well across different search domains proving the power of low-dimensional embeddings in query segmentation task, opening up a new direction of research for this problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {ZSCC: 0000004},
  file = {/home/johnlocke/.zotero/storage/LQILJQL5/Kale et al_2017_Towards Semantic Query Segmentation.pdf;/home/johnlocke/.zotero/storage/DNQZCPG6/1707.html}
}

@inproceedings{katoStructuredQuerySuggestion2012,
  title = {Structured Query Suggestion for Specialization and Parallel Movement: Effect on Search Behaviors},
  shorttitle = {Structured Query Suggestion for Specialization and Parallel Movement},
  booktitle = {Proceedings of the 21st International Conference on {{World Wide Web}}},
  author = {Kato, Makoto P. and Sakai, Tetsuya and Tanaka, Katsumi},
  date = {2012},
  pages = {389--398},
  annotation = {ZSCC: 0000038},
  file = {/home/johnlocke/.zotero/storage/FUZGIC8W/download.pdf;/home/johnlocke/.zotero/storage/UL6BUGJK/2187836.html}
}

@online{kayHistoricalThesaurusOxford2009,
  type = {Edited Books},
  title = {Historical {{Thesaurus}} of the {{Oxford English Dictionary}}},
  author = {Kay, C. and Roberts, J. and Samuels, M. and Wotherspoon, I.},
  date = {2009},
  url = {http://eprints.gla.ac.uk/52128/},
  urldate = {2019-12-17},
  abstract = {A 40-year project in the making, the Historical Thesaurus of the Oxford English Dictionary is the first historical thesaurus to include almost the entire vocabulary of English, from Old English to the present day. Conceived and compiled by the Department of English Language of the University of Glasgow, the Historical Thesaurus of the Oxford English Dictionary is a groundbreaking analysis of the historical inventory of English, allowing users to find words connected in meaning throughout the history of the language. · The largest thesaurus resource in the world, covering more than 920,000 words and meanings, based on the Oxford English Dictionary · The very first historical thesaurus to be compiled for any of the world's languages · Synonyms listed with dates of first recorded use in English, in chronological order, with earliest synonyms first · For obsolete words, the Thesaurus also includes last recorded use of word · Uses a specially devised thematic system of classification · Comprehensive index enables complete cross-referencing of nearly one million words and meanings · Contains a comprehensive sense inventory of Old English · Includes a free fold-out color chart which shows the top levels of the classification structure · Made up of two volumes: The main text, comprising numbers sections for semantic categories, and the index, comprising a full A-Z look up of nearly one million lexical items The Historical Thesaurus of the Oxford English Dictionary is a unique resource for word-lovers of all types-linguists and language specialists, historians, literary commentators, among others-as well as being a fascinating resource for everyone with an interest in the English language and its historical development. It is a perfect complement to the OED itself, allowing the words in the OED to be cross-referenced and viewed in wholly new ways.},
  langid = {british},
  annotation = {ZSCC: 0000129},
  file = {/home/johnlocke/.zotero/storage/92T5PN24/52128.html}
}

@unpublished{kidgerNeuralDifferentialEquations2022,
  title = {On {{Neural Differential Equations}}},
  author = {Kidger, Patrick},
  date = {2022-02-04},
  eprint = {2202.02435},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2202.02435},
  urldate = {2022-02-11},
  abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/UKQIWAV8/Kidger_2022_On Neural Differential Equations.pdf;/home/johnlocke/.zotero/storage/R74WYZDR/2202.html}
}

@article{kingmaVariationalInferenceDeep2017,
  title = {Variational Inference \& Deep Learning: {{A}} New Synthesis},
  shorttitle = {Variational Inference \& Deep Learning},
  author = {Kingma, Diederik Pieter},
  date = {2017},
  annotation = {ZSCC: 0000034},
  file = {/home/johnlocke/.zotero/storage/XS6DXQ2J/Kingma_2017_Variational inference & deep learning.pdf;/home/johnlocke/.zotero/storage/IW6WIML8/oaidare.uva.html}
}

@inproceedings{kohStrategiesSupportingQuery2013,
  title = {The Strategies for Supporting Query Specialization and Query Generalization in Social Tagging Systems},
  booktitle = {International {{Conference}} on {{Database Systems}} for {{Advanced Applications}}},
  author = {Koh, Jia-Ling and Chiang, Kuang-Ting and Chiu, I.-Chih},
  date = {2013},
  pages = {164--178},
  publisher = {{Springer}},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/UZDNNPWK/Koh et al_2013_The strategies for supporting query specialization and query generalization in.pdf;/home/johnlocke/.zotero/storage/IDMTFUHX/978-3-642-40270-8_14.html}
}

@article{kongMaxmarginActionPrediction2015,
  title = {Max-Margin Action Prediction Machine},
  author = {Kong, Yu and Fu, Yun},
  date = {2015},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {38},
  number = {9},
  pages = {1844--1858},
  publisher = {{IEEE}},
  annotation = {ZSCC: 0000065},
  file = {/home/johnlocke/.zotero/storage/H664MGK8/7299663.html}
}

@inproceedings{kumarCharacterizationOnlineBrowsing2010,
  ids = {kumarCharacterizationOnlineBrowsing2010a},
  title = {A Characterization of Online Browsing Behavior},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Kumar, Ravi and Tomkins, Andrew},
  date = {2010},
  pages = {561--570},
  keywords = {browsing,pageviews,toolbar analysis},
  annotation = {ZSCC: 0000235},
  file = {/home/johnlocke/.zotero/storage/493EFXCD/Kumar_Tomkins_2010_A characterization of online browsing behavior.pdf;/home/johnlocke/.zotero/storage/DCBSPYKR/1772690.html}
}

@inproceedings{kustarevSessionbasedQueryPerformance2012,
  ids = {kustarevSessionbasedQueryPerformance2012a,kustarevSessionbasedQueryPerformance2012b},
  title = {Session-Based Query Performance Prediction},
  booktitle = {Proceedings of the 21st {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Kustarev, Andrey and Ustinovskiy, Yury and Mazur, Anna and Serdyukov, Pavel},
  date = {2012-10-29},
  series = {{{CIKM}} '12},
  pages = {2563--2566},
  publisher = {{Association for Computing Machinery}},
  location = {{Maui, Hawaii, USA}},
  doi = {10.1145/2396761.2398692},
  url = {https://doi.org/10.1145/2396761.2398692},
  urldate = {2020-01-27},
  abstract = {Search sessions are known to be a rich source of diverse valuable information for individual query analysis. In this paper, we address the problem of query performance prediction by utilizing the entire logical search sessions containing the given query. Guided by the intuitions based on the observations made after the analysis of the search sessions' properties and performance of the queries they contain, we propose a number of features that significantly advance the existing query performance prediction models. Some of them specifically allow to focus on tail queries with sparse click-through statistics.},
  isbn = {978-1-4503-1156-4},
  keywords = {query flow graph,query performance prediction,user sessions},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/7GTWIIJR/Kustarev et al. - 2012 - Session-based query performance prediction.pdf;/home/johnlocke/.zotero/storage/XJZI9P25/2396761.html}
}

@inproceedings{larkinLessonsNRCPortage2010,
  title = {Lessons from {{NRC}}'s {{Portage}} System at {{WMT}} 2010},
  booktitle = {Proceedings of the {{Joint Fifth Workshop}} on {{Statistical Machine Translation}} and {{MetricsMATR}}},
  author = {Larkin, Samuel and Chen, Boxing and Foster, George and Germann, Ulrich and Joanis, Eric and Johnson, Howard and Kuhn, Roland},
  date = {2010},
  pages = {127--132},
  publisher = {{Association for Computational Linguistics}},
  annotation = {ZSCC: 0000011},
  file = {/home/johnlocke/.zotero/storage/9I9MYSRQ/1868850.html}
}

@incollection{lauPatternsSearchAnalyzing1999,
  ids = {lauPatternsSearchAnalyzing1999a},
  title = {Patterns of Search: Analyzing and Modeling Web Query Refinement},
  shorttitle = {Patterns of Search},
  booktitle = {{{UM99}} User Modeling},
  author = {Lau, Tessa and Horvitz, Eric},
  date = {1999},
  pages = {119--128},
  publisher = {{Springer}},
  annotation = {ZSCC: 0000357},
  file = {/home/johnlocke/.zotero/storage/78GD76YP/Lau_Horvitz_1999_Patterns of search.pdf;/home/johnlocke/.zotero/storage/FE9CCY6D/Lau_Horvitz_1999_Patterns of search.pdf;/home/johnlocke/.zotero/storage/7Z99LYHA/978-3-7091-2490-1_12.html;/home/johnlocke/.zotero/storage/HFMH3FIW/978-3-7091-2490-1_12.html}
}

@inproceedings{lavieSignificanceRecallAutomatic2004,
  title = {The {{Significance}} of {{Recall}} in {{Automatic Metrics}} for {{MT Evaluation}}},
  booktitle = {Machine {{Translation}}: {{From Real Users}} to {{Research}}},
  author = {Lavie, Alon and Sagae, Kenji and Jayaraman, Shyamsundar},
  editor = {Frederking, Robert E. and Taylor, Kathryn B.},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {134--143},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30194-3_16},
  abstract = {Recent research has shown that a balanced harmonic mean (F1 measure) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality. We show that significantly better correlations can be achieved by placing more weight on recall than on precision. While this may seem unexpected, since BLEU and NIST focus on n-gram precision and disregard recall, our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall. We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST.},
  isbn = {978-3-540-30194-3},
  langid = {english},
  keywords = {Bleu Score,Human Judgment,Machine Translation,Sentence Level,Word Error Rate},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/9MENJ4KX/Lavie et al. - 2004 - The Significance of Recall in Automatic Metrics fo.pdf}
}

@online{LearningExtractCrosssession,
  title = {Learning to Extract Cross-Session Search Tasks | {{Proceedings}} of the 22nd International Conference on {{World Wide Web}}},
  url = {https://dl.acm.org/doi/abs/10.1145/2488388.2488507},
  urldate = {2020-02-17},
  file = {/home/johnlocke/.zotero/storage/NT9SRVR5/2488388.html}
}

@unpublished{lewisBartDenoisingSequencetosequence2019,
  title = {Bart: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  shorttitle = {Bart},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000472},
  file = {/home/johnlocke/.zotero/storage/LEBUQEP2/Lewis et al_2019_Bart.pdf;/home/johnlocke/.zotero/storage/UGL37663/1910.html}
}

@incollection{liaoQuerySuggestion2020,
  title = {Query {{Suggestion}}},
  booktitle = {Query {{Understanding}} for {{Search Engines}}},
  author = {Liao, Zhen and Song, Yang and Zhou, Dengyong},
  editor = {Chang, Yi and Deng, Hongbo},
  date = {2020},
  series = {The {{Information Retrieval Series}}},
  pages = {171--203},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58334-7_8},
  url = {https://doi.org/10.1007/978-3-030-58334-7_8},
  urldate = {2021-03-11},
  abstract = {Query suggestion is one of the few fundamental problems in Web search. It assists users to refine queries in order to satisfy their information needs. Many query suggestion techniques have been proposed in the past decades. The mainstream idea is to leverage query logs which contain the search behaviors of users to generate useful query suggestions. In this chapter, we introduce several log-based query suggestion techniques. These methods fall into four categories: (1) query co-occurrence; (2) query-URL bipartite graph; (3) query transition graph; and (4) short-term search context. We also briefly discuss other related work in this field and point out several future directions.},
  isbn = {978-3-030-58334-7},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/XKWTFRVB/978-3-030-58334-7_8.html}
}

@inproceedings{liClickFeedbackAwareQuery2019,
  ids = {liClickFeedbackawareQuery2019},
  title = {Click {{Feedback-Aware Query Recommendation Using Adversarial Examples}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Li, Ruirui and Li, Liangda and Wu, Xian and Zhou, Yunhong and Wang, Wei},
  date = {2019-05-13},
  series = {{{WWW}} '19},
  pages = {2978--2984},
  publisher = {{Association for Computing Machinery}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1145/3308558.3313412},
  url = {https://doi.org/10.1145/3308558.3313412},
  urldate = {2020-02-23},
  abstract = {Search engine users always endeavor to formulate proper search queries during online search. To help users accurately express their information need during search, search engines are equipped with query suggestions to refine users' follow-up search queries. The success of a query suggestion system counts on whether we can understand and model user search intent accurately. In this work, we propose Click Feedback-Aware Network (CFAN) to provide feedback-aware query suggestions. In addition to modeling sequential search queries issued by a user, CFAN also considers user clicks on previous suggested queries as the user feedback. These clicked suggestions, together with the issued search query sequence, jointly capture the underlying search intent of users. In addition, we explicitly focus on improving the robustness of the query suggestion system through adversarial training. Adversarial examples are introduced into the training of the query suggestion system, which not only improves the robustness of system to nuisance perturbations, but also enhances the generalization performance for original training data. Extensive experiments are conducted on a recent real search engine log. The experimental results demonstrate that the proposed method, CFAN, outperforms competitive baseline methods across various situations on the task of query suggestion.},
  isbn = {978-1-4503-6674-8},
  keywords = {adversarial example,context-aware,Query recommendation},
  file = {/home/johnlocke/.zotero/storage/NDFPZ95H/Li et al_2019_Click Feedback-Aware Query Recommendation Using Adversarial Examples.pdf;/home/johnlocke/.zotero/storage/SXDS6BKB/3308558.html}
}

@patent{liDistributionalSimilaritybasedModels2009,
  type = {patent},
  title = {Distributional Similarity-Based Models for Query Correction},
  author = {Li, Mu and Zhou, Ming},
  date = {2009-09-15},
  annotation = {ZSCC: 0000064},
  file = {/home/johnlocke/.zotero/storage/QINKLR7K/Li_Zhou_2009_Distributional similarity-based models for query correction.pdf;/home/johnlocke/.zotero/storage/NIQUDD7A/en.html}
}

@article{liEfficientTypeaheadSearch,
  title = {Efficient Type-Ahead Search on Relational Data: A {{TASTIER}} Approach},
  author = {Li, Guoliang and Ji, Shengyue and Li, Chen and Feng, Jianhua},
  pages = {12},
  abstract = {Existing keyword-search systems in relational databases require users to submit a complete query to compute answers. Often users feel “left in the dark” when they have limited knowledge about the data, and have to use a try-and-see method to modify queries and find answers. In this paper we propose a novel approach to keyword search in the relational world, called Tastier. A Tastier system can bring instant gratification to users by supporting type-ahead search, which finds answers “on the fly” as the user types in query keywords. A main challenge is how to achieve a high interactive speed for large amounts of data in multiple tables, so that a query can be answered efficiently within milliseconds. We propose efficient index structures and algorithms for finding relevant answers on-the-fly by joining tuples in the database. We devise a partition-based method to improve query performance by grouping relevant tuples and pruning irrelevant tuples efficiently. We also develop a technique to answer a query efficiently by predicting highly relevant complete queries for the user. We have conducted a thorough experimental evaluation of the proposed techniques on real data sets to demonstrate the efficiency and practicality of this new search paradigm.},
  langid = {english},
  annotation = {ZSCC: 0000139},
  file = {/home/johnlocke/.zotero/storage/UH2U6MJL/Li et al. - Efficient type-ahead search on relational data a .pdf}
}

@inproceedings{liIdentifyingLabelingSearch2014,
  title = {Identifying and Labeling Search Tasks via Query-Based Hawkes Processes},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '14},
  author = {Li, Liangda and Deng, Hongbo and Dong, Anlei and Chang, Yi and Zha, Hongyuan},
  date = {2014},
  pages = {731--740},
  publisher = {{ACM Press}},
  location = {{New York, New York, USA}},
  doi = {10.1145/2623330.2623679},
  url = {http://dl.acm.org/citation.cfm?doid=2623330.2623679},
  urldate = {2019-12-10},
  abstract = {We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.},
  eventtitle = {The 20th {{ACM SIGKDD}} International Conference},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/CUFJEE6F/Li et al_2014_Identifying and labeling search tasks via query-based hawkes processes.pdf}
}

@unpublished{liInvolutionInvertingInherence2021,
  title = {Involution: {{Inverting}} the {{Inherence}} of {{Convolution}} for {{Visual Recognition}}},
  shorttitle = {Involution},
  author = {Li, Duo and Hu, Jie and Wang, Changhu and Li, Xiangtai and She, Qi and Zhu, Lei and Zhang, Tong and Chen, Qifeng},
  date = {2021-04-11},
  eprint = {2103.06255},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.06255},
  urldate = {2021-05-09},
  abstract = {Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6\% top-1 accuracy, 2.5\% and 2.4\% bounding box AP, and 4.7\% mean IoU absolutely while compressing the computational cost to 66\%, 65\%, 72\%, and 57\% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at https://github.com/d-li14/involution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/I2KF7W95/Li et al_2021_Involution.pdf;/home/johnlocke/.zotero/storage/D2K8QHSQ/2103.html}
}

@unpublished{liOptimizingInteractiveSystems2020,
  title = {Optimizing {{Interactive Systems}} via {{Data-Driven Objectives}}},
  author = {Li, Ziming and Kiseleva, Julia and Agarwal, Alekh and de Rijke, Maarten and White, Ryen W.},
  options = {useprefix=true},
  date = {2020},
  eprint = {2006.12999},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/JPSDLA7I/Li et al_2020_Optimizing Interactive Systems via Data-Driven Objectives.pdf;/home/johnlocke/.zotero/storage/WFDZ5QFS/2006.html}
}

@patent{liQuerySpellingCorrection2009,
  type = {patentus},
  title = {Query Spelling Correction},
  author = {Li, Mu},
  holder = {{Microsoft Corp}},
  date = {2009-03-26},
  number = {20090083255A1},
  url = {https://patents.google.com/patent/US20090083255A1/en},
  urldate = {2020-02-24},
  keywords = {correction,query,search results,web,web search},
  annotation = {ZSCC: 0000018},
  file = {/home/johnlocke/.zotero/storage/HEWBEUS4/Li_2009_Query spelling correction.pdf}
}

@article{liuLearningRankInformation2009,
  title = {Learning to Rank for Information Retrieval},
  author = {Liu, Tie-Yan},
  date = {2009},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  volume = {3},
  number = {3},
  pages = {225--331},
  annotation = {ZSCC: 0002558},
  file = {/home/johnlocke/.zotero/storage/FUI8KST5/Liu_2009_Learning to rank for information retrieval.pdf;/home/johnlocke/.zotero/storage/XCDPHYBQ/INR-016.html}
}

@inproceedings{liuNeuralQueryExpansion2019,
  title = {Neural Query Expansion for Code Search},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}},
  author = {Liu, Jason and Kim, Seohyun and Murali, Vijayaraghavan and Chaudhuri, Swarat and Chandra, Satish},
  date = {2019-06-22},
  series = {{{MAPL}} 2019},
  pages = {29--37},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3315508.3329975},
  url = {https://doi.org/10.1145/3315508.3329975},
  urldate = {2021-02-22},
  abstract = {Searching repositories of existing source code for code snippets is a key task in software engineering. Over the years, many approaches to this problem have been proposed. One recent tool called NCS, takes in a natural language query and outputs relevant code snippets, often being able to correctly answer Stack Overflow questions. But what happens when the developer doesn’t provide a query with a clear intent? What if shorter queries are used to demonstrate a more vague intent? We find that the performance of NCS regresses with shorter queries. Furthermore, data from developers’ code search history logs shows that shorter queries have a less successful code search session: there are more query reformulations and more time is spent browsing the results. These observations lead us to believe that using NCS alone with short queries may not be productive enough. In this paper, we explore an additional way of using neural networks in code search: the automatic expansion of queries. We present NQE, a neural model that takes in a set of keywords and predicts a set of keywords to expand the query to NCS. NQE learns to predict keywords that co-occur with the query keywords in the underlying corpus, which helps expand the query in a productive way. Our results show that with query expansion, NQE + NCS is able to perform better than using NCS alone.},
  isbn = {978-1-4503-6719-6},
  keywords = {code search,deep learning,word-embedding},
  annotation = {ZSCC: 0000007}
}

@inproceedings{longFullyConvolutionalNetworks2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015},
  pages = {3431--3440},
  annotation = {ZSCC: 0014558},
  file = {/home/johnlocke/.zotero/storage/WIYS25G6/Long et al_2015_Fully convolutional networks for semantic segmentation.pdf;/home/johnlocke/.zotero/storage/LVC4TR8K/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@inproceedings{luccheseIdentifyingTaskbasedSessions2011,
  ids = {luccheseIdentifyingTaskbasedSessions2011a},
  title = {Identifying Task-Based Sessions in Search Engine Query Logs},
  booktitle = {Proceedings of the Fourth {{ACM}} International Conference on {{Web}} Search and Data Mining - {{WSDM}} '11},
  author = {Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele and Silvestri, Fabrizio and Tolomei, Gabriele},
  date = {2011},
  pages = {277},
  publisher = {{ACM Press}},
  location = {{Hong Kong, China}},
  doi = {10.1145/1935826.1935875},
  url = {http://portal.acm.org/citation.cfm?doid=1935826.1935875},
  urldate = {2019-12-10},
  abstract = {The research challenge addressed in this paper is to devise e↵ective techniques for identifying task-based sessions, i.e. sets of possibly non contiguous queries issued by the user of a Web Search Engine for carrying out a given task. In order to evaluate and compare di↵erent approaches, we built, by means of a manual labeling process, a ground-truth where the queries of a given query log have been grouped in tasks. Our analysis of this ground-truth shows that users tend to perform more than one task at the same time, since about 75\% of the submitted queries involve a multi-tasking activity. We formally define the Task-based Session Discovery Problem (TSDP) as the problem of best approximating the manually annotated tasks, and we propose several variants of well known clustering algorithms, as well as a novel e cient heuristic algorithm, specifically tuned for solving the TSDP. These algorithms also exploit the collaborative knowledge collected by Wiktionary and Wikipedia for detecting query pairs that are not similar from a lexical content point of view, but actually semantically related. The proposed algorithms have been evaluated on the above groundtruth, and are shown to perform better than state-of-the-art approaches, because they e↵ectively take into account the multi-tasking behavior of users.},
  eventtitle = {The Fourth {{ACM}} International Conference},
  isbn = {978-1-4503-0493-1},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/GX7J4YDK/Lucchese et al_2011_Identifying task-based sessions in search engine query logs.pdf;/home/johnlocke/.zotero/storage/NQB5IS8L/1935826.html}
}

@inproceedings{lugoExtractingSearchTasks2021,
  title = {Extracting {{Search Tasks}} from {{Query Logs Using}} a {{Recurrent Deep Clustering Architecture}}},
  booktitle = {European {{Conference}} on {{Information Retrieval}}},
  author = {Lugo, Luis and Moreno, Jose G. and Hubert, Gilles},
  date = {2021},
  pages = {391--404},
  publisher = {{Springer}},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/6WT3NAV5/Lugo et al. - 2021 - Extracting Search Tasks from Query Logs Using a Re.pdf;/home/johnlocke/.zotero/storage/B67D4L3Z/978-3-030-72113-8_26.html;/home/johnlocke/.zotero/storage/SUJCP8A9/hal-03195934.html}
}

@inproceedings{lugoModelingUserSearch2021,
  title = {Modeling {{User Search Tasks}} with a {{Language-Agnostic Unsupervised Approach}}},
  booktitle = {European {{Conference}} on {{Information Retrieval}}},
  author = {Lugo, Luis and Moreno, Jose G. and Hubert, Gilles},
  date = {2021},
  pages = {405--418},
  publisher = {{Springer}},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/I9NZ5FGB/Lugo et al. - 2021 - Modeling User Search Tasks with a Language-Agnosti.pdf;/home/johnlocke/.zotero/storage/4W9QMFP4/978-3-030-72113-8_27.html;/home/johnlocke/.zotero/storage/JJTILF3N/hal-03195952.html}
}

@inproceedings{lugoMultilingualApproachUnsupervised2020,
  title = {A {{Multilingual Approach}} for {{Unsupervised Search Task Identification}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Lugo, Luis and Moreno, Jose G. and Hubert, Gilles},
  date = {2020},
  pages = {2041--2044},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/JWJQ8NR8/Lugo et al_2020_A Multilingual Approach for Unsupervised Search Task Identification.pdf;/home/johnlocke/.zotero/storage/7TDMUI4R/3397271.html}
}

@inproceedings{lugoSegmentingSearchQuery2020,
  title = {Segmenting {{Search Query Logs}} by {{Learning}} to {{Detect Search Task Boundaries}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Lugo, Luis and Moreno, Jose G. and Hubert, Gilles},
  date = {2020},
  pages = {2037--2040},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/G8W6J3RE/Lugo et al_2020_Segmenting Search Query Logs by Learning to Detect Search Task Boundaries.pdf;/home/johnlocke/.zotero/storage/MP44S2RF/3397271.html}
}

@incollection{luoDesigningStatesActions2015,
  title = {Designing {{States}}, {{Actions}}, and {{Rewards}} for {{Using POMDP}} in {{Session Search}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Luo, Jiyun and Zhang, Sicong and Dong, Xuchu and Yang, Hui},
  editor = {Hanbury, Allan and Kazai, Gabriella and Rauber, Andreas and Fuhr, Norbert},
  date = {2015},
  volume = {9022},
  pages = {526--537},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-16354-3_58},
  url = {http://link.springer.com/10.1007/978-3-319-16354-3_58},
  urldate = {2019-12-10},
  abstract = {Session search is an information retrieval task that involves a sequence of queries for a complex information need. It is characterized by rich user-system interactions and temporal dependency between queries and between consecutive user behaviors. Recent efforts have been made in modeling session search using the Partially Observable Markov Decision Process (POMDP). To best utilize the POMDP model, it is crucial to find suitable definitions for its fundamental elements – States, Actions and Rewards. This paper investigates the best ways to design the states, actions, and rewards within a POMDP framework. We lay out available design options of these major components based on a variety of related work and experiment on combinations of these options over the TREC 2012 \& 2013 Session datasets. We report our findings based on two evaluation aspects, retrieval accuracy and efficiency, and recommend practical design choices for using POMDP in session search.},
  isbn = {978-3-319-16353-6 978-3-319-16354-3},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/K6VX2ZMP/Luo et al_2015_Designing States, Actions, and Rewards for Using POMDP in Session Search.pdf}
}

@inproceedings{luoLearningReinforceSearch2015,
  title = {Learning to {{Reinforce Search Effectiveness}}},
  booktitle = {Proceedings of the 2015 {{International Conference}} on {{Theory}} of {{Information Retrieval}} - {{ICTIR}} '15},
  author = {Luo, Jiyun and Dong, Xuchu and Yang, Hui},
  date = {2015},
  pages = {271--280},
  publisher = {{ACM Press}},
  location = {{Northampton, Massachusetts, USA}},
  doi = {10.1145/2808194.2809468},
  url = {http://dl.acm.org/citation.cfm?doid=2808194.2809468},
  urldate = {2019-12-10},
  abstract = {Session search is an Information Retrieval (IR) task which handles a series of queries issued for a search task. In this paper, we propose a novel reinforcement learning style information retrieval framework and develop a new feedback learning algorithm to model user feedback, including clicks and query reformulations, as reinforcement signals and to generate rewards in the RL framework. From a new perspective, we view session search as a cooperative game played between two agents, the user and the search engine. We study the communications between the two agents; they always exchange opinions on “whether the current stage of search is relevant” and “whether we should explore now.” The algorithm infers user feedback models by an EM algorithm from the query logs. We compare to several state-of-the-art session search algorithms and evaluate our algorithm on the most recent TREC 2012 to 2014 Session Tracks. The experimental results demonstrates that our approach is highly effective for improving session search accuracy.},
  eventtitle = {The 2015 {{International Conference}}},
  isbn = {978-1-4503-3833-2},
  langid = {english},
  annotation = {ZSCC: 0000007},
  file = {/home/johnlocke/.zotero/storage/2TNSLCJ2/Luo et al_2015_Learning to Reinforce Search Effectiveness.pdf}
}

@unpublished{luongEffectiveApproachesAttentionbased2015,
  ids = {luongEffectiveApproachesAttentionbased2015a},
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  date = {2015-09-20},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1508.04025},
  urldate = {2020-02-23},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0003130},
  file = {/home/johnlocke/.zotero/storage/8RC2HBT8/Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf;/home/johnlocke/.zotero/storage/62QFEZ76/1508.html;/home/johnlocke/.zotero/storage/RVFLA8TL/1508.html}
}

@inproceedings{luoWinwinSearchDualagent2014,
  title = {Win-Win Search: Dual-Agent Stochastic Game in Session Search},
  shorttitle = {Win-Win Search},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval - {{SIGIR}} '14},
  author = {Luo, Jiyun and Zhang, Sicong and Yang, Hui},
  date = {2014},
  pages = {587--596},
  publisher = {{ACM Press}},
  location = {{Gold Coast, Queensland, Australia}},
  doi = {10.1145/2600428.2609629},
  url = {http://dl.acm.org/citation.cfm?doid=2600428.2609629},
  urldate = {2019-12-10},
  abstract = {Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user’s judgment of retrieved documents in the previous search iteration affects user’s actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term “win-win search”, is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.},
  eventtitle = {The 37th International {{ACM SIGIR}} Conference},
  isbn = {978-1-4503-2257-7},
  langid = {english},
  annotation = {ZSCC: 0000082},
  file = {/home/johnlocke/.zotero/storage/49VLAJQJ/Luo et al_2014_Win-win search.pdf}
}

@article{macavaneyABNIRMLAnalyzingBehavior2020,
  title = {{{ABNIRML}}: {{Analyzing}} the {{Behavior}} of {{Neural IR Models}}},
  shorttitle = {{{ABNIRML}}},
  author = {MacAvaney, Sean and Feldman, Sergey and Goharian, Nazli and Downey, Doug and Cohan, Arman},
  date = {2020},
  journaltitle = {ArXiv},
  abstract = {Numerous studies have demonstrated the effectiveness of pretrained contextualized language models such as BERT and T5 for ad-hoc search. However, it is not well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic tests that allow us to probe several characteristics---such as sensitivity to word order---that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit. We find evidence that recent neural ranking models have fundamentally different characteristics from prior ranking models. For instance, these models can be highly influenced by altered document word order, sentence order and inflectional endings. They can also exhibit unexpected behaviors when additional content is added to documents, or when documents are expressed with different levels of fluency or formality. We find that these differences can depend on the architecture and not just the underlying language model.},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/KBGT87CS/MacAvaney et al_2020_ABNIRML.pdf}
}

@article{manningIntroductionInformationRetrieval2009,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
  date = {2009},
  pages = {581},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/johnlocke/.zotero/storage/4ZRXN45P/Manning et al_2009_Introduction to Information Retrieval.pdf}
}

@inproceedings{maoInvestigatingReliabilityClick2019,
  title = {Investigating the {{Reliability}} of {{Click Models}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGIR International Conference}} on {{Theory}} of {{Information Retrieval}}},
  author = {Mao, Jiaxin and Chu, Zhumin and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
  date = {2019},
  pages = {125--128},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/LYYMYLFB/Mao et al_2019_Investigating the Reliability of Click Models.pdf;/home/johnlocke/.zotero/storage/JFQ4LWGV/citation.html}
}

@unpublished{mathurTangledBLEUReevaluating2020,
  title = {Tangled up in {{BLEU}}: {{Reevaluating}} the {{Evaluation}} of {{Automatic Machine Translation Evaluation Metrics}}},
  shorttitle = {Tangled up in {{BLEU}}},
  author = {Mathur, Nitika and Baldwin, Timothy and Cohn, Trevor},
  date = {2020-06-12},
  eprint = {2006.06264},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.06264},
  urldate = {2020-07-09},
  abstract = {Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/2RQWIWQ5/Mathur et al. - 2020 - Tangled up in BLEU Reevaluating the Evaluation of.pdf}
}

@unpublished{maUniversalTextRepresentation2019,
  title = {Universal {{Text Representation}} from {{BERT}}: {{An Empirical Study}}},
  shorttitle = {Universal {{Text Representation}} from {{BERT}}},
  author = {Ma, Xiaofei and Wang, Zhiguo and Ng, Patrick and Nallapati, Ramesh and Xiang, Bing},
  date = {2019-10-23},
  eprint = {1910.07973},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.07973},
  urldate = {2020-02-28},
  abstract = {We present a systematic investigation of layer-wise BERT activations for general-purpose text representations to understand what linguistic information they capture and how transferable they are across different tasks. Sentence-level embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval, while passage-level embeddings are evaluated on four question-answering (QA) datasets under a learning-to-rank problem setting. Embeddings from the pre-trained BERT model perform poorly in semantic similarity and sentence surface information probing tasks. Fine-tuning BERT on natural language inference data greatly improves the quality of the embeddings. Combining embeddings from different BERT layers can further boost performance. BERT embeddings outperform BM25 baseline significantly on factoid QA datasets at the passage level, but fail to perform better than BM25 on non-factoid datasets. For all QA datasets, there is a gap between embedding-based method and in-domain fine-tuned BERT (we report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/QZMKVSHS/Ma et al_2019_Universal Text Representation from BERT.pdf;/home/johnlocke/.zotero/storage/QP3JHZ6I/1910.html}
}

@inproceedings{mehrotraCharacterizingUsersMultiTasking2016,
  ids = {mehrotraCharacterizingUsersMultiTasking2016a,mehrotraCharacterizingUsersMultitasking2016},
  title = {Characterizing {{Users}}' {{Multi-Tasking Behavior}} in {{Web Search}}},
  booktitle = {Proceedings of the 2016 {{ACM}} on {{Conference}} on {{Human Information Interaction}} and {{Retrieval}} - {{CHIIR}} '16},
  author = {Mehrotra, Rishabh and Bhattacharya, Prasanta and Yilmaz, Emine},
  date = {2016},
  pages = {297--300},
  publisher = {{ACM Press}},
  location = {{Carrboro, North Carolina, USA}},
  doi = {10.1145/2854946.2855006},
  url = {http://dl.acm.org/citation.cfm?doid=2854946.2855006},
  urldate = {2019-12-10},
  abstract = {Multi-tasking within a single online search sessions is an increasingly popular phenomenon. In this work, we quantify multi-tasking behavior of web search users. Using insights from large-scale search logs, we seek to characterize user groups and search sessions with a focus on multi-task sessions. Our findings show that dual-task sessions are more prevalent than single-task sessions in online search, and that over 50\% of search sessions have more than 2 tasks. Further, we provide a method to categorize users into focused, multi-taskers or supertaskers depending on their level of task-multiplicity and show that the search effort expended by these users varies across the groups. The findings from this analysis provide useful insights about task-multiplicity in an online search environment and hold potential value for search engines that wish to personalize and support search experiences of users based on their task behavior.},
  eventtitle = {The 2016 {{ACM}}},
  isbn = {978-1-4503-3751-9},
  langid = {english},
  annotation = {ZSCC: 0000008},
  file = {/home/johnlocke/.zotero/storage/IBQP2DQX/Mehrotra et al. - 2016 - Characterizing Users' Multi-Tasking Behavior in We.pdf;/home/johnlocke/.zotero/storage/V5A77P9V/Mehrotra et al_2016_Characterizing Users' Multi-Tasking Behavior in Web Search.pdf;/home/johnlocke/.zotero/storage/QJY6Q6HG/2854946.html}
}

@inproceedings{mehrotraDeconstructingComplexTasks2016,
  title = {Deconstructing {{Complex Tasks}}},
  booktitle = {Proceedings of {{NAACL}}},
  author = {Mehrotra, Rishabh and Bhattacharya, Prasanta and Yilmaz, Emine},
  date = {2016},
  annotation = {ZSCC: 0000008}
}

@inproceedings{mehrotraDeepSequentialModels2017,
  title = {Deep {{Sequential Models}} for {{Task Satisfaction Prediction}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Mehrotra, Rishabh and Awadallah, Ahmed Hassan and Shokouhi, Milad and Yilmaz, Emine and Zitouni, Imed and El Kholy, Ahmed and Khabsa, Madian},
  date = {2017-11-06},
  series = {{{CIKM}} '17},
  pages = {737--746},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3132847.3133001},
  url = {https://doi.org/10.1145/3132847.3133001},
  urldate = {2021-02-23},
  abstract = {Detecting and understanding implicit signals of user satisfaction are essential for experimentation aimed at predicting searcher satisfaction. As retrieval systems have advanced, search tasks have steadily emerged as accurate units not only to capture searcher's goals but also in understanding how well a system is able to help the user achieve that goal. However, a major portion of existing work on modeling searcher satisfaction has focused on query level satisfaction. The few existing approaches for task satisfaction prediction have narrowly focused on simple tasks aimed at solving atomic information needs. In this work we go beyond such atomic tasks and consider the problem of predicting user's satisfaction when engaged in complex search tasks composed of many different queries and subtasks. We begin by considering holistic view of user interactions with the search engine result page (SERP) and extract detailed interaction sequences of their activity. We then look at query level abstraction and propose a novel deep sequential architecture which leverages the extracted interaction sequences to predict query level satisfaction. Further, we enrich this model with auxiliary features which have been traditionally used for satisfaction prediction and propose a unified multi-view model which combines the benefit of user interaction sequences with auxiliary features. Finally, we go beyond query level abstraction and consider query sequences issued by the user in order to complete a complex task, to make task level satisfaction predictions. We propose a number of functional composition techniques which take into account query level satisfaction estimates along with the query sequence to predict task level satisfaction. Through rigorous experiments, we demonstrate that the proposed deep sequential models significantly outperform established baselines at both query and task satisfaction prediction. Our findings have implications on metric development for gauging user satisfaction and on designing systems which help users accomplish complex search tasks.},
  isbn = {978-1-4503-4918-5},
  keywords = {lstm,search tasks,user interactions},
  annotation = {ZSCC: 0000022},
  file = {/home/johnlocke/.zotero/storage/RXIVGB95/Mehrotra et al_2017_Deep Sequential Models for Task Satisfaction Prediction.pdf}
}

@inproceedings{mehrotraTaskEmbeddingsLearning2017,
  ids = {mehrotraTaskEmbeddingsLearning2017a,mehrotraTaskEmbeddingsLearning2017b},
  title = {Task {{Embeddings}}: {{Learning Query Embeddings}} Using {{Task Context}}},
  shorttitle = {Task {{Embeddings}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}} - {{CIKM}} '17},
  author = {Mehrotra, Rishabh and Yilmaz, Emine},
  date = {2017},
  pages = {2199--2202},
  publisher = {{ACM Press}},
  location = {{Singapore, Singapore}},
  doi = {10.1145/3132847.3133098},
  url = {http://dl.acm.org/citation.cfm?doid=3132847.3133098},
  urldate = {2019-12-10},
  abstract = {Continuous space word embedding have been shown to be highly e ective in many information retrieval tasks. Embedding representation models make use of local information available in immediately surrounding words to project nearby context words closer in the embedding space. With rising multi-tasking nature of web search sessions, users o en try to accomplish di erent tasks in a single search session. Consequently, the search context gets polluted with queries from di erent unrelated tasks which renders the context heterogeneous. In this work, we hypothesize that task information provides be er context for IR systems to learn from. We propose a novel task context embedding architecture to learn representation of queries in low-dimensional space by leveraging their task context information from historical search logs using neural embedding models. In addition to qualitative analysis, we empirically demonstrate the bene t of leveraging task context to learn query representations.},
  eventtitle = {The 2017 {{ACM}}},
  isbn = {978-1-4503-4918-5},
  langid = {english},
  annotation = {ZSCC: 0000004},
  file = {/home/johnlocke/.zotero/storage/9RRGL8R8/Mehrotra_Yilmaz_2017_Task Embeddings.pdf;/home/johnlocke/.zotero/storage/GY49Y23T/Mehrotra et Yilmaz - 2017 - Task Embeddings Learning Query Embeddings using T.pdf;/home/johnlocke/.zotero/storage/JF6XVGKL/3132847.html}
}

@inproceedings{mehrotraUncoveringTaskBased2016,
  title = {Uncovering {{Task Based Behavioral Heterogeneities}} in {{Online Search Behavior}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}} - {{SIGIR}} '16},
  author = {Mehrotra, Rishabh and Bhattacharya, Prasanta and Yilmaz, Emine},
  date = {2016},
  pages = {1049--1052},
  publisher = {{ACM Press}},
  location = {{Pisa, Italy}},
  doi = {10.1145/2911451.2914755},
  url = {http://dl.acm.org/citation.cfm?doid=2911451.2914755},
  urldate = {2020-02-07},
  abstract = {While a major share of prior work have considered search sessions as the focal unit of analysis for seeking behavioral insights, search tasks are emerging as a competing perspective in this space. In the current work, we quantify user search task behavior for both single- as well as multi-task search sessions and relate it to tasks and topics. Specifically, we analyze user-disposition, topic and user-interest level heterogeneities that are prevalent in search task behavior. Our results show that while search multi-tasking is a common phenomenon among the search engine users, the extent and choice of multi-tasking topics vary significantly across users. We find that not only do users have varying propensities to multi-task, they also search for distinct topics across single-task and multi-task sessions. To our knowledge, this is among the first studies to fully characterize online search tasks with a focus on user- and topic-level differences that are observable from search sessions.},
  eventtitle = {The 39th {{International ACM SIGIR}} Conference},
  isbn = {978-1-4503-4069-4},
  langid = {english},
  annotation = {ZSCC: 0000010},
  file = {/home/johnlocke/.zotero/storage/IP8Z65T9/Mehrotra et al. - 2016 - Uncovering Task Based Behavioral Heterogeneities i.pdf;/home/johnlocke/.zotero/storage/W5V74J4H/2911451.html}
}

@inproceedings{meiQuerySuggestionUsing2008,
  title = {Query Suggestion Using Hitting Time},
  booktitle = {Proceeding of the 17th {{ACM}} Conference on {{Information}} and Knowledge Mining - {{CIKM}} '08},
  author = {Mei, Qiaozhu and Zhou, Dengyong and Church, Kenneth},
  date = {2008},
  pages = {469},
  publisher = {{ACM Press}},
  location = {{Napa Valley, California, USA}},
  doi = {10.1145/1458082.1458145},
  url = {http://portal.acm.org/citation.cfm?doid=1458082.1458145},
  urldate = {2020-01-28},
  abstract = {Generating alternative queries, also known as query suggestion, has long been proved useful to help a user explore and express his information need. In many scenarios, such suggestions can be generated from a large scale graph of queries and other accessory information, such as the clickthrough. However, how to generate suggestions while ensuring their semantic consistency with the original query remains a challenging problem.},
  eventtitle = {Proceeding of the 17th {{ACM}} Conference},
  isbn = {978-1-59593-991-3},
  langid = {english},
  annotation = {ZSCC: 0000358},
  file = {/home/johnlocke/.zotero/storage/H4GTHGLM/Mei et al. - 2008 - Query suggestion using hitting time.pdf;/home/johnlocke/.zotero/storage/39H5BYDS/1458082.html}
}

@article{mengSurveyQuerySuggestion2014,
  title = {A Survey on Query Suggestion},
  author = {Meng, Lingling},
  date = {2014},
  journaltitle = {International Journal of Hybrid Information Technology},
  volume = {7},
  number = {6},
  pages = {43--56},
  annotation = {ZSCC: 0000008},
  file = {/home/johnlocke/.zotero/storage/TWUKUDV6/Meng_2014_A survey on query suggestion.pdf}
}

@unpublished{meritySingleHeadedAttention2019,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  date = {2019-11-27},
  eprint = {1911.11423},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.11423},
  urldate = {2019-12-18},
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/VS7DX5MX/Merity_2019_Single Headed Attention RNN.pdf;/home/johnlocke/.zotero/storage/MN9KQ3VD/1911.html}
}

@unpublished{mikolovAdvancesPreTrainingDistributed2017,
  title = {Advances in {{Pre-Training Distributed Word Representations}}},
  author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  date = {2017-12-26},
  eprint = {1712.09405},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.09405},
  urldate = {2020-03-04},
  abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000285},
  file = {/home/johnlocke/.zotero/storage/QMU3QNWB/Mikolov et al_2017_Advances in Pre-Training Distributed Word Representations.pdf;/home/johnlocke/.zotero/storage/LKIWE36V/1712.html}
}

@unpublished{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2019-12-10},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0013994},
  file = {/home/johnlocke/.zotero/storage/ZAC6C4KN/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf}
}

@article{millerWordNetLexicalDatabase1995,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  date = {1995},
  journaltitle = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  annotation = {ZSCC: 0012837},
  file = {/home/johnlocke/.zotero/storage/N8RTTMMT/Miller_1995_WordNet.pdf;/home/johnlocke/.zotero/storage/4XWMQBGQ/citation.html}
}

@inproceedings{mishneFastDataEra2013,
  title = {Fast Data in the Era of Big Data: {{Twitter}}'s Real-Time Related Query Suggestion Architecture},
  shorttitle = {Fast Data in the Era of Big Data},
  booktitle = {Proceedings of the 2013 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Mishne, Gilad and Dalton, Jeff and Li, Zhenghua and Sharma, Aneesh and Lin, Jimmy},
  date = {2013-06-22},
  series = {{{SIGMOD}} '13},
  pages = {1147--1158},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, New York, USA}},
  doi = {10.1145/2463676.2465290},
  url = {https://doi.org/10.1145/2463676.2465290},
  urldate = {2020-02-04},
  abstract = {We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time "twist": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of "big data". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a "big data" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle "big" as well as "fast" data.},
  isbn = {978-1-4503-2037-5},
  keywords = {hadoop,log analysis,mapreduce},
  annotation = {ZSCC: 0000119},
  file = {/home/johnlocke/.zotero/storage/V2ELFBG6/Mishne et al_2013_Fast data in the era of big data.pdf}
}

@unpublished{misraMishSelfRegularized2020,
  title = {Mish: {{A Self Regularized Non-Monotonic Activation Function}}},
  shorttitle = {Mish},
  author = {Misra, Diganta},
  date = {2020-08-13},
  eprint = {1908.08681},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.08681},
  urldate = {2021-05-21},
  abstract = {We propose \$\textbackslash textit\{Mish\}\$, a novel self-regularized non-monotonic activation function which can be mathematically defined as: \$f(x)=x\textbackslash tanh(softplus(x))\$. As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision (\$AP\_\{50\}\^\{val\}\$) by 2.1\$\textbackslash\%\$ in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by \$\textbackslash approx\$1\$\textbackslash\%\$ while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks. Code is publicly available at https://github.com/digantamisra98/Mish.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/7KUDQ2S5/Misra_2020_Mish.pdf;/home/johnlocke/.zotero/storage/B3PVVC9G/1908.html}
}

@article{mitraIntroductionNeuralInformation2018,
  title = {An {{Introduction}} to {{Neural Information Retrieval}} t},
  author = {Mitra, Bhaskar and Craswell, Nick},
  date = {2018},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  shortjournal = {FNT in Information Retrieval},
  volume = {13},
  number = {1},
  pages = {1--126},
  issn = {1554-0669, 1554-0677},
  doi = {10.1561/1500000061},
  url = {http://www.nowpublishers.com/article/Details/INR-061},
  urldate = {2019-12-10},
  abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniques—including neural networks—over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical learning to rank models and non-neural approaches to IR, these new ML techniques are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of classical non-neural approaches to IR. We begin by introducing fundamental concepts of retrieval and different neural and non-neural approaches to unsupervised learning of vector representations of text. We then review IR methods that employ these pre-trained neural vector representations without learning the IR task end-to-end. We introduce the Learning to Rank (LTR) framework next, discussing standard loss functions for ranking. We follow that with an overview of deep neural networks (DNNs), including standard architectures and implementations. Finally, we review supervised neural learning to rank models, including recent DNN architectures trained end-to-end for ranking tasks. We conclude with a discussion on potential future directions for neural IR.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/VNYRXGU3/Mitra_Craswell_2018_An Introduction to Neural Information Retrieval t.pdf}
}

@inproceedings{mitraLearningMatchUsing2017,
  ids = {mitraLearningMatchUsing2017a,mitraLearningMatchUsing2017b},
  title = {Learning to {{Match}} Using {{Local}} and {{Distributed Representations}} of {{Text}} for {{Web Search}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Mitra, Bhaskar and Diaz, Fernando and Craswell, Nick},
  date = {2017-04-03},
  series = {{{WWW}} '17},
  pages = {1291--1299},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Perth, Australia}},
  doi = {10.1145/3038912.3052579},
  url = {https://doi.org/10.1145/3038912.3052579},
  urldate = {2020-02-23},
  abstract = {Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favourable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or 'duet' performs significantly better than either neural network individually on a Web page ranking task, and significantly outperforms traditional baselines and other recently proposed models based on neural networks.},
  isbn = {978-1-4503-4913-0},
  keywords = {document ranking,information retrieval,neural networks},
  annotation = {ZSCC: 0000185},
  file = {/home/johnlocke/.zotero/storage/UFM8UBWR/Mitra et al_2017_Learning to Match using Local and Distributed Representations of Text for Web.pdf;/home/johnlocke/.zotero/storage/DDB3MW9A/3038912.html;/home/johnlocke/.zotero/storage/VPDBDTJH/3038912.html}
}

@thesis{moralesTransformerModelQuery2018,
  title = {Transformer Model for Query Suggestion.},
  author = {Morales, Daniela Solis},
  date = {2018},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/IP9JE538/Morales_2018_Transformer model for query suggestion.pdf}
}

@unpublished{mousaviDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  date = {2018},
  volume = {16},
  eprint = {1806.08894},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {426--440},
  doi = {10.1007/978-3-319-56991-8_32},
  url = {http://arxiv.org/abs/1806.08894},
  urldate = {2019-12-10},
  abstract = {In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input. This paper reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/USM8HTK2/Mousavi et al_2018_Deep Reinforcement Learning.pdf}
}

@article{mustarStudyTransformersQuery,
  title = {On the {{Study}} of {{Transformers}} for {{Query Suggestion}}},
  author = {Mustar, Agnès and Lamprier, Sylvain and Piwowarski, Benjamin},
  volume = {1},
  number = {1},
  pages = {27},
  abstract = {AGNÈS MUSTAR, SYLVAIN LAMPRIER, BENJAMIN PIWOWARSKI, Sorbonne Université, CNRS, LIP6, F-75005, France When conducting a search task, users may find it difficult to articulate their need, even more so when the task is complex. To help them complete their search, search engine usually provide query suggestions. A good query suggestion system requires to model user behavior during the search session. In this paper, we study multiple Transformer architectures applied to the query suggestion task and compare them with RNN-based models. We experiment Transformer models with different tokenizers, with different Encoders (large pretrained models or fully trained ones), and with two kinds of architectures (flat or hierarchic). We study the performance and the behaviors of these various models, and observe that Transformer-based models outperform RNN-based ones. We show that while the hierarchical architectures exhibit very good performances for query suggestion, the flat models are more suitable for complex and long search tasks. Finally, we investigate the flat models behavior and demonstrate that they indeed learn to recover the hierarchy of a search session. CCS Concepts: • Information systems → Query representation; Query suggestion; Query intent; Query log analysis.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/DPLKUZ9P/Mustar et al. - On the Study of Transformers for Query Suggestion.pdf}
}

@inproceedings{mustarUsingBERTBART2020,
  ids = {mustarUsingBERTBART2020a},
  title = {Using {{BERT}} and {{BART}} for {{Query Suggestion}}},
  booktitle = {Joint {{Conference}} of the {{Information Retrieval Communities}} in {{Europe}}},
  author = {Mustar, Agnès and Lamprier, Sylvain and Piwowarski, Benjamin},
  date = {2020},
  volume = {2621},
  publisher = {{CEUR-WS. org}},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/JR2PKH5B/Mustar et al_2020_Using BERT and BART for Query Suggestion.pdf}
}

@patent{mysenDynamicQuerySuggestion2011,
  type = {patent},
  title = {Dynamic Query Suggestion},
  author = {Mysen, Clarence C. and Schwartz, Scott E.},
  date = {2011-09-27},
  annotation = {ZSCC: 0000156},
  file = {/home/johnlocke/.zotero/storage/WLJL5KB3/Mysen_Schwartz_2011_Dynamic query suggestion.pdf;/home/johnlocke/.zotero/storage/29KS4MEM/en.html}
}

@article{nakamotoShortIntroductionLearning2011,
  title = {A {{Short Introduction}} to {{Learning}} to {{Rank}}},
  author = {Nakamoto, Yukikazu},
  date = {2011},
  journaltitle = {IEICE Transactions on Information and Systems},
  shortjournal = {IEICE Trans. Inf. \& Syst.},
  volume = {E94-D},
  number = {1},
  pages = {1--2},
  issn = {0916-8532, 1745-1361},
  doi = {10.1587/transinf.E94.D.1},
  url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/transinf/E94.D.1?from=CrossRef},
  urldate = {2019-12-10},
  abstract = {Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive studies have been conducted on the problem and significant progress has been made [1], [2]. This short paper gives an introduction to learning to rank, and it specifically explains the fundamental problems, existing approaches, and future work of learning to rank. Several learning to rank methods using SVM techniques are described in details.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/A8KHHM6B/Nakamoto_2011_FOREWORD.pdf}
}

@unpublished{nambhiStuckNoWorries2019,
  title = {Stuck? {{No}} Worries!: {{Task-aware Command Recommendation}} and {{Proactive Help}} for {{Analysts}}},
  shorttitle = {Stuck?},
  author = {Nambhi, Aadhavan M. and Reddy, Bhanu Prakash and Agarwal, Aarsh Prakash and Verma, Gaurav and Singh, Harvineet and Burhanuddin, Iftikhar Ahamath},
  date = {2019-06-21},
  eprint = {1906.08973},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.08973},
  urldate = {2019-12-10},
  abstract = {Data analytics software applications have become an integral part of the decision-making process of analysts. Users of such a software face challenges due to insufficient product and domain knowledge, and find themselves in need of help. To alleviate this, we propose a task-aware command recommendation system, to guide the user on what commands could be executed next. We rely on topic modeling techniques to incorporate information about user’s task into our models. We also present a help prediction model to detect if a user is in need of help, in which case the system proactively provides the aforementioned command recommendations. We leverage the log data of a web-based analytics software to quantify the superior performance of our neural models, in comparison to competitive baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/JUIBE9I5/Nambhi et al_2019_Stuck.pdf}
}

@article{narwekarRecurrentNeuralNetwork,
  title = {Recurrent {{Neural Network Architectures}}},
  author = {Narwekar, Abhishek and Pampari, Anusri},
  pages = {124},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/J5HENEXJ/Narwekar_Pampari_Recurrent Neural Network Architectures.pdf}
}

@online{NeuralNetworkZoo,
  title = {The {{Neural Network Zoo}} - {{The Asimov Institute}}},
  url = {https://www.asimovinstitute.org/neural-network-zoo/},
  urldate = {2020-01-24},
  file = {/home/johnlocke/.zotero/storage/P25PWBQW/The Neural Network Zoo - The Asimov Institute.pdf}
}

@inproceedings{newellStackedHourglassNetworks2016,
  title = {Stacked {{Hourglass Networks}} for {{Human Pose Estimation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {483--499},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46484-8_29},
  abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
  isbn = {978-3-319-46484-8},
  langid = {english},
  keywords = {Human pose estimation},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/FISZZXBL/Newell et al_2016_Stacked Hourglass Networks for Human Pose Estimation.pdf}
}

@thesis{nguyenModelesNeuronauxPour2018,
  type = {phdthesis},
  title = {Modèles Neuronaux Pour La Recherche d'information : Approches Dirigées Par Les Ressources Sémantiques},
  shorttitle = {Modèles Neuronaux Pour La Recherche d'information},
  author = {Nguyen, Gia-Hung},
  date = {2018-12-18},
  institution = {{Université de Toulouse, Université Toulouse III - Paul Sabatier}},
  url = {http://thesesups.ups-tlse.fr/4141/},
  urldate = {2020-02-24},
  abstract = {Le projet de thèse porte sur l'application des approches neuronales pour la représentation de textes et l'appariement de textes en recherche d'information en vue de lever le  verrou du fossé sémantique. Plus précisément, les activités de thèse explorent la combinaison des apports de la sémantique relationnelle issue de ressources externes (comme  DPBedia et UMLS) et la sémantique distributionnelle basée sur les réseaux de neurones, dans le but : 1) d'apprendre des représentations de granules d'informations (mots,  concepts) et représentations de documents, et 2) d'apprendre la fonction pertinence d'un document pour une requête. Notre première contribution comprend des modèles neuronaux  pour l'apprentissage en ligne et apprentissage hors ligne des représentations de texte à plusieurs niveaux (mot, sens, document). Ces modèles intègrent les contraintes  relationnelles issues des ressources externes par régularisation de la fonction objectif ou par enrichissement sémantique des instances d'apprentissage. La deuxième contribution consiste en un modèle d'appariement requête-document par un réseau de neurones siamois. Ce réseau apprend à mesurer un score de pertinence entre un document et une requête à partir des vecteurs de représentation en entrée modélisant des objets (concepts, entités) identifiés dans la requêtes et documents et leurs relations issues des ressources externes. Les évaluation expérimentales sont conduites sur des tâches de RI et de traitement du langage naturel (TALN) en utilisant des collections standards TREC et des ressources largement utilisées comme DBpedia ou UMLS. Les résultats montrent principalement l'intérêt de l'utilisation des approches neuronales à la fois au niveau de la représentation des textes et de leur appariement ainsi que la variabilité de leurs performances selon les tâches considérées.},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/8NB58RIJ/Nguyen_2018_Modèles neuronaux pour la recherche d'information.pdf;/home/johnlocke/.zotero/storage/I5BRI7KT/4141.html}
}

@inproceedings{nguyenMSMARCOHuman2016,
  title = {{{MS MARCO}}: {{A}} Human Generated Machine Reading Comprehension Dataset},
  shorttitle = {{{MS MARCO}}},
  booktitle = {{{CoCo}}@ {{NIPS}}},
  author = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  date = {2016},
  file = {/home/johnlocke/.zotero/storage/L3ZA8GTA/Nguyen et al_2016_MS MARCO.pdf;/home/johnlocke/.zotero/storage/5EX8CMX8/forum.html}
}

@unpublished{nogueiraDocumentExpansionQuery2019,
  ids = {nogueiraDocumentExpansionQuery2019a},
  title = {Document Expansion by Query Prediction},
  author = {Nogueira, Rodrigo and Yang, Wei and Lin, Jimmy and Cho, Kyunghyun},
  date = {2019},
  eprint = {1904.08375},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000022},
  file = {/home/johnlocke/.zotero/storage/YG9YKFJ3/Nogueira et al_2019_Document expansion by query prediction.pdf;/home/johnlocke/.zotero/storage/7GF2BG7S/1904.html}
}

@unpublished{nogueiraTaskOrientedQueryReformulation2017,
  title = {Task-{{Oriented Query Reformulation}} with {{Reinforcement Learning}}},
  author = {Nogueira, Rodrigo and Cho, Kyunghyun},
  date = {2017-09-24},
  eprint = {1704.04572},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1704.04572},
  urldate = {2019-12-10},
  abstract = {Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20\% in terms of recall. Furthermore, we present a simple method to estimate a conservative upperbound performance of a model in a particular environment and verify that there is still large room for improvements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  annotation = {ZSCC: 0000061},
  file = {/home/johnlocke/.zotero/storage/J99EVLJJ/Nogueira_Cho_2017_Task-Oriented Query Reformulation with Reinforcement Learning.pdf}
}

@inproceedings{obendorfWebPageRevisitation2007,
  title = {Web Page Revisitation Revisited: Implications of a Long-Term Click-Stream Study of Browser Usage},
  shorttitle = {Web Page Revisitation Revisited},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Obendorf, Hartmut and Weinreich, Harald and Herder, Eelco and Mayer, Matthias},
  date = {2007-04-29},
  series = {{{CHI}} '07},
  pages = {597--606},
  publisher = {{Association for Computing Machinery}},
  location = {{San Jose, California, USA}},
  doi = {10.1145/1240624.1240719},
  url = {https://doi.org/10.1145/1240624.1240719},
  urldate = {2020-02-17},
  abstract = {This paper presents results of an extensive long-term click-stream study of Web browser usage. Focusing on character and challenges of page revisitation, previous findings from seven to thirteen years ago are updated. The term page re-visit had to be differentiated, since the recurrence rate--the key measure for the share of page revisits--turns out to strongly depend on interpretation. We identify different types of revisitation that allow assessing the quality of current user support and developing concepts for new tools. Individual navigation strategies differ dramatically and are strongly influenced by personal habits and type of site visited. Based on user action logs and interviews, we distinguished short-term revisits (backtrack or undo) from medium-term (re-utilize or observe) and long-term revisits (rediscover). We analyze current problems and provide suggestions for improving support for different revisitation types.},
  isbn = {978-1-59593-593-9},
  keywords = {history,hypertext,navigation,recurrence rate,revisitation,web browser interfaces,web browsing,WWW},
  annotation = {ZSCC: 0000195},
  file = {/home/johnlocke/.zotero/storage/L4TQTKU7/Obendorf et al_2007_Web page revisitation revisited.pdf}
}

@article{olahUnderstandingLstmNetworks2015,
  title = {Understanding Lstm Networks},
  author = {Olah, Christopher},
  date = {2015},
  annotation = {ZSCC: 0000544},
  file = {/home/johnlocke/.zotero/storage/HIPYDT27/Olah_2015_Understanding lstm networks.pdf;/home/johnlocke/.zotero/storage/LUVXSVU3/pub45500.html}
}

@article{onalNeuralInformationRetrieval2018,
  title = {Neural Information Retrieval: At the End of the Early Years},
  shorttitle = {Neural Information Retrieval},
  author = {Onal, Kezban Dilek and Zhang, Ye and Altingovde, Ismail Sengor and Rahman, Md Mustafizur and Karagoz, Pinar and Braylan, Alex and Dang, Brandon and Chang, Heng-Lu and Kim, Henna and McNamara, Quinten and Angert, Aaron and Banner, Edward and Khetan, Vivek and McDonnell, Tyler and Nguyen, An Thanh and Xu, Dan and Wallace, Byron C. and de Rijke, Maarten and Lease, Matthew},
  options = {useprefix=true},
  date = {2018-06},
  journaltitle = {Information Retrieval Journal},
  shortjournal = {Inf Retrieval J},
  volume = {21},
  number = {2-3},
  pages = {111--182},
  issn = {1386-4564, 1573-7659},
  doi = {10.1007/s10791-017-9321-y},
  url = {http://link.springer.com/10.1007/s10791-017-9321-y},
  urldate = {2019-12-10},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/LRP8QPT9/Onal et al_2018_Neural information retrieval.pdf}
}

@patent{ortegaSearchQueryProcessing2008,
  type = {patentus},
  title = {Search Query Processing to Identify Search String Corrections That Reflect Past Search Query Submissions of Users},
  author = {Ortega, Ruben Ernesto and Bowman, Dwayne Edward},
  holder = {{A9 com Inc}},
  date = {2008-10-28},
  number = {7444324B2},
  url = {https://patents.google.com/patent/US7444324B2/en},
  urldate = {2020-02-07},
  abstract = {A search engine process predicts the correct spellings of search terms within multiple-term search queries. In one embodiment, when a user submits a multiple-term search query that includes a non-matching term and at least one matching term, a table is accessed to look up a set of terms that are “related” to the matching term or terms. A spelling comparison function is then used to determine whether any of these related terms is sufficiently similar in spelling to the non-matching term to be deemed a candidate correctly-spelled replacement. A candidate replacement term may automatically be substituted for the non-matching term, or may be suggested to the user as a replacement. The invention also includes a process for identifying terms that are related to each other based on the relatively high frequencies with which they co-occur within search queries of users, database records, and/or specific database fields.},
  langid = {english},
  keywords = {corrected version,query,search,search query,user},
  annotation = {ZSCC: 0000258},
  file = {/home/johnlocke/.zotero/storage/CT8AD9MF/Ortega_Bowman_2008_Search query processing to identify search string corrections that reflect past.pdf}
}

@inproceedings{otteDynamicCortexMemory2014,
  title = {Dynamic Cortex Memory: {{Enhancing}} Recurrent Neural Networks for Gradient-Based Sequence Learning},
  shorttitle = {Dynamic Cortex Memory},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  author = {Otte, Sebastian and Liwicki, Marcus and Zell, Andreas},
  date = {2014},
  pages = {1--8},
  publisher = {{Springer}},
  annotation = {ZSCC: 0000023},
  file = {/home/johnlocke/.zotero/storage/PN4H5IVV/Otte et al_2014_Dynamic cortex memory.pdf;/home/johnlocke/.zotero/storage/8HCF9MD5/978-3-319-11179-7_1.html}
}

@inproceedings{ozertemLearningSuggestMachine2012,
  ids = {ozertemLearningSuggestMachine2012a},
  title = {Learning to Suggest: A Machine Learning Framework for Ranking Query Suggestions},
  shorttitle = {Learning to Suggest},
  booktitle = {Proceedings of the 35th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Ozertem, Umut and Chapelle, Olivier and Donmez, Pinar and Velipasaoglu, Emre},
  date = {2012},
  pages = {25--34},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000063},
  file = {/home/johnlocke/.zotero/storage/L977S8ET/Ozertem et al_2012_Learning to suggest.pdf;/home/johnlocke/.zotero/storage/LGJBB47R/Ozertem et al_2012_Learning to suggest.pdf;/home/johnlocke/.zotero/storage/29FGJWLJ/2348283.html;/home/johnlocke/.zotero/storage/5Q9C4KAX/2348283.html}
}

@inproceedings{papineniBleuMethodAutomatic2002,
  title = {Bleu: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {Bleu},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  date = {2002},
  pages = {311--318},
  file = {/home/johnlocke/.zotero/storage/UXCJHKPN/Papineni et al_2002_Bleu.pdf}
}

@unpublished{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  date = {2013-02-15},
  eprint = {1211.5063},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1211.5063},
  urldate = {2019-12-10},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/2BXEY2SS/Pascanu et al_2013_On the difficulty of training Recurrent Neural Networks.pdf}
}

@inproceedings{passPictureSearch2006,
  title = {A Picture of Search.},
  booktitle = {{{InfoScale}}},
  author = {Pass, Greg and Chowdhury, Abdur and Torgeson, Cayley},
  date = {2006},
  volume = {152},
  pages = {1},
  annotation = {ZSCC: 0000629},
  file = {/home/johnlocke/.zotero/storage/HMDG4VXX/Pass et al_2006_A picture of search.pdf}
}

@article{paternostreCarryAlgorithmeDesuffixation2002,
  title = {Carry, Un Algorithme de Désuffixation Pour Le Français},
  author = {Paternostre, Marjorie and Francq, Pascal and Lamoral, Julien and Wartel, David and Saerens, Marco},
  date = {2002},
  journaltitle = {Rapport technique du projet Galilei},
  annotation = {ZSCC: 0000026},
  file = {/home/johnlocke/.zotero/storage/3FUQKQCY/Paternostre et al_2002_Carry, un algorithme de désuffixation pour le français.pdf}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014},
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  location = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  urldate = {2019-12-10},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  eventtitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english},
  annotation = {ZSCC: 0011524},
  file = {/home/johnlocke/.zotero/storage/MIHHK56R/Pennington et al_2014_Glove.pdf}
}

@unpublished{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018-03-22},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2019-12-10},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/johnlocke/.zotero/storage/9YZMV9VN/Peters et al_2018_Deep contextualized word representations.pdf}
}

@article{petersenMatrixCookbook,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  langid = {english},
  annotation = {ZSCC: 0000030},
  file = {/home/johnlocke/.zotero/storage/UXF6LTF6/Petersen_Pedersen_[ http.pdf}
}

@inproceedings{petersSparseSequencetoSequenceModels2019,
  title = {Sparse {{Sequence-to-Sequence Models}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Peters, Ben and Niculae, Vlad and Martins, André F. T.},
  date = {2019-07},
  pages = {1504--1519},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1146},
  url = {https://aclanthology.org/P19-1146},
  urldate = {2022-01-10},
  abstract = {Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of α-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \$\textbackslash alpha {$>$} 1\$. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.},
  eventtitle = {{{ACL}} 2019},
  annotation = {ZSCC: 0000070},
  file = {/home/johnlocke/.zotero/storage/U6KNCKMB/Peters et al_2019_Sparse Sequence-to-Sequence Models.pdf}
}

@book{PhilipGuoPh,
  title = {Philip {{Guo}} - {{The Ph}}.{{D}}. {{Grind}}},
  url = {http://pgbovine.net/PhD-memoir.htm},
  urldate = {2020-01-24},
  file = {/home/johnlocke/.zotero/storage/LIQEPRDH/Philip Guo - The Ph.pdf;/home/johnlocke/.zotero/storage/ZAF2AZ4E/PhD-memoir.html}
}

@inproceedings{ponteLanguageModelingApproach1998,
  ids = {ponteLanguageModelingApproach1998a},
  title = {A Language Modeling Approach to Information Retrieval},
  booktitle = {Proceedings of the 21st Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Ponte, Jay M. and Croft, W. Bruce},
  date = {1998-08-01},
  series = {{{SIGIR}} '98},
  pages = {275--281},
  publisher = {{Association for Computing Machinery}},
  location = {{Melbourne, Australia}},
  doi = {10.1145/290941.291008},
  url = {https://doi.org/10.1145/290941.291008},
  urldate = {2020-02-23},
  isbn = {978-1-58113-015-7},
  annotation = {ZSCC: 0003378},
  file = {/home/johnlocke/.zotero/storage/C5QA3KI3/Ponte_Croft_1998_A language modeling approach to information retrieval.pdf;/home/johnlocke/.zotero/storage/PUI7W3X8/Ponte_Croft_1998_A language modeling approach to information retrieval.pdf}
}

@inproceedings{popovicWordErrorRates2007,
  title = {Word Error Rates: {{Decomposition}} over {{POS}} Classes and Applications for Error Analysis},
  shorttitle = {Word Error Rates},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Statistical Machine Translation}}},
  author = {Popović, Maja and Ney, Hermann},
  date = {2007},
  pages = {48--55},
  publisher = {{Association for Computational Linguistics}},
  annotation = {ZSCC: 0000079},
  file = {/home/johnlocke/.zotero/storage/EX4THPJE/Popović_Ney_2007_Word error rates.pdf;/home/johnlocke/.zotero/storage/EYY8C56E/1626355.html}
}

@book{porterSnowballLanguageStemming2001,
  title = {Snowball: {{A}} Language for Stemming Algorithms},
  shorttitle = {Snowball},
  author = {Porter, Martin F.},
  date = {2001},
  url = {http://snowball.tartarus.org/},
  annotation = {ZSCC: 0000561},
  file = {/home/johnlocke/.zotero/storage/VETYVF9H/introduction.html}
}

@unpublished{qiAnsweringComplexOpendomain2019,
  title = {Answering {{Complex Open-domain Questions Through Iterative Query Generation}}},
  author = {Qi, Peng and Lin, Xiaowen and Mehr, Leo and Wang, Zijian and Manning, Christopher D.},
  date = {2019-10-15},
  eprint = {1910.07000},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.07000},
  urldate = {2019-12-10},
  abstract = {It is challenging for current one-step retrieveand-read question answering (QA) systems to answer questions like “Which novel by the author of ‘Armada’ will be adapted as a feature film by Steven Spielberg?” because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GOLDEN (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GOLDEN Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GOLDEN Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GOLDEN Retriever on the recently proposed open-domain multi-hop QA dataset, HOTPOTQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/HYLADRYD/Qi et al_2019_Answering Complex Open-domain Questions Through Iterative Query Generation.pdf}
}

@inproceedings{qureshiLongShortTermMemory2021,
  title = {Long {{Short-Term Memory Based Query Auto-Completion}}},
  booktitle = {2021 8th {{International Conference}} on {{Electrical}} and {{Electronics Engineering}} ({{ICEEE}})},
  author = {Qureshi, Abdur Rehman Anwar and Akcayol, M. Ali},
  date = {2021-04-09},
  pages = {259--266},
  publisher = {{IEEE}},
  location = {{Antalya, Turkey}},
  doi = {10.1109/ICEEE52452.2021.9415918},
  url = {https://ieeexplore.ieee.org/document/9415918/},
  urldate = {2021-08-28},
  abstract = {In this study, Long Short-Term Memory (LSTM) based Query Auto-Completion (QAC) has been proposed to generate a query completion list using input prefix. The performance of the QAC system has been evaluated by using the relevancy score, and the quality of the QAC generation system has been evaluated by using partial and complete matching strategies, success rate, normalized discounted cumulative gain, and mean average precision. The proposed LSTM based QAC system has been extensively tested using AOL and ORCAS datasets. According to experimental results, the performance of the proposed QAC system is more successful with the partial matching strategy. Also, the quality of the QAC generation list by the proposed QAC system is better on the complete matching strategy.},
  eventtitle = {2021 8th {{International Conference}} on {{Electrical}} and {{Electronics Engineering}} ({{ICEEE}})},
  isbn = {978-1-66544-071-4},
  langid = {english},
  keywords = {Information retrieval,long short-term memory,one-hot encoding,query auto-completion,textual information retrieval},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/J8ZZ8IAH/Qureshi et Akcayol - 2021 - Long Short-Term Memory Based Query Auto-Completion.pdf;/home/johnlocke/.zotero/storage/EIRLHBMJ/9415918.html}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  annotation = {ZSCC: 0000272},
  file = {/home/johnlocke/.zotero/storage/U8AZAE7Q/Radford et al_Language Models are Unsupervised Multitask Learners.pdf}
}

@inproceedings{radlinskiQueryChainsLearning2005,
  title = {Query Chains: Learning to Rank from Implicit Feedback},
  shorttitle = {Query Chains},
  booktitle = {Proceedings of the Eleventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery in Data Mining},
  author = {Radlinski, Filip and Joachims, Thorsten},
  date = {2005},
  pages = {239--248},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000593},
  file = {/home/johnlocke/.zotero/storage/RWXPISAH/Radlinski_Joachims_2005_Query chains.pdf;/home/johnlocke/.zotero/storage/DVB2BKEZ/1081870.html}
}

@unpublished{raffelExploringLimitsTransfer2019,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2019-10-24},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.10683},
  urldate = {2019-12-10},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johnlocke/.zotero/storage/V5MNBT68/Raffel et al_2019_Exploring the Limits of Transfer Learning with a Unified Text-to-Text.pdf}
}

@article{ramanUnderstandingIntrinsicDiversity2014,
  title = {Understanding Intrinsic Diversity in Web Search: {{Improving}} Whole-Session Relevance},
  shorttitle = {Understanding Intrinsic Diversity in Web Search},
  author = {Raman, Karthik and Bennett, Paul N. and Collins-Thompson, Kevyn},
  date = {2014},
  journaltitle = {ACM Transactions on Information Systems (TOIS)},
  volume = {32},
  number = {4},
  pages = {20},
  annotation = {ZSCC: 0000011},
  file = {/home/johnlocke/.zotero/storage/DJANZFWI/Raman et al_2014_Understanding intrinsic diversity in web search.pdf;/home/johnlocke/.zotero/storage/NCMV4WMB/2629553.html}
}

@unpublished{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-24},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2021-02-25},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/J9YQLBV8/Ramesh et al_2021_Zero-Shot Text-to-Image Generation.pdf;/home/johnlocke/.zotero/storage/PXC72YPA/2102.html}
}

@unpublished{ribeiroAccuracyBehavioralTesting2020,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP}} Models with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  date = {2020-05-08},
  eprint = {2005.04118},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.04118},
  urldate = {2020-07-09},
  abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a taskagnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/SUKWJCJE/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP models .pdf}
}

@inproceedings{riehPatternsSequencesMultiple2001,
  title = {Patterns and Sequences of Multiple Query Reformulations in Web Searching: {{A}} Preliminary Study},
  shorttitle = {Patterns and Sequences of Multiple Query Reformulations in Web Searching},
  booktitle = {Proceedings of the {{Annual Meeting-American Society}} for {{Information Science}}},
  author = {Rieh, Soo Young and Xie, Hong},
  date = {2001},
  volume = {38},
  pages = {246--255},
  publisher = {{Information Today; 1998}},
  annotation = {ZSCC: 0000070},
  file = {/home/johnlocke/.zotero/storage/N66CCW59/Rieh_Xie_2001_Patterns and sequences of multiple query reformulations in web searching.pdf}
}

@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The Probabilistic Relevance Framework: {{BM25}} and Beyond},
  shorttitle = {The Probabilistic Relevance Framework},
  author = {Robertson, Stephen and Zaragoza, Hugo},
  date = {2009},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  volume = {3},
  number = {4},
  pages = {333--389},
  annotation = {ZSCC: 0000990},
  file = {/home/johnlocke/.zotero/storage/BLE3JT8D/Robertson_Zaragoza_2009_The probabilistic relevance framework.pdf;/home/johnlocke/.zotero/storage/JCVGHGMA/INR-019.html}
}

@article{robertStatisticalRethinking2017,
  title = {Statistical {{Rethinking}}},
  author = {Robert, Christian},
  date = {2017-03},
  journaltitle = {CHANCE},
  volume = {30},
  number = {1},
  pages = {40--42},
  doi = {10.1080/09332480.2017.1302722},
  url = {https://hal.archives-ouvertes.fr/hal-02274933},
  urldate = {2020-02-28},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/V5Q4PF4R/Robert_2017_Statistical Rethinking.pdf;/home/johnlocke/.zotero/storage/YXVQA25S/hal-02274933.html}
}

@article{rocchioRelevanceFeedbackInformation1971,
  title = {Relevance Feedback in Information Retrieval},
  author = {Rocchio, Joseph},
  date = {1971},
  journaltitle = {The Smart retrieval system-experiments in automatic document processing},
  pages = {313--323},
  url = {http://sigir.org/files/museum/pub-08/XXIII-1.pdf},
  annotation = {ZSCC: 0004001},
  file = {/home/johnlocke/.zotero/storage/3NFZVDSA/Rocchio_1971_Relevance feedback in information retrieval.pdf;/home/johnlocke/.zotero/storage/DA5624RX/10000074359.html}
}

@unpublished{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020-02-27},
  eprint = {2002.12327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.12327},
  urldate = {2020-03-03},
  abstract = {Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/FMBPQ2YU/Rogers et al_2020_A Primer in BERTology.pdf;/home/johnlocke/.zotero/storage/5ZZW66NW/2002.html}
}

@unpublished{rosenbaumRoutingNetworksAdaptive2017,
  title = {Routing {{Networks}}: {{Adaptive Selection}} of {{Non-linear Functions}} for {{Multi-Task Learning}}},
  shorttitle = {Routing {{Networks}}},
  author = {Rosenbaum, Clemens and Klinger, Tim and Riemer, Matthew},
  date = {2017-12-31},
  eprint = {1711.01239},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.01239},
  urldate = {2020-02-27},
  abstract = {Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85\% reduction in training time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000042},
  file = {/home/johnlocke/.zotero/storage/XUP7HKTI/Rosenbaum et al_2017_Routing Networks.pdf;/home/johnlocke/.zotero/storage/53RDXRM4/1711.html}
}

@inproceedings{rossReductionImitationLearning2011,
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Ross, Stéphane and Gordon, Geoffrey and Bagnell, Drew},
  date = {2011},
  pages = {627--635},
  annotation = {ZSCC: 0001208},
  file = {/home/johnlocke/.zotero/storage/8H6GYUDG/Ross et al_2011_A reduction of imitation learning and structured prediction to no-regret online.pdf}
}

@patent{roullandRealtimeQuerySuggestion2014,
  type = {patentus},
  title = {Real-Time Query Suggestion in a Troubleshooting Context},
  author = {Roulland, Frederic and Castellani, Stefania and Kaplan, Aaron N. and Grasso, Maria Antonietta and O'Neill, Jacki and Selin, Jonina},
  holder = {{Xerox Corp}},
  date = {2014-04-08},
  number = {8694483B2},
  url = {https://patents.google.com/patent/US8694483B2/en},
  urldate = {2020-02-04},
  langid = {english},
  keywords = {knowledge base,query,query suggestions,suggestions,user},
  annotation = {ZSCC: 0000116},
  file = {/home/johnlocke/.zotero/storage/ZI3DSFHA/Roulland et al_2014_Real-time query suggestion in a troubleshooting context.pdf}
}

@unpublished{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2017-06-15},
  eprint = {1609.04747},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.04747},
  urldate = {2019-12-10},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {ZSCC: 0000004},
  file = {/home/johnlocke/.zotero/storage/U4LPWLHA/Ruder_2017_An overview of gradient descent optimization algorithms.pdf}
}

@unpublished{ruderOverviewMultiTaskLearning2017,
  title = {An {{Overview}} of {{Multi-Task Learning}} in {{Deep Neural Networks}}},
  author = {Ruder, Sebastian},
  date = {2017-06-15},
  eprint = {1706.05098},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.05098},
  urldate = {2020-03-05},
  abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: 0000561},
  file = {/home/johnlocke/.zotero/storage/GY6CJ8KB/Ruder_2017_An Overview of Multi-Task Learning in Deep Neural Networks.pdf;/home/johnlocke/.zotero/storage/N4QDFLZU/1706.html}
}

@article{ruotsaloInteractiveFacetedQuery2020,
  title = {Interactive Faceted Query Suggestion for Exploratory Search: {{Whole-session}} Effectiveness and Interaction Engagement},
  shorttitle = {Interactive Faceted Query Suggestion for Exploratory Search},
  author = {Ruotsalo, Tuukka and Jacucci, Giulio and Kaski, Samuel},
  date = {2020},
  journaltitle = {Journal of the Association for Information Science and Technology},
  volume = {71},
  number = {7},
  pages = {742--756},
  issn = {2330-1643},
  doi = {10.1002/asi.24304},
  url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24304},
  urldate = {2021-03-11},
  abstract = {The outcome of exploratory information retrieval is not only dependent on the effectiveness of individual responses to a set of queries, but also on relevant information retrieved during the entire exploratory search session. We study the effect of search assistance, operationalized as an interactive faceted query suggestion, for both whole-session effectiveness and engagement through interactive faceted query suggestion. A user experiment is reported, where users performed exploratory search tasks, comparing interactive faceted query suggestion and a control condition with only conventional typed-query interaction. Data comprised of interaction and search logs show that the availability of interactive faceted query suggestion substantially improves whole-session effectiveness by increasing recall without sacrificing precision. The increased engagement with interactive faceted query suggestion is targeted to direct situated navigation around the initial query scope, but is not found to improve individual queries on average. The results imply that research in exploratory search should focus on measuring and designing tools that engage users with directed situated navigation support for improving whole-session performance.},
  langid = {english},
  annotation = {ZSCC: 0000006  \_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24304},
  file = {/home/johnlocke/.zotero/storage/GQMP2QYG/Ruotsalo et al_2020_Interactive faceted query suggestion for exploratory search.pdf;/home/johnlocke/.zotero/storage/SRB4UU2F/asi.html}
}

@inproceedings{salehiMultitaskLearningQuery2018,
  title = {Multitask {{Learning}} for {{Query Segmentation}} in {{Job Search}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGIR International Conference}} on {{Theory}} of {{Information Retrieval}} - {{ICTIR}} '18},
  author = {Salehi, Bahar and Liu, Fei and Baldwin, Timothy and Wong, Wilson},
  date = {2018},
  pages = {179--182},
  publisher = {{ACM Press}},
  location = {{Tianjin, China}},
  doi = {10.1145/3234944.3234965},
  url = {http://dl.acm.org/citation.cfm?doid=3234944.3234965},
  urldate = {2020-02-12},
  abstract = {In this paper, we present the first attempt to use multitask learning for query segmentation. We use the semantic category of the words as an auxiliary task and show that segmentation improves when the model is also trained to predict the semantic category of the query terms, outperforming benchmark methods over a novel dataset from a popular job search engine. Our further experiments show that the task of modeling the query term semantics performs better as a standalone task, without adding segmentation as an auxiliary task.},
  eventtitle = {The 2018 {{ACM SIGIR International Conference}}},
  isbn = {978-1-4503-5656-5},
  langid = {english},
  annotation = {ZSCC: 0000003},
  file = {/home/johnlocke/.zotero/storage/MWTJG2L3/Salehi et al. - 2018 - Multitask Learning for Query Segmentation in Job S.pdf;/home/johnlocke/.zotero/storage/RSJAWME8/3234944.html}
}

@article{saltonVectorSpaceModel1975,
  title = {A Vector Space Model for Automatic Indexing},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  date = {1975-11-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {18},
  number = {11},
  pages = {613--620},
  issn = {0001-0782},
  doi = {10.1145/361219.361220},
  url = {https://doi.org/10.1145/361219.361220},
  urldate = {2020-02-21},
  abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
  keywords = {automatic indexing,automatic information retrieval,content analysis,document space},
  annotation = {ZSCC: 0008965},
  file = {/home/johnlocke/.zotero/storage/VST96MR8/Salton et al_1975_A vector space model for automatic indexing.pdf}
}

@article{santosLearningRankQuery2013,
  title = {Learning to Rank Query Suggestions for Adhoc and Diversity Search},
  author = {Santos, Rodrygo L. T. and Macdonald, Craig and Ounis, Iadh},
  date = {2013-08-01},
  journaltitle = {Information Retrieval},
  shortjournal = {Inf Retrieval},
  volume = {16},
  number = {4},
  pages = {429--451},
  issn = {1573-7659},
  doi = {10.1007/s10791-012-9211-2},
  url = {https://doi.org/10.1007/s10791-012-9211-2},
  urldate = {2020-05-09},
  abstract = {Query suggestions have become pervasive in modern web search, as a mechanism to guide users towards a better representation of their information need. In this article, we propose a ranking approach for producing effective query suggestions. In particular, we devise a structured representation of candidate suggestions mined from a query log that leverages evidence from other queries with a common session or a common click. This enriched representation not only helps overcome data sparsity for long-tail queries, but also leads to multiple ranking criteria, which we integrate as features for learning to rank query suggestions. To validate our approach, we build upon existing efforts for web search evaluation and propose a novel framework for the quantitative assessment of query suggestion effectiveness. Thorough experiments using publicly available data from the TREC Web track show that our approach provides effective suggestions for adhoc and diversity search.},
  langid = {english},
  annotation = {ZSCC: 0000042},
  file = {/home/johnlocke/.zotero/storage/X79VSAW7/Santos et al_2013_Learning to rank query suggestions for adhoc and diversity search.pdf;/home/johnlocke/.zotero/storage/QP4LPA3G/s10791-012-9211-2.html}
}

@unpublished{schaulPrioritizedExperienceReplay2016,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2016-02-25},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1511.05952},
  urldate = {2020-03-02},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/FEBPNW8F/Schaul et al_2016_Prioritized Experience Replay.pdf;/home/johnlocke/.zotero/storage/L9L6BVSN/1511.html}
}

@unpublished{schickSelfDiagnosisSelfDebiasingProposal2021,
  ids = {schickSelfDiagnosisSelfDebiasingProposal2021a},
  title = {Self-{{Diagnosis}} and {{Self-Debiasing}}: {{A Proposal}} for {{Reducing Corpus-Based Bias}} in {{NLP}}},
  shorttitle = {Self-{{Diagnosis}} and {{Self-Debiasing}}},
  author = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
  date = {2021-02-28},
  eprint = {2103.00453},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.00453},
  urldate = {2021-03-05},
  abstract = {When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models often require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we investigate whether pretrained language models at least know when they exhibit some undesirable bias or produce toxic content. Based on our findings, we propose a decoding algorithm that reduces the probability of a model producing problematic text given only a textual description of the undesired behavior. This algorithm does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. While our approach does by no means eliminate the issue of language models generating biased text, we believe it to be an important step in this direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/HGNBZWE7/Schick et al_2021_Self-Diagnosis and Self-Debiasing.pdf;/home/johnlocke/.zotero/storage/V68QSU84/2103.html}
}

@book{schimelWritingScienceHow2012,
  title = {Writing Science: How to Write Papers That Get Cited and Proposals That Get Funded},
  shorttitle = {Writing Science},
  author = {Schimel, Joshua},
  date = {2012},
  publisher = {{Oxford University Press}},
  location = {{Oxford ; New York}},
  isbn = {978-0-19-976023-7 978-0-19-976024-4},
  langid = {english},
  pagetotal = {221},
  keywords = {Proposal writing for grants,Technical writing},
  annotation = {ZSCC: 0000089  OCLC: ocn738354410},
  file = {/home/johnlocke/.zotero/storage/EZ8AQIX7/Schimel_2012_Writing science.pdf}
}

@inproceedings{schmidEnrichedTreetaggerSystem2007,
  title = {The Enriched Treetagger System},
  booktitle = {Proceedings of the {{EVALITA}} 2007 Workshop},
  author = {Schmid, H. and Baroni, M. and Zanchetta, E. and Stein, A.},
  date = {2007},
  url = {https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger},
  annotation = {ZSCC: 0000015}
}

@article{schutzeAutomaticWordSense1998,
  title = {Automatic {{Word Sense Discrimination}}},
  author = {Schütze, Hinrich},
  date = {1998-03},
  journaltitle = {Comput. Linguist.},
  volume = {24},
  number = {1},
  pages = {97--123},
  issn = {0891-2017},
  url = {http://dl.acm.org/citation.cfm?id=972719.972724},
  urldate = {2019-12-17},
  abstract = {This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.},
  annotation = {ZSCC: 0001583},
  file = {/home/johnlocke/.zotero/storage/HE34TD8M/Schütze_1998_Automatic Word Sense Discrimination.pdf}
}

@inproceedings{sculleyWebscaleKmeansClustering2010,
  title = {Web-Scale k-Means Clustering},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Sculley, David},
  date = {2010},
  pages = {1177--1178},
  file = {/home/johnlocke/.zotero/storage/RI2VN348/Sculley_2010_Web-scale k-means clustering.pdf;/home/johnlocke/.zotero/storage/LALSXE9B/1772690.html}
}

@inproceedings{senTempolexicalContextDriven2018,
  title = {Tempo-Lexical Context Driven Word Embedding for Cross-Session Search Task Extraction},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Sen, Procheta and Ganguly, Debasis and Jones, Gareth},
  date = {2018},
  pages = {283--292},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/RK676XC8/Sen et al_2018_Tempo-lexical context driven word embedding for cross-session search task.pdf}
}

@unpublished{shazeerFastTransformerDecoding2019,
  title = {Fast {{Transformer Decoding}}: {{One Write-Head}} Is {{All You Need}}},
  shorttitle = {Fast {{Transformer Decoding}}},
  author = {Shazeer, Noam},
  date = {2019-11-05},
  eprint = {1911.02150},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.02150},
  urldate = {2019-12-18},
  abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/X9H2K3SP/Shazeer_2019_Fast Transformer Decoding.pdf;/home/johnlocke/.zotero/storage/IH7Z8XZ7/1911.html}
}

@inproceedings{shenContextsensitiveInformationRetrieval2005,
  title = {Context-Sensitive Information Retrieval Using Implicit Feedback},
  booktitle = {Proceedings of the 28th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Shen, Xuehua and Tan, Bin and Zhai, ChengXiang},
  date = {2005-08-15},
  series = {{{SIGIR}} '05},
  pages = {43--50},
  publisher = {{Association for Computing Machinery}},
  location = {{Salvador, Brazil}},
  doi = {10.1145/1076034.1076045},
  url = {https://doi.org/10.1145/1076034.1076045},
  urldate = {2020-02-23},
  abstract = {A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.},
  isbn = {978-1-59593-034-7},
  keywords = {context,interactive retrieval,query expansion,query history},
  annotation = {ZSCC: 0000598},
  file = {/home/johnlocke/.zotero/storage/5BFUZBDN/Shen et al_2005_Context-sensitive information retrieval using implicit feedback.pdf}
}

@inproceedings{shenLatentSemanticModel2014,
  title = {A {{Latent Semantic Model}} with {{Convolutional-Pooling Structure}} for {{Information Retrieval}}},
  booktitle = {Proceedings of the 23rd {{ACM International Conference}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Grégoire},
  date = {2014-11-03},
  series = {{{CIKM}} '14},
  pages = {101--110},
  publisher = {{Association for Computing Machinery}},
  location = {{Shanghai, China}},
  doi = {10.1145/2661829.2661935},
  url = {https://doi.org/10.1145/2661829.2661935},
  urldate = {2020-02-23},
  abstract = {In this paper, we propose a new latent semantic model that incorporates a convolutional-pooling structure over word sequences to learn low-dimensional, semantic vector representations for search queries and Web documents. In order to capture the rich contextual structures in a query or a document, we start with each word within a temporal context window in a word sequence to directly capture contextual features at the word n-gram level. Next, the salient word n-gram features in the word sequence are discovered by the model and are then aggregated to form a sentence-level feature vector. Finally, a non-linear transformation is applied to extract high-level semantic information to generate a continuous vector representation for the full text string. The proposed convolutional latent semantic model (CLSM) is trained on clickthrough data and is evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that the proposed model effectively captures salient semantic information in queries and documents for the task while significantly outperforming previous state-of-the-art semantic models.},
  isbn = {978-1-4503-2598-1},
  keywords = {convolutional neural network,deep learning,semantic representation,web search},
  annotation = {ZSCC: 0000394},
  file = {/home/johnlocke/.zotero/storage/RYZ5FTJ6/Shen et al_2014_A Latent Semantic Model with Convolutional-Pooling Structure for Information.pdf;/home/johnlocke/.zotero/storage/G35UR3GA/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval.html}
}

@inproceedings{shenLearningSemanticRepresentations2014,
  ids = {shenLearningSemanticRepresentations2014a},
  title = {Learning Semantic Representations Using Convolutional Neural Networks for Web Search},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{World Wide Web}}},
  author = {Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Grégoire},
  date = {2014},
  pages = {373--374},
  annotation = {ZSCC: 0000431},
  file = {/home/johnlocke/.zotero/storage/L3Z8MQZS/Shen et al_2014_Learning semantic representations using convolutional neural networks for web.pdf;/home/johnlocke/.zotero/storage/MR8M69CA/2567948.html}
}

@unpublished{shiConvolutionalLSTMNetwork2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  date = {2015-09-19},
  eprint = {1506.04214},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.04214},
  urldate = {2021-03-08},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0002963},
  file = {/home/johnlocke/.zotero/storage/V754NUIP/Shi et al_2015_Convolutional LSTM Network.pdf;/home/johnlocke/.zotero/storage/2PWD5GMI/1506.html}
}

@article{shindeSurveyVariousQuery2014,
  title = {Survey of Various Query Suggestion System},
  author = {Shinde, Prajakta and Joshi, Pranjali},
  date = {2014},
  journaltitle = {International Journal Of Engineering and Computer Science},
  volume = {3},
  number = {12},
  pages = {9576--9580},
  annotation = {ZSCC: 0000005},
  file = {/home/johnlocke/.zotero/storage/7CSQXKVY/Shinde_Joshi_2014_Survey of various query suggestion system.pdf}
}

@inproceedings{shokouhiLearningPersonalizeQuery2013,
  title = {Learning to Personalize Query Auto-Completion},
  booktitle = {Proceedings of the 36th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Shokouhi, Milad},
  date = {2013},
  pages = {103--112},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000155},
  file = {/home/johnlocke/.zotero/storage/V7G63JRK/Shokouhi_2013_Learning to personalize query auto-completion.pdf;/home/johnlocke/.zotero/storage/NGNVTS92/2484028.html}
}

@article{silversteinAnalysisVeryLarge,
  title = {Analysis of a {{Very Large AltaVista Query Log}}},
  author = {Silverstein, Craig and Henzinger, Monika and Marais, Hannes and Moricz, Michael},
  journaltitle = {1999},
  pages = {18},
  langid = {english},
  annotation = {ZSCC: 0000493},
  file = {/home/johnlocke/.zotero/storage/RGDGJUM4/Silverstein et al. - Analysis of a Very Large AltaVista Query Log.pdf}
}

@inproceedings{sordoniHierarchicalRecurrentEncoderdecoder2015,
  title = {A Hierarchical Recurrent Encoder-Decoder for Generative Context-Aware Query Suggestion},
  booktitle = {Proceedings of the 24th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Sordoni, Alessandro and Bengio, Yoshua and Vahabi, Hossein and Lioma, Christina and Grue Simonsen, Jakob and Nie, Jian-Yun},
  date = {2015},
  pages = {553--562},
  publisher = {{ACM}},
  abstract = {Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications.},
  annotation = {ZSCC: 0000248},
  file = {/home/johnlocke/.zotero/storage/VL36SCLX/Sordoni et al_2015_A hierarchical recurrent encoder-decoder for generative context-aware query.pdf;/home/johnlocke/.zotero/storage/Y7VBCEJ9/citation.html}
}

@article{spinkSearchingWebPublic2001,
  ids = {spinkSearchingWebPublic},
  title = {Searching the Web: {{The}} Public and Their Queries},
  shorttitle = {Searching the Web},
  author = {Spink, Amanda and Wolfram, Dietmar and Jansen, Major BJ and Saracevic, Tefko},
  date = {2001},
  journaltitle = {Journal of the American society for information science and technology},
  volume = {52},
  number = {3},
  pages = {226--234},
  publisher = {{Wiley Online Library}},
  annotation = {ZSCC: 0001374},
  file = {/home/johnlocke/.zotero/storage/W7YRGVY6/Spink et Wolfram - Searching the web The public and their queries.pdf;/home/johnlocke/.zotero/storage/JVSD4LWE/1097-4571(2000)99999999AID-ASI15913.0.html;/home/johnlocke/.zotero/storage/M6KXQMZ6/1097-4571(2000)99999999AID-ASI15913.0.html}
}

@book{spinkWebSearchPublic2006,
  title = {Web Search: {{Public}} Searching of the {{Web}}},
  shorttitle = {Web Search},
  author = {Spink, Amanda and Jansen, Bernard J.},
  date = {2006},
  volume = {6},
  publisher = {{Springer Science \& Business Media}},
  annotation = {ZSCC: 0000438}
}

@article{srinivasanHowReadPaper2007,
  title = {How to Read a Paper},
  author = {Srinivasan, Keshav},
  date = {2007},
  journaltitle = {ACM SIGCOMM Computer Communication Review},
  edition = {3},
  pages = {83--84},
  url = {https://www.computing.dcu.ie/~ray/teaching/CA485/notes/01_how_to_read_a_paper.pdf},
  urldate = {2019-12-11},
  entrysubtype = {newspaper},
  journalsubtitle = {37},
  annotation = {ZSCC: 0000088},
  file = {/home/johnlocke/.zotero/storage/B53K8H4R/Srinivasan_2007_How to read a paper.pdf}
}

@unpublished{srivastavaHighwayNetworks2015,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  date = {2015-11-03},
  eprint = {1505.00387},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1505.00387},
  urldate = {2020-02-27},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6},
  annotation = {ZSCC: 0001064},
  file = {/home/johnlocke/.zotero/storage/USANNVML/Srivastava et al_2015_Highway Networks.pdf;/home/johnlocke/.zotero/storage/9T5B7LVC/1505.html}
}

@unpublished{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014},
  eprint = {1409.3215},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1409.3215},
  urldate = {2019-12-10},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0009223},
  file = {/home/johnlocke/.zotero/storage/2Q8JNTLU/Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf}
}

@article{suttonReinforcementLearningIntroduction2011,
  title = {Reinforcement Learning: {{An}} Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2011},
  annotation = {ZSCC: 0000208},
  file = {/home/johnlocke/.zotero/storage/872D3UTV/Sutton_Barto_2011_Reinforcement learning.pdf;/home/johnlocke/.zotero/storage/D8G9BQ36/737FD21CA908246DF17779E9C20B6DF6.html}
}

@inproceedings{szegedyInceptionv4InceptionResNetImpact2017,
  title = {Inception-v4, {{Inception-ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  date = {2017-02-12},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806},
  urldate = {2020-03-06},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  eventtitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  annotation = {ZSCC: 0004011},
  file = {/home/johnlocke/.zotero/storage/YZUIUPCD/Szegedy et al_2017_Inception-v4, Inception-ResNet and the Impact of Residual Connections on.pdf;/home/johnlocke/.zotero/storage/FPMHDANT/14806.html}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  date = {2016-06},
  pages = {2818--2826},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.308},
  url = {http://ieeexplore.ieee.org/document/7780677/},
  urldate = {2020-03-06},
  abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error on the validation set and 3.6\% top-5 error on the official test set.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {ZSCC: 0006970},
  file = {/home/johnlocke/.zotero/storage/UCS8UKIP/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf;/home/johnlocke/.zotero/storage/N9KAG2PU/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html}
}

@inproceedings{szpektorImprovingRecommendationLongtail2011,
  ids = {szpektorImprovingRecommendationLongtail2011a},
  title = {Improving Recommendation for Long-Tail Queries via Templates},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web},
  author = {Szpektor, Idan and Gionis, Aristides and Maarek, Yoelle},
  date = {2011},
  pages = {47--56},
  annotation = {ZSCC: 0000103},
  file = {/home/johnlocke/.zotero/storage/2A6FSVMN/Szpektor et al. - 2011 - Improving recommendation for long-tail queries via.pdf;/home/johnlocke/.zotero/storage/8QPTTQ6S/1963405.html}
}

@unpublished{talmorOLMpicsWhatLanguage2019,
  title = {{{oLMpics}} -- {{On}} What {{Language Model Pre-training Captures}}},
  author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
  date = {2019-12-31},
  eprint = {1912.13283},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.13283},
  urldate = {2020-01-30},
  abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/76KWST5T/Talmor et al_2019_oLMpics -- On what Language Model Pre-training Captures.pdf;/home/johnlocke/.zotero/storage/9RM8NVJR/1912.html}
}

@incollection{tamineWhatCanTask2020,
  title = {What {{Can Task Teach Us About Query Reformulations}}?},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Tamine, Lynda and Melgarejo, Jesús Lovón and Pinel-Sauvagnat, Karen},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12035},
  pages = {636--650},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-45439-5_42},
  url = {http://link.springer.com/10.1007/978-3-030-45439-5_42},
  urldate = {2021-02-23},
  abstract = {A significant amount of prior research has been devoted to understanding query reformulations. The majority of these works rely on time-based sessions which are sequences of contiguous queries segmented using time threshold on users’ activities. However, queries are generally issued by users having in mind a particular task, and time-based sessions unfortunately fail in revealing such tasks. In this paper, we are interested in revealing in which extent time-based sessions vs. task-based sessions represent significantly different background contexts to be used in the perspective of better understanding users’ query reformulations. Using insights from large-scale search logs, our findings clearly show that task is an additional relevant search unit that helps better understanding user’s query reformulation patterns and predicting the next user’s query. The findings from our analyses provide potential implications for model design of task-based search engines.},
  isbn = {978-3-030-45438-8 978-3-030-45439-5},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/47W9RBQD/Tamine et al. - 2020 - What Can Task Teach Us About Query Reformulations.pdf;/home/johnlocke/.zotero/storage/BVCG24VU/978-3-030-45439-5_42.html}
}

@online{TransformerArchitecturePositional,
  title = {Transformer {{Architecture}}: {{The Positional Encoding}} - {{Amirhossein Kazemnejad}}'s {{Blog}}},
  url = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/},
  urldate = {2021-01-08},
  file = {/home/johnlocke/.zotero/storage/HC7L6RGH/transformer_architecture_positional_encoding.html}
}

@article{UnreasonableEffectivenessRecurrent,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  pages = {20},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/YCDFEFWX/The Unreasonable Effectiveness of Recurrent Neural Networks.pdf}
}

@inproceedings{vahabiOrthogonalQueryRecommendation2013,
  title = {Orthogonal Query Recommendation},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Recommender}} Systems},
  author = {Vahabi, Hossein and Ackerman, Margareta and Loker, David and Baeza-Yates, Ricardo and Lopez-Ortiz, Alejandro},
  date = {2013},
  pages = {33--40},
  publisher = {{ACM}},
  annotation = {ZSCC: 0000031},
  file = {/home/johnlocke/.zotero/storage/79V9J5VR/Vahabi et al_2013_Orthogonal query recommendation.pdf;/home/johnlocke/.zotero/storage/AWNLCKVR/2507157.html}
}

@article{vakkariTheoryTaskbasedInformation2001,
  title = {A Theory of the Task-Based Information Retrieval Process: A Summary and Generalisation of a Longitudinal Study},
  shorttitle = {A Theory of the Task-Based Information Retrieval Process},
  author = {Vakkari, Pertti},
  date = {2001},
  journaltitle = {Journal of documentation},
  volume = {57},
  number = {1},
  pages = {44--60},
  annotation = {ZSCC: 0000361},
  file = {/home/johnlocke/.zotero/storage/LGDZQ55J/Vakkari_2001_A theory of the task-based information retrieval process.pdf;/home/johnlocke/.zotero/storage/GLFUUVTB/art00002.html}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  pages = {11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/AZWU7VJB/Vaswani et al_Attention is All you Need.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \textbackslash Lukasz and Polosukhin, Illia},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  file = {/home/johnlocke/.zotero/storage/Q2R3LQH3/Vaswani et al_2017_Attention is all you need.pdf;/home/johnlocke/.zotero/storage/2HI46CFV/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@unpublished{venugopalanSequenceSequenceVideo2015,
  title = {Sequence to {{Sequence}} -- {{Video}} to {{Text}}},
  author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeff and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  date = {2015-10-19},
  eprint = {1505.00487},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1505.00487},
  urldate = {2021-03-05},
  abstract = {Real-world videos often have complex dynamics; and methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0001068},
  file = {/home/johnlocke/.zotero/storage/975ZM9FM/Venugopalan et al_2015_Sequence to Sequence -- Video to Text.pdf;/home/johnlocke/.zotero/storage/5X9YIYGW/1505.html}
}

@inproceedings{vinyalsPointerNetworks2015,
  title = {Pointer Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  date = {2015},
  pages = {2692--2700},
  annotation = {ZSCC: 0000947},
  file = {/home/johnlocke/.zotero/storage/KTB6CDN2/Vinyals et al_2015_Pointer networks.pdf;/home/johnlocke/.zotero/storage/F4FUEP44/5866-pointer-networks.html}
}

@inproceedings{volskeQueryTaskMapping2019,
  title = {Query-{{Task Mapping}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Völske, Michael and Fatehifar, Ehsan and Stein, Benno and Hagen, Matthias},
  date = {2019-07-18},
  series = {{{SIGIR}}'19},
  pages = {969--972},
  publisher = {{Association for Computing Machinery}},
  location = {{Paris, France}},
  doi = {10.1145/3331184.3331286},
  url = {https://doi.org/10.1145/3331184.3331286},
  urldate = {2020-03-03},
  abstract = {Several recent task-based search studies aim at splitting query logs into sets of queries for the same task or information need. We address the natural next step: mapping a currently submitted query to an appropriate task in an already task-split log. This query-task mapping can, for instance, enhance query suggestions---rendering efficiency of the mapping, besides accuracy, a key objective. Our main contributions are three large benchmark datasets and preliminary experiments with four query-task mapping approaches: (1) a Trie-based approach, (2) MinHash\textasciitilde LSH, (3) word movers distance in a Word2Vec setup, and (4) an inverted index-based approach. The experiments show that the fast and accurate inverted index-based method forms a strong baseline.},
  isbn = {978-1-4503-6172-9},
  keywords = {query-task mapping,task-based search},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/3CUL23UQ/Völske et al_2019_Query-Task Mapping.pdf}
}

@article{vyasSurveyGradedRelevance,
  title = {Survey of Graded Relevance Metrics for Information Retrieval},
  author = {Vyas, Jaladhi},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/VD59T7WA/Vyas_Survey of graded relevance metrics for information retrieval.pdf}
}

@inproceedings{wangInnerAttentionBased2016,
  title = {Inner Attention Based Recurrent Neural Networks for Answer Selection},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Bingning and Liu, Kang and Zhao, Jun},
  date = {2016},
  pages = {1288--1297},
  annotation = {ZSCC: 0000168},
  file = {/home/johnlocke/.zotero/storage/KN8ZFXGJ/Wang et al_2016_Inner attention based recurrent neural networks for answer selection.pdf}
}

@inproceedings{wangLearningExtractCrosssession2013,
  ids = {wangLearningExtractCrosssession2013a},
  title = {Learning to Extract Cross-Session Search Tasks},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}} - {{WWW}} '13},
  author = {Wang, Hongning and Song, Yang and Chang, Ming-Wei and He, Xiaodong and White, Ryen W. and Chu, Wei},
  date = {2013},
  pages = {1353--1364},
  publisher = {{ACM Press}},
  location = {{Rio de Janeiro, Brazil}},
  doi = {10.1145/2488388.2488507},
  url = {http://dl.acm.org/citation.cfm?doid=2488388.2488507},
  urldate = {2020-02-07},
  abstract = {Search tasks, comprising a series of search queries serving the same information need, have recently been recognized as an accurate atomic unit for modeling user search intent. Most prior research in this area has focused on shortterm search tasks within a single search session, and heavily depend on human annotations for supervised classification model learning. In this work, we target the identification of long-term, or cross-session, search tasks (transcending session boundaries) by investigating inter-query dependencies learned from users’ searching behaviors. A semi-supervised clustering model is proposed based on the latent structural SVM framework, and a set of effective automatic annotation rules are proposed as weak supervision to release the burden of manual annotation. Experimental results based on a large-scale search log collected from Bing.com confirms the effectiveness of the proposed model in identifying cross-session search tasks and the utility of the introduced weak supervision signals. Our learned model enables a more comprehensive understanding of users’ search behaviors via search logs and facilitates the development of dedicated search-engine support for long-term tasks.},
  eventtitle = {The 22nd International Conference},
  isbn = {978-1-4503-2035-1},
  langid = {english},
  keywords = {cross-session search task,query log mining,semi-supervised clustering,weak supervision},
  annotation = {ZSCC: 0000075},
  file = {/home/johnlocke/.zotero/storage/BFXYJJDU/Wang et al_2013_Learning to extract cross-session search tasks.pdf;/home/johnlocke/.zotero/storage/X7MULD3P/Wang et al. - 2013 - Learning to extract cross-session search tasks.pdf}
}

@inproceedings{wangNeuralMachineTranslation2020,
  title = {Neural Machine Translation with Byte-Level Subwords},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao},
  date = {2020},
  volume = {34},
  number = {05},
  pages = {9154--9160},
  annotation = {ZSCC: 0000009},
  file = {/home/johnlocke/.zotero/storage/PRL66HSU/Wang et al_2020_Neural machine translation with byte-level subwords.pdf;/home/johnlocke/.zotero/storage/NTDMHEKG/6451.html}
}

@unpublished{weinshallCurriculumLearningTransfer2018,
  title = {Curriculum {{Learning}} by {{Transfer Learning}}: {{Theory}} and {{Experiments}} with {{Deep Networks}}},
  shorttitle = {Curriculum {{Learning}} by {{Transfer Learning}}},
  author = {Weinshall, Daphna and Cohen, Gad and Amir, Dan},
  date = {2018-06-08},
  eprint = {1802.03796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.03796},
  urldate = {2020-03-06},
  abstract = {We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {ZSCC: 0000035},
  file = {/home/johnlocke/.zotero/storage/D4VPYHT3/Weinshall et al_2018_Curriculum Learning by Transfer Learning.pdf;/home/johnlocke/.zotero/storage/WBACE2YW/1802.html}
}

@article{werbosBackpropagationTimeWhat1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  author = {Werbos, Paul},
  date = {1990},
  url = {https://ieeexplore.ieee.org/document/58337},
  urldate = {2019-12-11},
  annotation = {ZSCC: 0003955},
  file = {/home/johnlocke/.zotero/storage/JT4BLJBZ/Werbos_1990_Backpropagation through time.pdf}
}

@report{wiederholdMediatorArchitectureAbstract1990,
  ids = {wiederholdMediatorArchitectureAbstract1990a,wiederholdMediatorArchitectureAbstract1990b},
  title = {A {{Mediator Architecture}} for {{Abstract Data Access}}:},
  shorttitle = {A {{Mediator Architecture}} for {{Abstract Data Access}}},
  author = {Wiederhold, Gio and Risch, Tore and Rathmann, Peter and DeMichiel, Linda and Chaudhuri, Surajit},
  date = {1990-02-23},
  institution = {{Defense Technical Information Center}},
  location = {{Fort Belvoir, VA}},
  doi = {10.21236/ADA227362},
  url = {http://www.dtic.mil/docs/citations/ADA227362},
  urldate = {2020-02-07},
  langid = {english},
  annotation = {ZSCC: 0000014},
  file = {/home/johnlocke/.zotero/storage/7PYEM8EH/Wiederhold et al. - 1990 - A Mediator Architecture for Abstract Data Access.pdf;/home/johnlocke/.zotero/storage/FY28347U/Wiederhold et al. - 1990 - A Mediator Architecture for Abstract Data Access.pdf;/home/johnlocke/.zotero/storage/Q2Q4XAW2/Wiederhold et al. - 1990 - A Mediator Architecture for Abstract Data Access.pdf}
}

@article{wolframVoxPopuliPublic2001,
  title = {Vox Populi: {{The}} Public Searching of the Web},
  shorttitle = {Vox Populi},
  author = {Wolfram, Dietmar and Spink, Amanda and Jansen, Bernard J. and Saracevic, Tefko},
  date = {2001},
  journaltitle = {Journal of the American Society for Information Science and Technology},
  shortjournal = {J. Am. Soc. Inf. Sci.},
  volume = {52},
  number = {12},
  pages = {1073--1074},
  issn = {1532-2882, 1532-2890},
  doi = {10.1002/asi.1157},
  url = {http://doi.wiley.com/10.1002/asi.1157},
  urldate = {2020-03-01},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/IFAA2XRX/Wolfram et al. - 2001 - Vox populi The public searching of the web.pdf}
}

@article{wuAdaptingBoostingInformation2010,
  title = {Adapting Boosting for Information Retrieval Measures},
  author = {Wu, Qiang and Burges, Christopher J. C. and Svore, Krysta M. and Gao, Jianfeng},
  date = {2010-06},
  journaltitle = {Information Retrieval},
  shortjournal = {Inf Retrieval},
  volume = {13},
  number = {3},
  pages = {254--270},
  issn = {1386-4564, 1573-7659},
  doi = {10.1007/s10791-009-9112-1},
  url = {http://link.springer.com/10.1007/s10791-009-9112-1},
  urldate = {2020-01-20},
  abstract = {We present a new ranking algorithm that combines the strengths of two previous methods: boosted tree classification, and LambdaRank, which has been shown to be empirically optimal for a widely used information retrieval measure. Our algorithm is based on boosted regression trees, although the ideas apply to any weak learners, and it is significantly faster in both train and test phases than the state of the art, for comparable accuracy. We also show how to find the optimal linear combination for any two rankers, and we use this method to solve the line search problem exactly during boosting. In addition, we show that starting with a previously trained model, and boosting using its residuals, furnishes an effective technique for model adaptation, and we give significantly improved results for a particularly pressing problem in web search—training rankers for markets for which only small amounts of labeled data are available, given a ranker trained on much more data from a larger market.},
  langid = {english},
  annotation = {ZSCC: 0000390},
  file = {/home/johnlocke/.zotero/storage/ENK7ER8A/Wu et al_2010_Adapting boosting for information retrieval measures.pdf;/home/johnlocke/.zotero/storage/ZE8UR3KL/s10791-009-9112-1.html}
}

@unpublished{wuGlobaltolocalMemoryPointer2019,
  title = {Global-to-Local {{Memory Pointer Networks}} for {{Task-Oriented Dialogue}}},
  author = {Wu, Chien-Sheng and Socher, Richard and Xiong, Caiming},
  date = {2019-03-29},
  eprint = {1901.04713},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1901.04713},
  urldate = {2020-02-24},
  abstract = {End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/johnlocke/.zotero/storage/GUTYSUW9/Wu et al_2019_Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.pdf;/home/johnlocke/.zotero/storage/FEWSXJ6D/1901.html}
}

@inproceedings{wuQuerySuggestionFeedback2018,
  ids = {wuQuerySuggestionFeedback2018a,wuQuerySuggestionFeedback2018b},
  title = {Query {{Suggestion}} with {{Feedback Memory Network}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Wu, Bin and Xiong, Chenyan and Sun, Maosong and Liu, Zhiyuan},
  date = {2018-04-10},
  series = {{{WWW}} '18},
  pages = {1563--1571},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Lyon, France}},
  doi = {10.1145/3178876.3186068},
  url = {https://doi.org/10.1145/3178876.3186068},
  urldate = {2020-02-19},
  abstract = {This paper presents Feedback Memory Network (\textbackslash textttFMN) which models user interactions with the search engine for query suggestion. Besides modeling the queries issued by the user, \textbackslash textttFMN also considers user feedback on the search results. It converts user browsing and click actions to the attention over the top-ranked documents and combines them into the feedback memories of the query, thus better models the underlying information needs. The feedback memories and the query sequence are then combined to suggest queries by the sequence-to-sequence neural network. Modeling user feedback makes it possible to suggest diverse queries for the same query sequence, if users have preferred different search results that indicate different information needs. Our experiments on the search log from a Chinese commercial search engine showed the stable and robust advantages of \textbackslash textttFMN. Especially when the feedback is richer or more informative, \textbackslash textttFMN provides more diverse and accurate suggestions, which is exceptionally helpful for ambiguous sessions where more information is required to infer the search intents.},
  isbn = {978-1-4503-5639-8},
  keywords = {feedback memory network,query suggestion,user modeling},
  annotation = {ZSCC: 0000009},
  file = {/home/johnlocke/.zotero/storage/9NJABCTX/Wu et al_2018_Query Suggestion with Feedback Memory Network.pdf;/home/johnlocke/.zotero/storage/CEFELPLA/Wu et al_2018_Query Suggestion with Feedback Memory Network.pdf;/home/johnlocke/.zotero/storage/VWQTV2YJ/Wu et al_2018_Query Suggestion with Feedback Memory Network.pdf;/home/johnlocke/.zotero/storage/S5Z5YTRT/3178876.html}
}

@unpublished{xueByT5TokenfreeFuture2021,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  shorttitle = {{{ByT5}}},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  date = {2021-05-28},
  eprint = {2105.13626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.13626},
  urldate = {2021-06-09},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/2W7HNL27/Xue et al_2021_ByT5.pdf;/home/johnlocke/.zotero/storage/DISSTY8E/2105.html}
}

@article{xuShowAttendTell,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image CaptionGeneration}} with {{Visual Attention}}},
  author = {Xu, Kelvin and Lei, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
  pages = {10},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/johnlocke/.zotero/storage/5DLV59E4/Xu et al. - Show, Attend and Tell Neural Image CaptionGenerat.pdf}
}

@unpublished{xuUnderstandingImprovingLayer2019,
  title = {Understanding and Improving Layer Normalization},
  author = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  date = {2019},
  eprint = {1911.07013},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000021},
  file = {/home/johnlocke/.zotero/storage/E9VMCB92/Xu et al_2019_Understanding and improving layer normalization.pdf;/home/johnlocke/.zotero/storage/NPU9SKYG/1911.html}
}

@inproceedings{yabePredictingEvacuationDecisions2019,
  ids = {yabePredictingEvacuationDecisions2019a,yabePredictingEvacuationDecisions2019b,yabePredictingEvacuationDecisions2019c},
  title = {Predicting {{Evacuation Decisions}} Using {{Representations}} of {{Individuals}}' {{Pre-Disaster Web Search Behavior}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '19},
  author = {Yabe, Takahiro and Tsubouchi, Kota and Shimizu, Toru and Sekimoto, Yoshihide and Ukkusuri, Satish V.},
  date = {2019},
  pages = {2707--2717},
  publisher = {{ACM Press}},
  location = {{Anchorage, AK, USA}},
  doi = {10.1145/3292500.3330697},
  url = {http://dl.acm.org/citation.cfm?doid=3292500.3330697},
  urldate = {2020-02-07},
  abstract = {Predicting the evacuation decisions of individuals before the disaster strikes is crucial for planning first response strategies. In addition to the studies on post-disaster analysis of evacuation behavior, there are various works that attempt to predict the evacuation decisions beforehand. Most of these predictive methods, however, require real time location data for calibration, which are becoming much harder to obtain due to the rising privacy concerns. Meanwhile, web search queries of anonymous users have been collected by web companies. Although such data raise less privacy concerns, they have been under-utilized for various applications. In this study, we investigate whether web search data observed prior to the disaster can be used to predict the evacuation decisions. More specifically, we utilize a session-based query encoder that learns the representations of each user’s web search behavior prior to evacuation. Our proposed approach is empirically tested using web search data collected from users affected by a major flood in Japan. Results are validated using location data collected from mobile phones of the same set of users as ground truth. We show that evacuation decisions can be accurately predicted (84\%) using only the users’ pre-disaster web search data as input. This study proposes an alternative method for evacuation prediction that does not require highly sensitive location data, which can assist local governments to prepare effective first response strategies.},
  eventtitle = {The 25th {{ACM SIGKDD International Conference}}},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/AXLAKGSN/Yabe et al. - 2019 - Predicting Evacuation Decisions using Representati.pdf;/home/johnlocke/.zotero/storage/GWCMFCCD/Yabe et al. - 2019 - Predicting Evacuation Decisions using Representati.pdf;/home/johnlocke/.zotero/storage/UZ5255HG/Yabe et al. - 2019 - Predicting Evacuation Decisions using Representati.pdf;/home/johnlocke/.zotero/storage/XQ4G2DCA/3292500.html}
}

@inproceedings{yadavSessionAwareQueryAutocompletion2021,
  title = {Session-{{Aware Query Auto-completion}} Using {{Extreme Multi-Label Ranking}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Yadav, Nishant and Sen, Rajat and Hill, Daniel N. and Mazumdar, Arya and Dhillon, Inderjit S.},
  date = {2021-08-14},
  series = {{{KDD}} '21},
  pages = {3835--3844},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467087},
  url = {https://doi.org/10.1145/3447548.3467087},
  urldate = {2021-09-16},
  abstract = {Query auto-completion (QAC) is a fundamental feature in search engines where the task is to suggest plausible completions of a prefix typed in the search bar. Previous queries in the user session can provide useful context for the user's intent and can be leveraged to suggest auto-completions that are more relevant while adhering to the user's prefix. Such session-aware QACs can be generated by recent sequence-to-sequence deep learning models; however, these generative approaches often do not meet the stringent latency requirements of responding to each user keystroke. Moreover, these generative approaches pose the risk of showing nonsensical queries. One can pre-compute a relatively small subset of relevant queries for common prefixes and rank them based on the context. However, such an approach fails when no relevant queries for the current context are present in the pre-computed set. In this paper, we provide a solution to this problem: we take the novel approach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR) problem where the input is the previous query in the session and the user's current prefix, while the output space is the set of tens of millions of queries entered by users in the recent past. We adapt a popular XMR algorithm for this purpose by proposing several modifications to the key steps in the algorithm. The proposed modifications yield a 10x improvement in terms of Mean Reciprocal Rank (MRR) over the baseline XMR approach on a public search logs dataset. We are able to maintain an inference latency of less than 10 ms while still using session context. When compared against baseline models of acceptable latency, we observed a 33\% improvement in MRR for short prefixes of up to 3 characters. Moreover, our model yielded a statistically significant improvement of 2.81\% over a production QAC system in terms of suggestion acceptance rate, when deployed on the search bar of an online shopping store as part of an A/B test.},
  isbn = {978-1-4503-8332-5},
  keywords = {auto-complete,extreme multi-label ranking,multi-label,session-aware},
  annotation = {ZSCC: 0000001},
  file = {/home/johnlocke/.zotero/storage/FRVKB8FZ/Yadav et al_2021_Session-Aware Query Auto-completion using Extreme Multi-Label Ranking.pdf}
}

@thesis{YagmurGizemCinar,
  title = {Yagmur {{Gizem Cinar}} - {{Sequence Prediction}} Using {{Recurrent Neural Networks}} ({{RNNs}}) in the {{Context}} of {{Time Series}} and {{Information Retrieval Search Sessions}} | {{LIG}} - {{Laboratoire}} d'{{Informatique}} de {{Grenoble}}},
  url = {https://www.liglab.fr/en/events/thesis-defenses/yagmur-gizem-cinar-sequence-prediction-using-recurrent-neural-networks-rnns},
  urldate = {2020-01-24},
  file = {/home/johnlocke/.zotero/storage/QXXZSKK8/Yagmur Gizem Cinar - Sequence Prediction using Recurrent Neural Networks (RNNs).pdf}
}

@unpublished{yangAppliedFederatedLearning2018,
  title = {Applied Federated Learning: {{Improving}} Google Keyboard Query Suggestions},
  shorttitle = {Applied Federated Learning},
  author = {Yang, Timothy and Andrew, Galen and Eichner, Hubert and Sun, Haicheng and Li, Wei and Kong, Nicholas and Ramage, Daniel and Beaufays, Françoise},
  date = {2018},
  eprint = {1812.02903},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000024},
  file = {/home/johnlocke/.zotero/storage/CQC3K2SC/Yang et al_2018_Applied federated learning.pdf;/home/johnlocke/.zotero/storage/JDN69G9S/1812.html}
}

@inproceedings{yangSearchbasedQuerySuggestion2008,
  title = {Search-Based Query Suggestion},
  booktitle = {Proceeding of the 17th {{ACM}} Conference on {{Information}} and Knowledge Mining - {{CIKM}} '08},
  author = {Yang, Jiang-Ming and Cai, Rui and Jing, Feng and Wang, Shuo and Zhang, Lei and Ma, Wei-Ying},
  date = {2008},
  pages = {1439},
  publisher = {{ACM Press}},
  location = {{Napa Valley, California, USA}},
  doi = {10.1145/1458082.1458321},
  url = {http://portal.acm.org/citation.cfm?doid=1458082.1458321},
  urldate = {2020-01-20},
  abstract = {In this paper, we proposed a unified strategy to combine query log and search results for query suggestion. In this way, we leverage both the users’ search intentions for popular queries and the power of search engines for unpopular queries. The suggested queries are also ranked according to their relevance and qualities; and each suggestion is described with a rich snippet including a photo and related description.},
  eventtitle = {Proceeding of the 17th {{ACM}} Conference},
  isbn = {978-1-59593-991-3},
  langid = {english},
  annotation = {ZSCC: 0000025},
  file = {/home/johnlocke/.zotero/storage/KGX64DKJ/Yang et al_2008_Search-based query suggestion.pdf}
}

@article{yangSessionSearchModeling2018,
  title = {Session Search Modeling by Partially Observable {{Markov}} Decision Process},
  author = {Yang, Grace Hui and Dong, Xuchu and Luo, Jiyun and Zhang, Sicong},
  date = {2018-02},
  journaltitle = {Information Retrieval Journal},
  shortjournal = {Inf Retrieval J},
  volume = {21},
  number = {1},
  pages = {56--80},
  issn = {1386-4564, 1573-7659},
  doi = {10.1007/s10791-017-9316-8},
  url = {http://link.springer.com/10.1007/s10791-017-9316-8},
  urldate = {2019-12-10},
  abstract = {Session search, the task of document retrieval for a series of queries in a session, has been receiving increasing attention from the information retrieval research community. Session search exhibits the properties of rich user-system interactions and temporal dependency. These properties lead to our proposal of using partially observable Markov decision process to model session search. On the basis of a design choice schema for states, actions and rewards, we evaluate different combinations of these choices over the TREC 2012 and 2013 session track datasets. According to the experimental results, practical design recommendations for using PODMP in session search are discussed.},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/JC4JSRGS/Yang et al_2018_Session search modeling by partially observable Markov decision process.pdf}
}

@article{yuanResearchKValueSelection2019,
  title = {Research on {{K-Value Selection Method}} of {{K-Means Clustering Algorithm}}},
  author = {Yuan, Chunhui and Yang, Haitao},
  date = {2019-06},
  journaltitle = {J},
  volume = {2},
  number = {2},
  pages = {226--235},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2571-8800},
  doi = {10.3390/j2020016},
  url = {https://www.mdpi.com/2571-8800/2/2/16},
  urldate = {2022-05-12},
  abstract = {Among many clustering algorithms, the K-means clustering algorithm is widely used because of its simple algorithm and fast convergence. However, the K-value of clustering needs to be given in advance and the choice of K-value directly affect the convergence result. To solve this problem, we mainly analyze four K-value selection algorithms, namely Elbow Method, Gap Statistic, Silhouette Coefficient, and Canopy; give the pseudo code of the algorithm; and use the standard data set Iris for experimental verification. Finally, the verification results are evaluated, the advantages and disadvantages of the above four algorithms in a K-value selection are given, and the clustering range of the data set is pointed out.},
  issue = {2},
  langid = {english},
  keywords = {Clustering,Convergence,K-means,K-value},
  file = {/home/johnlocke/.zotero/storage/LVAY7B9G/Yuan_Yang_2019_Research on K-Value Selection Method of K-Means Clustering Algorithm.pdf;/home/johnlocke/.zotero/storage/PXZ2EMFI/htm.html}
}

@inproceedings{zamaniMimicsLargescaleData2020,
  title = {Mimics: {{A}} Large-Scale Data Collection for Search Clarification},
  shorttitle = {Mimics},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Zamani, Hamed and Lueck, Gord and Chen, Everest and Quispe, Rodolfo and Luu, Flint and Craswell, Nick},
  date = {2020},
  pages = {3189--3196},
  annotation = {ZSCC: 0000002},
  file = {/home/johnlocke/.zotero/storage/HR4JUK3C/Zamani et al_2020_Mimics.pdf;/home/johnlocke/.zotero/storage/H2XF3C6L/3340531.html}
}

@inproceedings{zamaniNeuralReRankingNeural2018,
  title = {From {{Neural Re-Ranking}} to {{Neural Ranking}}: {{Learning}} a {{Sparse Representation}} for {{Inverted Indexing}}},
  shorttitle = {From {{Neural Re-Ranking}} to {{Neural Ranking}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}} - {{CIKM}} '18},
  author = {Zamani, Hamed and Dehghani, Mostafa and Croft, W. Bruce and Learned-Miller, Erik and Kamps, Jaap},
  date = {2018},
  pages = {497--506},
  publisher = {{ACM Press}},
  location = {{Torino, Italy}},
  doi = {10.1145/3269206.3271800},
  url = {http://dl.acm.org/citation.cfm?doid=3269206.3271800},
  urldate = {2019-12-10},
  abstract = {The availability of massive data and computing power allowing for effective data driven neural approaches is having a major impact on machine learning and information retrieval research, but these models have a basic problem with efficiency. Current neural ranking models are implemented as multistage rankers: for efficiency reasons, the neural model only re-ranks the top ranked documents retrieved by a first-stage efficient ranker in response to a given query. Neural ranking models learn dense representations causing essentially every query term to match every document term, making it highly inefficient or intractable to rank the whole collection. The reliance on a first stage ranker creates a dual problem: First, the interaction and combination effects are not well understood. Second, the first stage ranker serves as a “gate-keeper” or filter, effectively blocking the potential of neural models to uncover new relevant documents. In this work, we propose a standalone neural ranking model (SNRM) by introducing a sparsity property to learn a latent sparse representation for each query and document. This representation captures the semantic relationship between the query and documents, but is also sparse enough to enable constructing an inverted index for the whole collection. We parameterize the sparsity of the model to yield a retrieval model as efficient as conventional term based models. Our model gains in efficiency without loss of effectiveness: it not only outperforms the existing term matching baselines, but also performs similarly to the recent re-ranking based neural models with dense representations. Our model can also take advantage of pseudo-relevance feedback for further improvements. More generally, our results demonstrate the importance of sparsity in neural IR models and show that dense representations can be pruned effectively, giving new insights about essential semantic features and their distributions.},
  eventtitle = {The 27th {{ACM International Conference}}},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  file = {/home/johnlocke/.zotero/storage/CQWVVEPL/Zamani et al_2018_From Neural Re-Ranking to Neural Ranking.pdf}
}

@inproceedings{zhangLearningMultitask2018,
  title = {Learning to Multitask},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Yu and Wei, Ying and Yang, Qiang},
  date = {2018},
  pages = {5771--5782},
  annotation = {ZSCC: 0000016},
  file = {/home/johnlocke/.zotero/storage/CA83WHLL/Zhang et al_2018_Learning to multitask.pdf;/home/johnlocke/.zotero/storage/VV6AX6PK/7819-learning-to-multitask.html}
}

@unpublished{zhangMinimizeExposureBias2020,
  title = {Minimize {{Exposure Bias}} of {{Seq2Seq Models}} in {{Joint Entity}} and {{Relation Extraction}}},
  author = {Zhang, Haoran and Liu, Qianying and Fan, Aysa Xuemo and Ji, Heng and Zeng, Daojian and Cheng, Fei and Kawahara, Daisuke and Kurohashi, Sadao},
  date = {2020},
  eprint = {2009.07503},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/QKMUPNIL/Zhang et al_2020_Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation Extraction.pdf;/home/johnlocke/.zotero/storage/YK3KKA6N/2009.html}
}

@unpublished{zhangSurveyMultitaskLearning2017,
  title = {A Survey on Multi-Task Learning},
  author = {Zhang, Yu and Yang, Qiang},
  date = {2017},
  eprint = {1707.08114},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  annotation = {ZSCC: 0000252},
  file = {/home/johnlocke/.zotero/storage/BYKGJ928/Zhang_Yang_2017_A survey on multi-task learning.pdf;/home/johnlocke/.zotero/storage/TQQDUSXL/1707.html}
}

@article{zhangTimeawareQuerySuggestion2020,
  title = {Time-Aware Query Suggestion Diversification for Temporally Ambiguous Queries},
  author = {Zhang, Xiaojuan and Jiang, Xixi and Qin, Jiewen},
  date = {2020-01-01},
  journaltitle = {The Electronic Library},
  volume = {38},
  number = {4},
  pages = {725--744},
  publisher = {{Emerald Publishing Limited}},
  issn = {0264-0473},
  doi = {10.1108/EL-12-2019-0296},
  url = {https://doi.org/10.1108/EL-12-2019-0296},
  urldate = {2021-03-11},
  abstract = {Purpose The purpose of this study is to generate diversified results for temporally ambiguous queries and the candidate queries are ensured to have a high coverage of subtopics, which are derived from different temporal periods. Design/methodology/approach Two novel time-aware query suggestion diversification models are developed by integrating semantics and temporality information involved in queries into two state-of-the-art explicit diversification algorithms (i.e. IA-select and xQuaD), respectively, and then specifying the components on which these two models rely on. Most importantly, first explored is how to explicitly determine query subtopics for each unique query from the query log or clicked documents and then modeling the subtopics into query suggestion diversification. The discussion on how to mine temporal intent behind a query from query log is also followed. Finally, to verify the effectiveness of the proposal, experiments on a real-world query log are conducted. Findings Preliminary experiments demonstrate that the proposed method can significantly outperform the existing state-of-the-art methods in terms of producing the candidate query suggestion for temporally ambiguous queries. Originality/value This study reports the first attempt to generate query suggestions indicating diverse interested time points to the temporally ambiguous (input) queries. The research will be useful in enhancing users’ search experience through helping them to formulate accurate queries for their search tasks. In addition, the approaches investigated in the paper are general enough to be used in many domains; that is, experimental information retrieval systems, Web search engines, document archives and digital libraries.},
  keywords = {Diversification,Query subtopics,Query suggestions,Temporal intent,Temporally ambiguous queries},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/Z4T2P3ZR/html.html}
}

@inproceedings{zhongPersonalizedQuerySuggestions2020,
  title = {Personalized {{Query Suggestions}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Zhong, Jianling and Guo, Weiwei and Gao, Huiji and Long, Bo},
  date = {2020-07-25},
  series = {{{SIGIR}} '20},
  pages = {1645--1648},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3397271.3401331},
  url = {https://doi.org/10.1145/3397271.3401331},
  urldate = {2021-02-23},
  abstract = {With the exponential growth of information on the internet, users have been relying on search engines for finding the precise documents. However, user queries are often short. The inherent ambiguity of short queries imposes great challenges for search engines to understand user intent. Query suggestion is one key technique for search engines to augment user queries so that they can better understand user intent. In the past, query suggestions have been relying on either term-frequency--based methods with little semantic understanding of the query, or word-embedding--based methods with little personalization efforts. Here, we present a sequence-to-sequence-model--based query suggestion framework that is capable of modeling structured, personalized features and unstructured query texts naturally. This capability opens up the opportunity to better understand query semantics and user intent at the same time. As the largest professional network, LinkedIn has the advantage of utilizing a rich amount of accurate member profile information to personalize query suggestions. We applied this framework in the LinkedIn production traffic and showed that personalized query suggestions significantly improved member search experience as measured by key business metrics at LinkedIn.},
  isbn = {978-1-4503-8016-4},
  keywords = {deep learning model deployment,personalization,query suggestion,sequence-to-sequence model},
  annotation = {ZSCC: 0000000},
  file = {/home/johnlocke/.zotero/storage/2UDZ23B2/Zhong et al_2020_Personalized Query Suggestions.pdf}
}

@unpublished{zhuTexygenBenchmarkingPlatform2018,
  title = {Texygen: {{A Benchmarking Platform}} for {{Text Generation Models}}},
  shorttitle = {Texygen},
  author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  date = {2018-02-06},
  eprint = {1802.01886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.01886},
  urldate = {2020-02-21},
  abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000066},
  file = {/home/johnlocke/.zotero/storage/RAIEF8E3/Zhu et al_2018_Texygen.pdf;/home/johnlocke/.zotero/storage/D5L9BYJQ/1802.html}
}


